---
_build:
  render: never
  list: never

date: "2025-08-17"
title: "Thoughts on MAS"
summary: "Thoughts on MAS"
lastmod: "2025-08-17"
category: "Notes"
series: ["RL Topics", "MARL", "MAS"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---

## Papers to cite:

Multi-Agent Risks from Advanced AI (https://arxiv.org/abs/2502.14143)

Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems (https://arxiv.org/abs/2504.01990)

Agency Is Frame-Dependent (https://arxiv.org/abs/2502.04403)

Why is prompting hard? Understanding prompts on binary sequence predictors (https://arxiv.org/abs/2502.10760)

Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies (https://arxiv.org/abs/2502.02533)

RL, but don't do anything I wouldn't do (https://arxiv.org/abs/2410.06213)

MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking (https://arxiv.org/abs/2501.13011)

MULTI-AGENT REINFORCEMENT LEARNING FOUNDATIONS AND MODERN APPROACHES

Don't build multi-agents (https://cognition.ai/blog/dont-build-multi-agents)

How we built our multi-agent research system (https://www.anthropic.com/engineering/multi-agent-research-system)

How and when to build multi-agent systems (https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/)

An Introduction to Universal Artificial Intelligence

A Survey of Context Engineering for Large Language Models (https://arxiv.org/abs/2507.13334)

What makes a good feedforward computational graph? (https://arxiv.org/abs/2502.06751)

Softmax is not Enough (for Sharp Size Generalisation) (https://arxiv.org/abs/2410.01104)

Why do LLMs attend to the first token? (https://arxiv.org/abs/2504.02732)

Transformers need glasses! Information over-squashing in language tasks (https://arxiv.org/abs/2406.04267)

Round and Round We Go! What makes Rotary Positional Encodings useful? (https://arxiv.org/abs/2410.06205)

Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis (https://arxiv.org/abs/2505.11581)

Genie: Generative Interactive Environments (https://arxiv.org/abs/2402.15391)

Scaling Instructable Agents Across Many Simulated Worlds (https://arxiv.org/abs/2404.10179)

Circuit Tracing: Revealing Computational Graphs in Language Models (https://transformer-circuits.pub/2025/attribution-graphs/methods.html)

On the Biology of a Large Language Model (https://transformer-circuits.pub/2025/attribution-graphs/biology.html)

Tracing Attention Computation Through Feature Interactions (https://transformer-circuits.pub/2025/attention-qk/index.html)

Large Language Models and Emergence: A Complex Systems Perspective (https://arxiv.org/abs/2506.11135)

Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction (https://arxiv.org/abs/2406.19108)

Learning Universal Predictors (https://arxiv.org/abs/2401.14953)

Plasticity as the Mirror of Empowerment (https://arxiv.org/abs/2505.10361)

Polysemanticity and Capacity in Neural Networks (https://arxiv.org/abs/2210.01892)

Capacity-Constrained Continual Learning (https://arxiv.org/abs/2507.21479)

A Definition of Continual Reinforcement Learning (https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d8cf1247786d6dfeefeeb53b8b5f6d7-Abstract-Conference.html)

Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data (https://alignment.anthropic.com/2025/subliminal-learning/)

AI Control: Improving Safety Despite Intentional Subversion (https://arxiv.org/abs/2312.06942)

Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs (https://arxiv.org/abs/2502.08640)

Open Problems in Mechanistic Interpretability (https://arxiv.org/abs/2501.16496)

Ctrl-Z: Controlling AI Agents via Resampling (https://arxiv.org/abs/2504.10374)

We Canâ€™t Understand AI Using our Existing Vocabulary (https://arxiv.org/abs/2502.07586)

Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero (https://arxiv.org/abs/2310.16410)

Lessons from Defending Gemini Against Indirect Prompt Injections (https://arxiv.org/pdf/2505.14534)

Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety (https://arxiv.org/abs/2507.11473)

From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training (https://www.arxiv.org/abs/2508.09224)

Towards Compositional Interpretability for XAI (https://arxiv.org/abs/2406.17583)









