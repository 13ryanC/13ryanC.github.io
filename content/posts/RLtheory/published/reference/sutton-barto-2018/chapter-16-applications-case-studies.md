---
_build:
  render: never
  list: never

date: "2025-06-26"
title: "Chapter 16 Application and Case Studies"
summary: "errata"
category: "Reference"
series: ["Sutton & Barto 2018"]
author: "Author: Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---

# Chapter 16 Applications and Case Studies

In this chapter we present a few case studies of reinforcement learning. Several of these are substantial applications of potential economic significance. One, Samuel's checkers player, is primarily of historical interest. Our presentations are intended to illustrate some of the trade- offs and issues that arise in real applications. For example, we emphasize how domain knowledge is incorporated into the formulation and solution of the problem. We also highlight the representation issues that are so often critical to successful applications. The algorithms used in some of these case studies are substantially more complex than those we have presented in the rest of the book. Applications of reinforcement learning are still far from routine and typically require as much art as science. Making applications easier and more straightforward is one of the goals of current research in reinforcement learning.

# 16.1 TD-Gammon

One of the most impressive applications of reinforcement learning to date is that by Gerald Tesauro to the game of backgammon (Tesauro, 1992, 1994, 1995, 2002). Tesauro's program, TD- Gammon, required little backgammon knowledge, yet learned to play extremely well, near the level of the world's strongest grandmasters. The learning algorithm in TD- Gammon was a straightforward combination of the TD(  \(\lambda\)  ) algorithm and nonlinear function approximation using a multilayer artificial neural network (ANN) trained by backpropagating TD errors.

Backgammon is a major game in the sense that it is played throughout the world, with numerous tournaments and regular world championship matches. It is in part a game of chance, and it is a popular vehicle for waging significant sums of money. There are probably more professional backgammon players than there are professional chess players. The game is played with 15 white and 15 black pieces on a board of 24 locations, called points. To the right on the next page is shown a typical position early in the game, seen from the perspective of the white player. White here has just rolled the dice and obtained a 5 and a 2. This means that he can move one of his pieces 5 steps and one

(possibly the same piece) 2 steps. For example, he could move two pieces from the 12 point, one to the 17 point, and one to the 14 point. White's objective is to advance all of his pieces into the last quadrant (points 19- 24) and then off the board. The first player to remove all his pieces wins. One complication is that the pieces interact as they pass each other going in different directions. For example, if it were black's move, he could use the dice roll of 2 to move a piece from the 24 point to the 22 point,

![](images/92f63e4bbcc1674822cd74ca1cad148f6345893d5105e5201136d98675100476.jpg)  
A backgammon position

"hitting" the white piece there. Pieces that have been hit are placed on the "bar" in the middle of the board (where we already see one previously hit black piece), from whence they reenter the race from the start. However, if there are two pieces on a point, then the opponent cannot move to that point; the pieces are protected from being hit. Thus, white cannot use his 5- 2 dice roll to move either of his pieces on the 1 point, because their possible resulting points are occupied by groups of black pieces. Forming contiguous blocks of occupied points to block the opponent is one of the elementary strategies of the game.

Backgammon involves several further complications, but the above description gives the basic idea. With 30 pieces and 24 possible locations (26, counting the bar and off- the- board) it should be clear that the number of possible backgammon positions is enormous, far more than the number of memory elements one could have in any physically realizable computer. The number of moves possible from each position is also large. For a typical dice roll there might be 20 different ways of playing. In considering future moves, such as the response of the opponent, one must consider the possible dice rolls as well. The result is that the game tree has an effective branching factor of about 400. This is far too large to permit effective use of the conventional heuristic search methods that have proved so effective in games like chess and checkers.

On the other hand, the game is a good match to the capabilities of TD learning methods. Although the game is highly stochastic, a complete description of the game's state is available at all times. The game evolves over a sequence of moves and positions until finally ending in a win for one player or the other, ending the game. The outcome can be interpreted as a final reward to be predicted. On the other hand, the theoretical results we have described so far cannot be usefully applied to this task. The number of states is so large that a lookup table cannot be used, and the opponent is a source of uncertainty and time variation.

TD- Gammon used a nonlinear form of  \(\mathrm{TD}(\lambda)\) . The estimated value,  \(\hat{v} (s, \mathbf{w})\) , of any state (board position)  \(s\)  was meant to estimate the probability of winning starting from state  \(s\) . To achieve this, rewards were defined as zero for all time steps except those on which the game is won. To implement the value function, TD- Gammon used a standard multilayer ANN, much like that shown to the right on the next page. (The real

network had two additional units in its final layer to estimate the probability of each player's winning in a special way called a "gammon" or "backgammon." The network consisted of a layer of input units, a layer of hidden units, and a final output unit. The input to the network was a representation of a backgammon position, and the output was an estimate of the value of that position.

In the first version of TD- Gammon, TD- Gammon 0.0, backgammon positions were represented to the network in a relatively direct way that involved little backgammon knowledge. It did, however, involve substantial knowledge of how ANNs work and how information is best presented to them. It is instructive to note the exact representation Tesauro chose. There were a total of 198 input units to the network. For each point on the backgammon board, four units indicated the number of white pieces on the point. If there were no white pieces, then all four units took on the value zero. If there was one piece, then the first unit took on the value 1. This encoded the elementary concept of a "blot," i.e., a piece that can be hit by the opponent. If there were two or more pieces, then the second unit was set to 1. This encoded the basic concept of a "made point" on which the opponent cannot land. If there were exactly three pieces on the point, then the third unit was set to 1. This encoded the basic concept of a "single spare," i.e., an extra piece in addition to the two pieces that made the point. Finally, if there were more than three pieces, the fourth unit was set to a value proportionate to the number of additional pieces beyond three. Letting  \(n\)  denote the total number of pieces on the point, if  \(n > 3\) , then the fourth unit took on the value  \((n - 3) / 2\) . This encoded a linear representation of "multiple spares" at the given point.

![](images/3d2fb4e9e9236a3cec879fe92b7ed44037a00726f89df98f9874db428c8225d8.jpg)  
Figure 16.1: The TD-Gammon ANN

With four units for white and four for black at each of the 24 points, that made a total of 192 units. Two additional units encoded the number of white and black pieces on the bar (each took the value  \(n / 2\) , where  \(n\)  is the number of pieces on the bar), and two more encoded the number of black and white pieces already successfully removed from the board (these took the value  \(n / 15\) , where  \(n\)  is the number of pieces already borne off). Finally, two units indicated in a binary fashion whether it was white's or black's turn to move. The general logic behind these choices should be clear. Basically, Tesauro tried to represent the position in a straightforward way, while keeping the number of units relatively small. He provided one unit for each conceptually distinct possibility that seemed likely to be relevant, and he scaled them to roughly the same range, in this case between 0 and 1.

Given a representation of a backgammon position, the network computed its estimated value in the standard way. Corresponding to each connection from an input unit to a hidden unit was a real- valued weight. Signals from each input unit were multiplied by

their corresponding weights and summed at the hidden unit. The output,  \(h(j)\) , of hidden unit  \(j\)  was a nonlinear sigmoid function of the weighted sum:

$$
h(j) = \sigma \left(\sum_{i} w_{ij} x_{i}\right) = \frac{1}{1 + e^{-\sum_{i} w_{ij} x_{i}}},
$$

where  \(x_{i}\)  is the value of the  \(i\) th input unit and  \(w_{ij}\)  is the weight of its connection to the  \(j\) th hidden unit (all the weights in the network together make up the parameter vector  \(\mathbf{w}\) ). The output of the sigmoid is always between 0 and 1, and has a natural interpretation as a probability based on a summation of evidence. The computation from hidden units to the output unit was entirely analogous. Each connection from a hidden unit to the output unit had a separate weight. The output unit formed the weighted sum and then passed it through the same sigmoid nonlinearity.

TD- Gammon used the semi- gradient form of the  \(\operatorname {TD}(\lambda)\)  algorithm described in Section 12.2, with the gradients computed by the error backpropagation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that the general update rule for this case is

$$
\mathbf{w}_{t + 1} \doteq \mathbf{w}_t + \alpha \Big[R_{t + 1} + \gamma \lambda (S_{t + 1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_t)\Big] \mathbf{z}_t, \tag{16.1}
$$

where  \(\mathbf{w}_t\)  is the vector of all modifiable parameters (in this case, the weights of the network) and  \(\mathbf{z}_t\)  is a vector of eligibility traces, one for each component of  \(\mathbf{w}_t\) , updated by

$$
\mathbf{z}_t \doteq \gamma \lambda \mathbf{z}_{t - 1} + \nabla \hat{v} (S_t, \mathbf{w}_t),
$$

with  \(\mathbf{z}_0 \doteq \mathbf{0}\) . The gradient in this equation can be computed efficiently by the backpropagation procedure. For the backgammon application, in which  \(\gamma = 1\)  and the reward is always zero except upon winning, the TD error portion of the learning rule is usually just  \(\hat{v} (S_{t + 1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w})\) , as suggested in Figure 16.1.

To apply the learning rule we need a source of backgammon games. Tesauro obtained an unending sequence of games by playing his learning backgammon player against itself. To choose its moves, TD- Gammon considered each of the 20 or so ways it could play its dice roll and the corresponding positions that would result. The resulting positions are afterstates as discussed in Section 6.8. The network was consulted to estimate each of their values. The move was then selected that would lead to the position with the highest estimated value. Continuing in this way, with TD- Gammon making the moves for both sides, it was possible to easily generate large numbers of backgammon games. Each game was treated as an episode, with the sequence of positions acting as the states,  \(S_0, S_1, S_2, \ldots\) . Tesauro applied the nonlinear TD rule (16.1) fully incrementally, that is, after each individual move.

The weights of the network were set initially to small random values. The initial evaluations were thus entirely arbitrary. Because the moves were selected on the basis of these evaluations, the initial moves were inevitably poor, and the initial games often lasted hundreds or thousands of moves before one side or the other won, almost by accident. After a few dozen games however, performance improved rapidly.

After playing about 300,000 games against itself, TD- Gammon 0.0 as described above learned to play approximately as well as the best previous backgammon computer

programs. This was a striking result because all the previous high- performance computer programs had used extensive backgammon knowledge. For example, the reigning champion program at the time was, arguably, Neurogammon, another program written by Tesauro that used an ANN but not TD learning. Neurogammon's network was trained on a large training corpus of exemplary moves provided by backgammon experts, and, in addition, started with a set of features specially crafted for backgammon. Neurogammon was a highly tuned, highly effective backgammon program that decisively won the World Backgammon Olympiad in 1989. TD- Gammon 0.0, on the other hand, was constructed with essentially zero backgammon knowledge. That it was able to do as well as Neurogammon and all other approaches is striking testimony to the potential of self- play learning methods.

The tournament success of TD- Gammon 0.0 with zero expert backgammon knowledge suggested an obvious modification: add the specialized backgammon features but keep the self- play TD learning method. This produced TD- Gammon 1.0. TD- Gammon 1.0 was clearly substantially better than all previous backgammon programs and found serious competition only among human experts. Later versions of the program, TD- Gammon 2.0 (40 hidden units) and TD- Gammon 2.1 (80 hidden units), were augmented with a selective two- ply search procedure. To select moves, these programs looked ahead not just to the positions that would immediately result, but also to the opponent's possible dice rolls and moves. Assuming the opponent always took the move that appeared immediately best for him, the expected value of each candidate move was computed and the best was selected. To save computer time, the second ply of search was conducted only for candidate moves that were ranked highly after the first ply, about four or five moves on average. Two- ply search affected only the moves selected; the learning process proceeded exactly as before. The final versions of the program, TD- Gammon 3.0 and 3.1, used 160 hidden units and a selective three- ply search. TD- Gammon illustrates the combination of learned value functions and decision- time search as in heuristic search and MCTS methods. In follow- on work, Tesauro and Galperin (1997) explored trajectory sampling methods as an alternative to full- width search, which reduced the error rate of live play by large numerical factors (4x- 6x) while keeping the think time reasonable at  \(\sim 5 - 10\)  seconds per move.

During the 1990s, Tesauro was able to play his programs in a significant number of games against world- class human players. A summary of the results is given in Table 16.1.

Table 16.1: Summary of TD-Gammon Results  

<table><tr><td>Program</td><td>Hidden Units</td><td>Training Games</td><td>Opponents</td><td>Results</td></tr><tr><td>TD-Gammon 0.0</td><td>40</td><td>300,000</td><td>other programs</td><td>tied for best</td></tr><tr><td>TD-Gammon 1.0</td><td>80</td><td>300,000</td><td>Robertie, Magriel, ...</td><td>-13 pts / 51 games</td></tr><tr><td>TD-Gammon 2.0</td><td>40</td><td>800,000</td><td>various Grandmasters</td><td>-7 pts / 38 games</td></tr><tr><td>TD-Gammon 2.1</td><td>80</td><td>1,500,000</td><td>Robertie</td><td>-1 pt / 40 games</td></tr><tr><td>TD-Gammon 3.0</td><td>80</td><td>1,500,000</td><td>Kazaros</td><td>+6 pts / 20 games</td></tr></table>

Based on these results and analyses by backgammon grandmasters (Robertie, 1992; see Tesauro, 1995), TD- Gammon 3.0 appeared to play at close to, or possibly better than, the playing strength of the best human players in the world. Tesauro reported in a subsequent article (Tesauro, 2002) the results of an extensive rollout analysis of the move decisions and doubling decisions of TD- Gammon relative to top human players. The conclusion was that TD- Gammon 3.1 had a "lopsided advantage" in piece- movement decisions, and a "slight edge" in doubling decisions, over top humans.

TD- Gammon had a significant impact on the way the best human players play the game. For example, it learned to play certain opening positions differently than was the convention among the best human players. Based on TD- Gammon's success and further analysis, the best human players now play these positions as TD- Gammon does (Tesauro, 1995). The impact on human play was greatly accelerated when several other self- teaching ANN backgammon programs inspired by TD- Gammon, such as Jellyfish, Snowie, and GNUBackgammon, became widely available. These programs enabled wide dissemination of new knowledge generated by the ANNs, resulting in great improvements in the overall caliber of human tournament play (Tesauro, 2002).

# 16.2 Samuel's Checkers Player

An important precursor to Tesauro's TD- Gammon was the seminal work of Arthur Samuel (1959, 1967) in constructing programs for learning to play checkers. Samuel was one of the first to make effective use of heuristic search methods and of what we would now call temporal- difference learning. His checkers players are instructive case studies in addition to being of historical interest. We emphasize the relationship of Samuel's methods to modern reinforcement learning methods and try to convey some of Samuel's motivation for using them.

Samuel first wrote a checkers- playing program for the IBM 701 in 1952. His first learning program was completed in 1955 and was demonstrated on television in 1956. Later versions of the program achieved good, though not expert, playing skill. Samuel was attracted to game- playing as a domain for studying machine learning because games are less complicated than problems "taken from life" while still allowing fruitful study of how heuristic procedures and learning can be used together. He chose to study checkers instead of chess because its relative simplicity made it possible to focus more strongly on learning.

Samuel's programs played by performing a lookahead search from each current position. They used what we now call heuristic search methods to determine how to expand the search tree and when to stop searching. The terminal board positions of each search were evaluated, or "scored," by a value function, or "scoring polynomial," using linear function approximation. In this and other respects Samuel's work seems to have been inspired by the suggestions of Shannon (1950). In particular, Samuel's program was based on Shannon's minimax procedure to find the best move from the current position. Working backward through the search tree from the scored terminal positions, each position was given the score of the position that would result from the best move, assuming that the machine would always try to maximize the score, while the opponent would always try to

minimize it. Samuel called this the "backed- up score" of the position. When the minimax procedure reached the search tree's root—the current position—it yielded the best move under the assumption that the opponent would be using the same evaluation criterion, shifted to its point of view. Some versions of Samuel's programs used sophisticated search control methods analogous to what are known as "alpha- beta" cutoffs (e.g., see Pearl, 1984).

Samuel used two main learning methods, the simplest of which he called rote learning. It consisted simply of saving a description of each board position encountered during play together with its backed- up value determined by the minimax procedure. The result was that if a position that had already been encountered were to occur again as a terminal position of a search tree, the depth of the search was effectively amplified because this position's stored value cached the results of one or more searches conducted earlier. One initial problem was that the program was not encouraged to move along the most direct path to a win. Samuel gave it a "a sense of direction" by decreasing a position's value a small amount each time it was backed up a level (called a ply) during the minimax analysis. "If the program is now faced with a choice of board positions whose scores differ only by the ply number, it will automatically make the most advantageous choice, choosing a low- ply alternative if winning and a high- ply alternative if losing" (Samuel, 1959, p. 80). Samuel found this discounting- like technique essential to successful learning. Rote learning produced slow but continual improvement that was most effective for opening and endgame play. His program became a "better- than- average novice" after learning from many games against itself, a variety of human opponents, and from book games in a supervised learning mode.

Rote learning and other aspects of Samuel's work strongly suggest the essential idea of temporal- difference learning—that the value of a state should equal the value of likely following states. Samuel came closest to this idea in his second learning method, his "learning by generalization" procedure for modifying the parameters of the value function. Samuel's method was the same in concept as that used much later by Tesauro in TD- Gammon. He played his program many games against another version of itself and performed an update after each move. The idea of Samuel's update is suggested by the backup diagram in Figure 16.2. Each open circle represents a position where the program moves next, an on- move position, and each solid circle represents a position where the opponent moves next. An update was made to the value of each on- move position after a move by each side, resulting in a second on- move position. The update was toward the minimax value of a search launched from the second on- move position. Thus, the overall effect was that of a backing- up over one full move of real events and then a search over possible events, as suggested by Figure 16.2. Samuel's actual algorithm was significantly more complex than this for computational reasons, but this was the basic idea.

Samuel did not include explicit rewards. Instead, he fixed the weight of the most important feature, the piece advantage feature, which measured the number of pieces the program had relative to how many its opponent had, giving higher weight to kings, and including refinements so that it was better to trade pieces when winning than when losing. Thus, the goal of Samuel's program was to improve its piece advantage, which in checkers is highly correlated with winning.

![](images/7377b8ad97d443c6ab21a613dfefd3de069809d34f7569c10bd19d298ede7086.jpg)  
Figure 16.2: The backup diagram for Samuel's checkers player.

However, Samuel's learning method may have been missing an essential part of a sound temporal- difference algorithm. Temporal- difference learning can be viewed as a way of making a value function consistent with itself, and this we can clearly see in Samuel's method. But also needed is a way of tying the value function to the true value of the states. We have enforced this via rewards and by discounting or giving a fixed value to the terminal state. But Samuel's method included no rewards and no special treatment of the terminal positions of games. As Samuel himself pointed out, his value function could have become consistent merely by giving a constant value to all positions. He hoped to discourage such solutions by giving his piece- advantage term a large, nonmodifiable weight. But although this may decrease the likelihood of finding useless evaluation functions, it does not prohibit them. For example, a constant function could still be attained by setting the modifiable weights so as to cancel the effect of the nonmodifiable one.

Because Samuel's learning procedure was not constrained to find useful evaluation functions, it should have been possible for it to become worse with experience. In fact, Samuel reported observing this during extensive self- play training sessions. To get the program improving again, Samuel had to intervene and set the weight with the largest absolute value back to zero. His interpretation was that this drastic intervention jarred the program out of local optima, but another possibility is that it jarred the program out of evaluation functions that were consistent but had little to do with winning or losing the game.

Despite these potential problems, Samuel's checkers player using the generalization learning method approached "better- than- average" play. Fairly good amateur opponents characterized it as "tricky but beatable" (Samuel, 1959). In contrast to the rote- learning version, this version was able to develop a good middle game but remained weak in opening and endgame play. This program also included an ability to search through sets of features to find those that were most useful in forming the value function. A later version (Samuel, 1967) included refinements in its search procedure, such as alpha- beta pruning,

extensive use of a supervised learning mode called "book learning," and hierarchical lookup tables called signature tables (Griffith, 1966) to represent the value function instead of linear function approximation. This version learned to play much better than the 1959 program, though still not at a master level. Samuel's checkers- playing program was widely recognized as a significant achievement in artificial intelligence and machine learning.

# 16.3 Watson's Daily-Double Wagering

IBM WATsON  \(\mathbf{\Sigma}^{1}\)  is the system developed by a team of IBM researchers to play the popular TV quiz show Jeopardy!2 It gained fame in 2011 by winning first prize in an exhibition match against human champions. Although the main technical achievement demonstrated by WATsON was its ability to quickly and accurately answer natural language questions over broad areas of general knowledge, its winning Jeopardy! performance also relied on sophisticated decision- making strategies for critical parts of the game. Tesauro, Gondek, Lechner, Fan, and Prager (2012, 2013) adapted Tesauro's TD- Gammon system described above to create the strategy used by WATsON in "Daily- Double" (DD) wagering in its celebrated winning performance against human champions. These authors report that the effectiveness of this wagering strategy went well beyond what human players are able to do in live game play, and that it, along with other advanced strategies, was an important contributor to WATsON's impressive winning performance. Here we focus only on DD wagering because it is the component of WATsON that owes the most to reinforcement learning.

Jeopardy! is played by three contestants who face a board showing 30 squares, each of which hides a clue and has a dollar value. The squares are arranged in six columns, each corresponding to a different category. A contestant selects a square, the host reads the square's clue, and each contestant may choose to respond to the clue by sounding a buzzer ("buzzing in"). The first contestant to buzz in gets to try responding to the clue. If this contestant's response is correct, their score increases by the dollar value of the square; if their response is not correct, or if they do not respond within five seconds, their score decreases by that amount, and the other contestants get a chance to buzz in to respond to the same clue. One or two squares (depending on the game's current round) are special DD squares. A contestant who selects one of these gets an exclusive opportunity to respond to the square's clue and has to decide—before the clue is revealed—on how much to wager, or bet. The bet has to be greater than five dollars but not greater than the contestant's current score. If the contestant responds correctly to the DD clue, their score increases by the bet amount; otherwise it decreases by the bet amount. At the end of each game is a "Final Jeopardy" (FJ) round in which each contestant writes down a sealed bet and then writes an answer after the clue is read. The contestant with the highest score after three rounds of play (where a round consists of revealing all 30 clues) is the winner. The game has many other details, but these are enough to appreciate

the importance of DD wagering. Winning or losing often depends on a contestant's DD wagering strategy.

Whenever WATsON selected a DD square, it chose its bet by comparing action values,  \(\hat{q} (s,b e t)\)  , that estimated the probability of a win from the current game state,  \(s\)  , for each round- dollar legal bet. Except for some risk- abatement measures described below, WATsON selected the bet with the maximum action value. Action values were computed whenever a betting decision was needed by using two types of estimates that were learned before any live game play took place. The first were estimated values of the afterstates (Section 6.8) that would result from selecting each legal bet. These estimates were obtained from a state- value function,  \(\hat{v} (\cdot ,\mathbf{w})\)  , defined by parameters  \(\mathbf{w}\)  , that gave estimates of the probability of a win for WATsON from any game state. The second estimates used to compute action values gave the "in- category DD confidence,"  \(p_{D D}\)  , which estimated the likelihood that WATsON would respond correctly to the as- yet unrevealed DD clue.

Tesauro et al. used the reinforcement learning approach of TD- Gammon described above to learn  \(\hat{v} (\cdot ,\mathbf{w})\)  : a straightforward combination of nonlinear  \(\mathrm{TD}(\lambda)\)  using a multilayer ANN with weights  \(\mathbf{w}\)  trained by backpropagating TD errors during many simulated games. States were represented to the network by feature vectors specifically designed for Jeopardy!. Features included the current scores of the three players, how many DDs remained, the total dollar value of the remaining clues, and other information related to the amount of play left in the game. Unlike TD- Gammon, which learned by self- play, WATsON's  \(\hat{v}\)  was learned over millions of simulated games against carefully- crafted models of human players. In- category confidence estimates were conditioned on the number of right responses  \(r\)  and wrong responses  \(w\)  that WATsON gave in previously- played clues in the current category. The dependencies on  \((r,w)\)  were estimated from WATsON's actual accuracies over many thousands of historical categories.

With the previously learned value function  \(\hat{v}\)  and in- category DD confidence  \(p_{DD}\)  WATsON computed  \(\hat{q} (s,b e t)\)  for each legal round- dollar bet as follows:

$$
\hat{q} (s,b e t) = p_{D D}\times \hat{v} (S_{W} + b e t,\ldots) + (1 - p_{D D})\times \hat{v} (S_{W} - b e t,\ldots), \tag{16.2}
$$

where  \(S_{W}\)  is WATsON's current score, and  \(\hat{v}\)  gives the estimated value for the game state after WATsON's response to the DD clue, which is either correct or incorrect. Computing an action value this way corresponds to the insight from Exercise 3.19 that an action value is the expected next state value given the action (except that here it is the expected next afterstate value because the full next state of the entire game depends on the next square selection).

Tesauro et al. found that selecting bets by maximizing action values incurred "a frightening amount of risk," meaning that if WATsON's response to the clue happened to be wrong, the loss could be disastrous for its chances of winning. To decrease the downside risk of a wrong answer, Tesauro et al. adjusted (16.2) by subtracting a small fraction of the standard deviation over WATsON's correct/incorrect afterstate evaluations. They further reduced risk by prohibiting bets that would cause the wrong- answer afterstate value to decrease below a certain limit. These measures slightly reduced WATsON's expectation of winning, but they significantly reduced downside risk, not only in terms of average risk per DD bet, but even more so in extreme- risk scenarios where a risk- neutral WATsON would bet most or all of its bankroll.

Why was the TD- Gammon method of self- play not used to learn the critical value function  \(\hat{v}\) ? Learning from self- play in Jeopardy! would not have worked very well because WATSON was so different from any human contestant. Self- play would have led to exploration of state space regions that are not typical for play against human opponents, particularly human champions. In addition, unlike backgammon, Jeopardy! is a game of imperfect information because contestants do not have access to all the information influencing their opponents' play. In particular, Jeopardy! contestants do not know how much confidence their opponents have for responding to clues in the various categories. Self- play would have been something like playing poker with someone who is holding the same cards that you hold.

As a result of these complications, much of the effort in developing WATSON's DD- wagering strategy was devoted to creating good models of human opponents. The models did not address the natural language aspect of the game, but were instead stochastic process models of events that can occur during play. Statistics were extracted from an extensive fan- created archive of game information from the beginning of the show to the present day. The archive includes information such as the ordering of the clues, right and wrong contestant answers, DD locations, and DD and FJ bets for nearly 300,000 clues. Three models were constructed: an Average Contestant model (based on all the data), a Champion model (based on statistics from games with the 100 best players), and a Grand Champion model (based on statistics from games with the 10 best players). In addition to serving as opponents during learning, the models were used to assess the benefits produced by the learned DD- wagering strategy. WATSON's win rate in simulation when it used a baseline heuristic DD- wagering strategy was  \(61\%\) ; when it used the learned values and a default confidence value, its win rate increased to  \(64\%\) ; and with live in- category confidence, it was  \(67\%\) . Tesauro et al. regarded this as a significant improvement, given that the DD wagering was needed only about 1.5 to 2 times in each game.

Because WATSON had only a few seconds to bet, as well as to select squares and decide whether or not to buzz in, the computation time needed to make these decisions was a critical factor. The ANN implementation of  \(\hat{v}\)  allowed DD bets to be made quickly enough to meet the time constraints of live play. However, once games could be simulated fast enough through improvements in the simulation software, near the end of a game it was feasible to estimate the value of bets by averaging over many Monte- Carlo trials in which the consequence of each bet was determined by simulating play to the game's end. Selecting endgame DD bets in live play based on Monte- Carlo trials instead of the ANN significantly improved WATSON's performance because errors in value estimates in endgames could seriously affect its chances of winning. Making all the decisions via Monte- Carlo trials might have led to better wagering decisions, but this was simply impossible given the complexity of the game and the time constraints of live play.

Although its ability to quickly and accurately answer natural language questions stands out as WATSON's major achievement, all of its sophisticated decision strategies contributed to its impressive defeat of human champions. According to Tesauro et al. (2012):

... it is plainly evident that our strategy algorithms achieve a level of quantitative precision and real- time performance that exceeds human capabilities.

This is particularly true in the cases of DD wagering and endgame buzzing, where humans simply cannot come close to matching the precise equity and confidence estimates and complex decision calculations performed by Watson.

# 16.4 Optimizing Memory Control

Most computers use dynamic random access memory (DRAM) as their main memory because of its low cost and high capacity. The job of a DRAM memory controller is to efficiently use the interface between the processor chip and an off- chip DRAM system to provide the high- bandwidth and low- latency data transfer necessary for high- speed program execution. A memory controller needs to deal with dynamically changing patterns of read/write requests while adhering to a large number of timing and resource constraints required by the hardware. This is a formidable scheduling problem, especially with modern processors with multiple cores sharing the same DRAM.

Ipek, Mutlu, Martinez, and Caruana (2008) (also Martinez and Ipek, 2009) designed a reinforcement learning memory controller and demonstrated that it can significantly improve the speed of program execution over what was possible with conventional controllers at the time of their research. They were motivated by limitations of existing state- of- the- art controllers that used policies that did not take advantage of past scheduling experience and did not account for long- term consequences of scheduling decisions. Ipek et al.'s project was carried out by means of simulation, but they designed the controller at the detailed level of the hardware needed to implement it- - including the learning algorithm- - directly on a processor chip.

Accessing DRAM involves a number of steps that have to be done according to strict time constraints. DRAM systems consist of multiple DRAM chips, each containing multiple rectangular arrays of storage cells arranged in rows and columns. Each cell stores a bit as the charge on a capacitor. Because the charge decreases over time, each DRAM cell needs to be recharged- - refreshed- - every few milliseconds to prevent memory content from being lost. This need to refresh the cells is why DRAM is called "dynamic."

Each cell array has a row buffer that holds a row of bits that can be transferred into or out of one of the array's rows. An activate command "opens a row," which means moving the contents of the row whose address is indicated by the command into the row buffer. With a row open, the controller can issue read and write commands to the cell array. Each read command transfers a word (a short sequence of consecutive bits) in the row buffer to the external data bus, and each write command transfers a word in the external data bus to the row buffer. Before a different row can be opened, a precharge command must be issued which transfers the (possibly updated) data in the row buffer back into the addressed row of the cell array. After this, another activate command can open a new row to be accessed. Read and write commands are column commands because they sequentially transfer bits into or out of columns of the row buffer; multiple bits can be transferred without re- opening the row. Read and write commands to the currently- open row can be carried out more quickly than accessing a different row, which would involve additional row commands: precharge and activate; this is sometimes

referred to as "row locality." A memory controller maintains a memory transaction queue that stores memory- access requests from the processors sharing the memory system. The controller has to process requests by issuing commands to the memory system while adhering to a large number of timing constraints.

A controller's policy for scheduling access requests can have a large effect on the performance of the memory system, such as the average latency with which requests can be satisfied and the throughput the system is capable of achieving. The simplest scheduling strategy handles access requests in the order in which they arrive by issuing all the commands required by the request before beginning to service the next one. But if the system is not ready for one of these commands, or executing a command would result in resources being underutilized (e.g., due to timing constraints arising from servicing that one command), it makes sense to begin servicing a newer request before finishing the older one. Policies can gain efficiency by reordering requests, for example, by giving priority to read requests over write requests, or by giving priority to read/write commands to already open rows. The policy called First- Ready, First- Come- First- Serve (FR- FCFS), gives priority to column commands (read and write) over row commands (activate and precharge), and in case of a tie gives priority to the oldest command. FR- FCFS was shown to outperform other scheduling policies in terms of average memory- access latency under conditions commonly encountered (Rixner, 2004).

Figure 16.3 is a high- level view of Ipek et al.'s reinforcement learning memory controller. They modeled the DRAM access process as an MDP whose states are the contents of the transaction queue and whose actions are commands to the DRAM system: precharge, activate, read, write, and NoOp. The reward signal is 1 whenever the action is read or write, and otherwise it is 0. State transitions were considered to be stochastic because the next state of the system not only depends on the scheduler's command, but also on aspects of the system's behavior that the scheduler cannot control, such as the workloads of the processor cores accessing the DRAM system.

![](images/59b22ee72a64ec707b2d53953877aec9a22bcd4b1d3583022a4971ac647301bf.jpg)  
Figure 16.3: High-level view of the reinforcement learning DRAM controller. The scheduler is the reinforcement learning agent. Its environment is represented by features of the transaction queue, and its actions are commands to the DRAM system. ©2009 IEEE. Reprinted, with permission, from J. F. Martinez and E. Ipek, Dynamic multicore resource management: A machine learning approach, Micro, IEEE, 29(5), p. 12.

Critical to this MDP are constraints on the actions available in each state. Recall from Chapter 3 that the set of available actions can depend on the state:  \(A_{t} \in \mathcal{A}(S_{t})\) , where  \(A_{t}\)  is the action at time step  \(t\)  and  \(\mathcal{A}(S_{t})\)  is the set of actions available in state  \(S_{t}\) . In this application, the integrity of the DRAM system was assured by not allowing actions that would violate timing or resource constraints. Although Ipek et al. did not make it explicit, they effectively accomplished this by pre- defining the sets  \(\mathcal{A}(S_{t})\)  for all possible states  \(S_{t}\) .

These constraints explain why the MDP has a NoOp action and why the reward signal is 0 except when a read or write command is issued. NoOp is issued when it is the sole legal action in a state. To maximize utilization of the memory system, the controller's task is to drive the system to states in which either a read or a write action can be selected: only these actions result in sending data over the external data bus, so it is only these that contribute to the throughput of the system. Although precharge and activate produce no immediate reward, the agent needs to select these actions to make it possible to later select the rewarded read and write actions.

The scheduling agent used Sarsa (Section 6.4) to learn an action- value function. States were represented by six integer- valued features. To approximate the action- value function, the algorithm used linear function approximation implemented by tile coding with hashing (Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16- bit fixed point numbers. Exploration was  \(\epsilon\) - greedy with  \(\epsilon = 0.05\) .

State features included the number of read requests in the transaction queue, the number of write requests in the transaction queue, the number of write requests in the transaction queue waiting for their row to be opened, and the number of read requests in the transaction queue waiting for their row to be opened that are the oldest issued by their requesting processors. (The other features depended on how the DRAM interacts with cache memory, details we omit here.) The selection of the state features was based on Ipek et al.'s understanding of factors that impact DRAM performance. For example, balancing the rate of servicing reads and writes based on how many of each are in the transaction queue can help avoid stalling the DRAM system's interaction with cache memory. The authors in fact generated a relatively long list of potential features, and then pared them down to a handful using simulations guided by stepwise feature selection.

An interesting aspect of this formulation of the scheduling problem as an MDP is that the features input to the tile coding for defining the action- value function were different from the features used to specify the action- constraint sets  \(\mathcal{A}(S_{t})\) . Whereas the tile coding input was derived from the contents of the transaction queue, the constraint sets depended on a host of other features related to timing and resource constraints that had to be satisfied by the hardware implementation of the entire system. In this way, the action constraints ensured that the learning algorithm's exploration could not endanger the integrity of the physical system, while learning was effectively limited to a "safe" region of the much larger state space of the hardware implementation.

Because an objective of this work was that the learning controller could be implemented on a chip so that learning could occur online while a computer is running, hardware implementation details were important considerations. The design included two five- stage pipelines to calculate and compare two action values at every processor clock cycle, and

to update the appropriate action value. This included accessing the tile coding which was stored on- chip in static RAM. For the configuration Ipek et al. simulated, which was a 4GHz 4- core chip typical of high- end workstations at the time of their research, there were 10 processor cycles for every DRAM cycle. Considering the cycles needed to fill the pipes, up to 12 actions could be evaluated in each DRAM cycle. Ipek et al. found that the number of legal commands for any state was rarely greater than this, and that performance loss was negligible if enough time was not always available to consider all legal commands. These and other clever design details made it feasible to implement the complete controller and learning algorithm on a multi- processor chip.

Ipek et al. evaluated their learning controller in simulation by comparing it with three other controllers: (1) the FR- FCFS controller mentioned above that produces the best on- average performance, (2) a conventional controller that processes each request in order, and (3) an unrealizable ideal controller, called the Optimistic controller, able to sustain  \(100\%\)  DRAM throughput if given enough demand by ignoring all timing and resource constraints, but otherwise modeling DRAM latency (as row buffer hits) and bandwidth. They simulated nine memory- intensive parallel workloads consisting of scientific and data- mining applications. Figure 16.4 shows the performance (the inverse of execution time normalized to the performance of FR- FCFS) of each controller for the nine applications, together with the geometric mean of their performances over the applications. The learning controller, labeled RL in the figure, improved over that of FR- FCFS by from  \(7\%\)  to  \(33\%\)  over the nine applications, with an average improvement of  \(19\%\) . Of course, no realizable controller can match the performance of Optimistic, which ignores all timing and resource constraints, but the learning controller's performance closed the gap with Optimistic's upper bound by an impressive  \(27\%\) .

Because the rationale for on- chip implementation of the learning algorithm was to allow the scheduling policy to adapt online to changing workloads, Ipek et al. analyzed the impact of online learning compared to a previously- learned fixed policy. They trained

![](images/a5b5ddfc46102c76ed0e64fea41e245f0fac522f2b84fb4fe5866ebe787ad8a7.jpg)  
Figure 16.4: Performances of four controllers over a suite of 9 simulated benchmark applications. The controllers are: the simplest 'in-order' controller, FR-FCFS, the learning controller RL, and the unrealizable Optimistic controller which ignores all timing and resource constraints to provide a performance upper bound. Performance, normalized to that of FR-FCFS, is the inverse of execution time. At far right is the geometric mean of performances over the 9 benchmark applications for each controller. Controller RL comes closest to the ideal performance. ©2009 IEEE. Reprinted, with permission, from J. F. Martínez and E. Ipek, Dynamic multicore resource management: A machine learning approach, Micro, IEEE, 29(5), p. 13.

their controller with data from all nine benchmark applications and then held the resulting action values fixed throughout the simulated execution of the applications. They found that the average performance of the controller that learned online was  \(8\%\)  better than that of the controller using the fixed policy, leading them to conclude that online learning is an important feature of their approach.

This learning memory controller was never committed to physical hardware because of the large cost of fabrication. Nevertheless, Ípek et al. could convincingly argue on the basis of their simulation results that a memory controller that learns online via reinforcement learning has the potential to improve performance to levels that would otherwise require more complex and more expensive memory systems, while removing from human designers some of the burden required to manually design efficient scheduling policies. Mukundan and Martínez (2012) took this project forward by investigating learning controllers with additional actions, other performance criteria, and more complex reward functions derived using genetic algorithms. They considered additional performance criteria related to energy efficiency. The results of these studies surpassed the earlier results described above and significantly surpassed the 2012 state- of- the- art for all of the performance criteria they considered. The approach is especially promising for developing sophisticated power- aware DRAM interfaces.

# 16.5 Human-level Video Game Play

One of the greatest challenges in applying reinforcement learning to real- world problems is deciding how to represent and store value functions and/or policies. Unless the state set is finite and small enough to allow exhaustive representation by a lookup table—as in many of our illustrative examples—one must use a parameterized function approximation scheme. Whether linear or nonlinear, function approximation relies on features that have to be readily accessible to the learning system and able to convey the information necessary for skilled performance. Most successful applications of reinforcement learning owe much to sets of features carefully handcrafted based on human knowledge and intuition about the specific problem to be tackled.

A team of researchers at Google DeepMind developed an impressive demonstration that a deep multi- layer ANN can automate the feature design process (Mnih et al., 2013, 2015). Multi- layer ANNs have been used for function approximation in reinforcement learning ever since the 1986 popularization of the backpropagation algorithm as a method for learning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.7). Striking results have been obtained by coupling reinforcement learning with backpropagation. The results obtained by Tesauro and colleagues with TD- Gammon and WATSON discussed above are notable examples. These and other applications benefited from the ability of multi- layer ANNs to learn task- relevant features. However, in all the examples of which we are aware, the most impressive demonstrations required the network's input to be represented in terms of specialized features handcrafted for the given problem. This is vividly apparent in the TD- Gammon results. TD- Gammon 0.0, whose network input was essentially a "raw" representation of the backgammon board, meaning that it involved very little knowledge of backgammon, learned to play approximately as well as

the best previous backgammon computer programs. Adding specialized backgammon features produced TD- Gammon 1.0 which was substantially better than all previous backgammon programs and competed well against human experts.

Mnih et al. developed a reinforcement learning agent called deep  \(Q\) - network (DQN) that combined Q- learning with a deep convolutional ANN, a many- layered, or deep, ANN specialized for processing spatial arrays of data such as images. We describe deep convolutional ANNs in Section 9.7. By the time of Mnih et al.'s work with DQN, deep ANNs, including deep convolutional ANNs, had produced impressive results in many applications, but they had not been widely used in reinforcement learning.

Mnih et al. used DQN to show how a reinforcement learning agent can achieve a high level of performance on any of a collection of different problems without having to use different problem- specific feature sets. To demonstrate this, they let DQN learn to play 49 different Atari 2600 video games by interacting with a game emulator. DQN learned a different policy for each of the 49 games (because the weights of its ANN were reset to random values before learning on each game), but it used the same raw input, network architecture, and parameter values (e.g., step size, discount rate, exploration parameters, and many more specific to the implementation) for all the games. DQN achieved levels of play at or beyond human level on a large fraction of these games. Although the games were alike in being played by watching streams of video images, they varied widely in other respects. Their actions had different effects, they had different state- transition dynamics, and they needed different policies for learning high scores. The deep convolutional ANN learned to transform the raw input common to all the games into features specialized for representing the action values required for playing at the high level DQN achieved for most of the games.

The Atari 2600 is a home video game console that was sold in various versions by Atari Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are now considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although much simpler than modern video games, Atari 2600 games are still entertaining and challenging for human players, and they have been attractive as testbeds for developing and evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf, 2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2013). Bellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study learning and planning algorithms.

These previous studies and the availability of ALE made the Atari 2600 game collection a good choice for Mnih et al.'s demonstration, which was also influenced by the impressive human- level performance that TD- Gammon was able to achieve in backgammon. DQN is similar to TD- Gammon in using a multi- layer ANN as the function approximation method for a semi- gradient form of a TD algorithm, with the gradients computed by the backpropagation algorithm. However, instead of using TD( \(\lambda\) ) as TD- Gammon did, DQN used the semi- gradient form of Q- learning. TD- Gammon estimated the values of afterstates, which were easily obtained from the rules for making backgammon moves. To use the same algorithm for the Atari games would have required generating the next states for each possible action (which would not have been afterstates in that case). This could have been done by using the game emulator to run single- step simulations

for all the possible actions (which ALE makes possible). Or a model of each game's state- transition function could have been learned and used to predict next states (Oh, Guo, Lee, Lewis, and Singh, 2015). While these methods might have produced results comparable to DQN's, they would have been more complicated to implement and would have significantly increased the time needed for learning. Another motivation for using Q- learning was that DQN used the experience replay method, described below, which requires an off- policy algorithm. Being model- free and off- policy made Q- learning a natural choice.

Before describing the details of DQN and how the experiments were conducted, we look at the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with the scores of the best performing learning system in the literature at the time, the scores of a professional human games tester, and the scores of an agent that selected actions at random. The best system from the literature used linear function approximation with features designed using some knowledge about Atari 2600 games (Bellemare, Naddaf, Veness, and Bowling, 2013). DQN learned on each game by interacting with the game emulator for 50 million frames, which corresponds to about 38 days of experience with the game. At the start of learning on each game, the weights of DQN's network were reset to random values. To evaluate DQN's skill level after learning, its score was averaged over 30 sessions on each game, each lasting up to 5 minutes and beginning with a random initial game state. The professional human tester played using the same emulator (with the sound turned off to remove any possible advantage over DQN which did not process audio). After 2 hours of practice, the human played about 20 episodes of each game for up to 5 minutes each and was not allowed to take any break during this time. DQN learned to play better than the best previous reinforcement learning systems on all but 6 of the games, and played better than the human player on 22 of the games. By considering any performance that scored at or above  \(75\%\)  of the human score to be comparable to, or better than, human- level play, Mnih et al. concluded that the levels of play DQN learned reached or exceeded human level on 29 of the 46 games. See Mnih et al. (2015) for a more detailed account of these results.

For an artificial learning system to achieve these levels of play would be impressive enough, but what makes these results remarkable—and what many at the time considered to be breakthrough results for artificial intelligence—is that the very same learning system achieved these levels of play on widely varying games without relying on any game- specific modifications.

A human playing any of these 49 Atari games sees  \(210 \times 160\)  pixel image frames with 128 colors at 60Hz. In principle, exactly these images could have formed the raw input to DQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each frame to produce an  \(84 \times 84\)  array of luminance values. Because the full states of many of the Atari games are not completely observable from the image frames, Mnih et al. "stacked" the four most recent frames so that the inputs to the network had dimension  \(84 \times 84 \times 4\) . This did not eliminate partial observability for all of the games, but it was helpful in making many of them more Markovian.

An essential point here is that these preprocessing steps were exactly the same for all 46 games. No game- specific prior knowledge was involved beyond the general understanding that it should still be possible to learn good policies with this reduced dimension and

that stacking adjacent frames should help with the partial observability of some of the games. Because no game- specific prior knowledge beyond this minimal amount was used in preprocessing the image frames, we can think of the  \(84 \times 84 \times 4\)  input vectors as being "raw" input to DQN.

The basic architecture of DQN is similar to the deep convolutional ANN illustrated in Figure 9.15 (though unlike that network, subsampling in DQN is treated as part of each convolutional layer, with feature maps consisting of units having only a selection of the possible receptive fields). DQN has three hidden convolutional layers, followed by one fully connected hidden layer, followed by the output layer. The three successive hidden convolutional layers of DQN produce  \(3220 \times 20\)  feature maps,  \(649 \times 9\)  feature maps, and  \(647 \times 7\)  feature maps. The activation function of the units of each feature map is a rectifier nonlinearity  \((\max (0,x))\) . The 3,136  \((64 \times 7 \times 7)\)  units in this third convolutional layer all connect to each of 512 units in the fully connected hidden layer, which then each connect to all 18 units in the output layer, one for each possible action in an Atari game.

The activation levels of DQN's output units were the estimated optimal action values of the corresponding state- action pairs, for the state represented by the network's input. The assignment of output units to a game's actions varied from game to game, and because the number of valid actions varied between 4 and 18 for the games, not all output units had functional roles in all of the games. It helps to think of the network as if it were 18 separate networks, one for estimating the optimal action value of each possible action. In reality, these networks shared their initial layers, but the output units learned to use the features extracted by these layers in different ways.

DQN's reward signal indicated how a game's score changed from one time step to the next:  \(+1\)  whenever it increased,  \(- 1\)  whenever it decreased, and 0 otherwise. This standardized the reward signal across the games and made a single step- size parameter work well for all the games despite their varying ranges of scores. DQN used an  \(\epsilon\) - greedy policy, with  \(\epsilon\)  decreasing linearly over the first million frames and remaining at a low value for the rest of the learning session. The values of the various other parameters, such as the learning step size, discount rate, and others specific to the implementation, were selected by performing informal searches to see which values worked best for a small selection of the games. These values were then held fixed for all of the games.

After DQN selected an action, the action was executed by the game emulator, which returned a reward and the next video frame. The frame was preprocessed and added to the four- frame stack that became the next input to the network. Skipping for the moment the changes to the basic Q- learning procedure made by Mnih et al., DQN used the following semi- gradient form of Q- learning to update the network's weights:

$$
\begin{array}{r}\mathbf{w}_{t + 1} = \mathbf{w}_t + \alpha \Big[R_{t + 1} + \gamma \underset {a}{\max}\hat{q} (S_{t + 1},a,\mathbf{w}_t) - \hat{q} (S_t,A_t,\mathbf{w}_t)\Big]\nabla \hat{q} (S_t,A_t,\mathbf{w}_t), \end{array} \tag{16.3}
$$

where  \(\mathbf{w}_t\)  is the vector of the network's weights,  \(A_{t}\)  is the action selected at time step  \(t\) , and  \(S_t\)  and  \(S_{t + 1}\)  are respectively the preprocessed image stacks input to the network at time steps  \(t\)  and  \(t + 1\) .

The gradient in (16.3) was computed by backpropagation. Imagining again that there was a separate network for each action, for the update at time step  \(t\) , backpropagation was applied only to the network corresponding to  \(A_t\) . Mnih et al. took advantage of techniques shown to improve the basic backpropagation algorithm when applied to large

networks. They used a mini- batch method that updated weights only after accumulating gradient information over a small batch of images (here after 32 images). This yielded smoother sample gradients compared to the usual procedure that updates weights after each action. They also used a gradient- ascent algorithm called RMSProp (Tieleman and Hinton, 2012) that accelerates learning by adjusting the step- size parameter for each weight based on a running average of the magnitudes of recent gradients for that weight.

Mnih et al. modified the basic Q- learning procedure in three ways. First, they used a method called experience replay first studied by Lin (1992). This method stores the agent's experience at each time step in a replay memory that is accessed to perform the weight updates. It worked like this in DQN. After the game emulator executed action  \(A_{t}\)  in a state represented by the image stack  \(S_{t}\) , and returned reward  \(R_{t + 1}\)  and image stack  \(S_{t + 1}\) , it added the tuple  \((S_{t}, A_{t}, R_{t + 1}, S_{t + 1})\)  to the replay memory. This memory accumulated experiences over many plays of the same game. At each time step multiple Q- learning updates—a mini- batch—were performed based on experiences sampled uniformly at random from the replay memory. Instead of  \(S_{t + 1}\)  becoming the new  \(S_{t}\)  for the next update as it would in the usual form of Q- learning, a new unconnected experience was drawn from the replay memory to supply data for the next update. Because Q- learning is an off- policy algorithm, it does not need to be applied along connected trajectories.

Q- learning with experience replay provided several advantages over the usual form of Q- learning. The ability to use each stored experience for many updates allowed DQN to learn more efficiently from its experiences. Experience replay reduced the variance of the updates because successive updates were not correlated with one another as they would be with standard Q- learning. And by removing the dependence of successive experiences on the current weights, experience replay eliminated one source of instability.

Mnih et al. modified standard Q- learning in a second way to improve its stability. As in other methods that bootstrap, the target for a Q- learning update depends on the current action- value function estimate. When a parameterized function approximation method is used to represent action values, the target is a function of the same parameters that are being updated. For example, the target in the update given by (16.3) is  \(\gamma \max_{a} \hat{q} (S_{t + 1}, a, \mathbf{w}_{t})\) . Its dependence on  \(\mathbf{w}_{t}\)  complicates the process compared to the simpler supervised- learning situation in which the targets do not depend on the parameters being updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence.

To address this problem Mnih et al. used a technique that brought Q- learning closer to the simpler supervised- learning case while still allowing it to bootstrap. Whenever a certain number,  \(C\) , of updates had been done to the weights  \(\mathbf{w}\)  of the action- value network, they inserted the network's current weights into another network and held these duplicate weights fixed for the next  \(C\)  updates of  \(\mathbf{w}\) . The outputs of this duplicate network over the next  \(C\)  updates of  \(\mathbf{w}\)  were used as the Q- learning targets. Letting  \(\tilde{q}\)  denote the output of this duplicate network, then instead of (16.3) the update rule was:

$$
\mathbf{w}_{t + 1} = \mathbf{w}_t + \alpha \Big[R_{t + 1} + \gamma \max_a\tilde{q} (S_{t + 1},a,\mathbf{w}_t) - \hat{q} (S_t,A_t,\mathbf{w}_t)\Big]\nabla \tilde{q} (S_t,A_t,\mathbf{w}_t).
$$

A final modification of standard Q- learning was also found to improve stability. They clipped the error term  \(R_{t + 1} + \gamma \max_{a} \tilde{q} (S_{t + 1}, a, \mathbf{w}_{t}) - \hat{q} (S_{t}, A_{t}, \mathbf{w}_{t})\)  so that it remained in the interval  \([- 1, 1]\) .

Mnih et al. conducted a large number of learning runs on 5 of the games to gain insight into the effect that various of DQN's design features had on its performance. They ran DQN with the four combinations of experience replay and the duplicate target network being included or not included. Although the results varied from game to game, each of these features alone significantly improved performance, and very dramatically improved performance when used together. Mnih et al. also studied the role played by the deep convolutional ANN in DQN's learning ability by comparing the deep convolutional version of DQN with a version having a network of just one linear layer, both receiving the same stacked preprocessed video frames. Here, the improvement of the deep convolutional version over the linear version was particularly striking across all 5 of the test games.

Creating artificial agents that excel over a diverse collection of challenging tasks has been an enduring goal of artificial intelligence. The promise of machine learning as a means for achieving this has been frustrated by the need to craft problem- specific representations. DeepMind's DQN stands as a major step forward by demonstrating that a single agent can learn problem- specific features enabling it to acquire human- competitive skills over a range of tasks. This demonstration did not produce one agent that simultaneously excelled at all the tasks (because learning occurred separately for each task), but it showed that deep learning can reduce, and possibly eliminate, the need for problem- specific design and tuning. As Mnih et al. point out, however, DQN is not a complete solution to the problem of task- independent learning. Although the skills needed to excel on the Atari games were markedly diverse, all the games were played by observing video images, which made a deep convolutional ANN a natural choice for this collection of tasks. In addition, DQN's performance on some of the Atari 2600 games fell considerably short of human skill levels on these games. The games most difficult for DQN—especially Montezuma's Revenge on which DQN learned to perform about as well as the random player—require deep planning beyond what DQN was designed to do. Further, learning control skills through extensive practice, like DQN learned how to play the Atari games, is just one of the types of learning humans routinely accomplish. Despite these limitations, DQN advanced the state- of- the- art in machine learning by impressively demonstrating the promise of combining reinforcement learning with modern methods of deep learning.

# 16.6 Mastering the Game of Go

The ancient Chinese game of Go has challenged artificial intelligence researchers for many decades. Methods that achieve human- level skill, or even superhuman- level skill, in other games have not been successful in producing strong Go programs. Thanks to a very active community of Go programmers and international competitions, the level of Go program play has improved significantly over the years. Until recently, however, no Go program had been able to play anywhere near the level of a human Go master.

A team at DeepMind (Silver et al., 2016) developed the program AlphaGo that broke this barrier by combining deep ANNs (Section 9.7), supervised learning, Monte Carlo

tree search (MCTS, Section 8.11), and reinforcement learning. By the time of Silver et al.'s 2016 publication, AlphaGo had been shown to be decisively stronger than other Go programs, and it had defeated the European Go champion Fan Hui 5 games to 0. These were the first victories of a Go program over a professional human Go player without handicap in full Go games. Shortly thereafter, a similar version of AlphaGo won stunning victories over the 18- time world champion Lee Sedol, winning 4 out of a 5 games in a challenge match, making worldwide headline news. Artificial intelligence researchers thought that it would be many more years, perhaps decades, before a program reached this level of play.

Here we describe AlphaGo and a successor program called AlphaGo Zero (Silver et al. 2017a). Where in addition to reinforcement learning, AlphaGo relied on supervised learning from a large database of expert human moves, AlphaGo Zero used only reinforcement learning and no human data or guidance beyond the basic rules of the game (hence the Zero in its name). We first describe AlphaGo in some detail in order to highlight the relative simplicity of AlphaGo Zero, which is both higher- performing and more of a pure reinforcement learning program.

In many ways, both AlphaGo and AlphaGo Zero are descendants of Tesauro's TD- Gammon (Section 16.1), itself a descendant of Samuel's checkers player (Section 16.2). All these programs included reinforcement learning over simulated games of self- play. AlphaGo and AlphaGo Zero also built upon the progress made by DeepMind on playing Atari games with the program DQN (Section 16.5) that used deep convolutional ANNs to approximate optimal value functions.

Go is a game between two players who alternately place black and white 'stones' on unoccupied intersections, or 'points,' on a board with a grid of 19 horizontal and 19 vertical lines to produce positions like that shown to the right. The game's goal is to capture an area of the board larger than that captured by the opponent. Stones are captured according to simple rules. A player's stones are captured if they are completely surrounded by the other player's stones, meaning that there is no horizontally or vertically adjacent point that is unoccupied. For example, the left panel of Figure 16.5 (on the next page) shows three white stones with an unoccupied adjacent point (labeled X). If black were to place a stone on X, then the three white stones would be captured and taken off the board (middle panel). However, if white were to place a stone on point X first, then the possibility of this capture would be blocked (right panel). Other rules are needed to prevent infinite capturing/recapturing loops. The game ends when neither player wishes to place another stone. These rules are simple, but they produce a very complex game that has had wide appeal for thousands of years.

![](images/0d99696a646620ef87b752440174a7e2338fa728c23b78f43dc7b52d2f74e4f3.jpg)  
A Go board configuration

the board (middle panel). However, if white were to place a stone on point X first, then the possibility of this capture would be blocked (right panel). Other rules are needed to prevent infinite capturing/recapturing loops. The game ends when neither player wishes to place another stone. These rules are simple, but they produce a very complex game that has had wide appeal for thousands of years.

![](images/08000ca1d8d4334734e0cc14e382f6e9c2610f2fc83bc6c3fb279cfd35b47762.jpg)  
Figure 16.5: Go capturing rule. Left: the three white stones are not surrounded because point X is unoccupied. Middle: if black places a stone on X, the three white stones are captured and removed from the board. Right: if white places a stone on point X first, the capture is blocked.

Methods that produce strong play for other games, such as chess, have not worked as well for Go. The search space for Go is significantly larger than that of chess because Go has a larger number of legal moves per position than chess ( \(\approx 250\)  versus  \(\approx 35\) ) and Go games tend to involve more moves than chess games ( \(\approx 150\)  versus  \(\approx 80\) ). But the size of the search space is not the major factor that makes Go so difficult. Exhaustive search is infeasible for both chess and Go, and Go on smaller boards (e.g.,  \(9 \times 9\) ) has proven to be exceedingly difficult as well. Experts agree that the major stumbling block to creating stronger- than- amateur Go programs is the difficulty of defining an adequate position evaluation function. A good evaluation function allows search to be truncated at a feasible depth by providing relatively easy- to- compute predictions of what deeper search would likely yield. According to Müller (2002): "No simple yet reasonable evaluation function will ever be found for Go." A major step forward was the introduction of MCTS to Go programs. The strongest programs at the time of AlphaGo's development all included MCTS, but master- level skill remained elusive.

Recall from Section 8.11 that MCTS is a decision- time planning procedure that does not attempt to learn and store a global evaluation function. Like a rollout algorithm (Section 8.10), it runs many Monte Carlo simulations of entire episodes (here, entire Go games) to select each action (here, each Go move: where to place a stone or to resign). Unlike a simple rollout algorithm, however, MCTS is an iterative procedure that incrementally extends a search tree whose root node represents the current environment state. As illustrated in Figure 8.10, each iteration traverses the tree by simulating actions guided by statistics associated with the tree's edges. In its basic version, when a simulation reaches a leaf node of the search tree, MCTS expands the tree by adding some, or all, of the leaf node's children to the tree. From the leaf node, or one of its newly added child nodes, a rollout is executed: a simulation that typically proceeds all the way to a terminal state, with actions selected by a rollout policy. When the rollout completes, the statistics associated with the search tree's edges that were traversed in this iteration are updated by backing up the return produced by the rollout. MCTS continues this process, starting each time at the search tree's root at the current state, for as many iterations as possible given the time constraints. Then, finally, an action from the root node (which still represents the current environment state) is selected according to statistics accumulated in the root node's outgoing edges. This is the action the agent takes. After the environment transitions to its next state, MCTS is executed again with the root node set to represent the new current state. The search tree at the start of this next execution might be just this new root node, or it might include descendants of this node left over from MCTS's previous execution. The remainder of the tree is discarded.

# 16.6.1 AlphaGo

The main innovation that made AlphaGo such a strong player is that it selected moves by a novel version of MCTS that was guided by both a policy and a value function learned by reinforcement learning with function approximation provided by deep convolutional ANNs. Another key feature is that instead of reinforcement learning starting from random network weights, it started from weights that were the result of previous supervised learning from a large collection of human expert moves.

The DeepMind team called AlphaGo's modification of basic MCTS "asynchronous policy and value MCTS," or APV- MCTS. It selected actions via basic MCTS as described above but with some twists in how it extended its search tree and how it evaluated action edges. In contrast to basic MCTS, which expands its current search tree by using stored action values to select an unexplored edge from a leaf node, APV- MCTS, as implemented in AlphaGo, expanded its tree by choosing an edge according to probabilities supplied by a 13- layer deep convolutional ANN, called the SL- policy network, trained previously by supervised learning to predict moves contained in a database of nearly 30 million human expert moves.

Then, also in contrast to basic MCTS, which evaluates the newly- added state node solely by the return of a rollout initiated from it, APV- MCTS evaluated the node in two ways: by this return of the rollout, but also by a value function,  \(v_{\theta}\)  , learned previously by a reinforcement learning method. If  \(s\)  was the newly- added node, its value became

$$
v(s) = (1 - \eta)v_{\theta}(s) + \eta G, \tag{16.4}
$$

where  \(G\)  was the return of the rollout and  \(\eta\)  controlled the mixing of the values resulting from these two evaluation methods. In AlphaGo, these values were supplied by the value network, another 13- layer deep convolutional ANN that was trained as we describe below to output estimated values of board positions. APV- MCTS's rollouts in AlphaGo were simulated games with both players using a fast rollout policy provided by a simple linear network, also trained by supervised learning before play. Throughout its execution, APV- MCTS kept track of how many simulations passed through each edge of the search tree, and when its execution completed, the most- visited edge from the root node was selected as the action to take, here the move AlphaGo actually made in a game.

The value network had the same structure as the deep convolutional SL policy network except that it had a single output unit that gave estimated values of game positions instead of the SL policy network's probability distributions over legal actions. Ideally, the value network would output optimal state values, and it might have been possible to approximate the optimal value function along the lines of TD- Gammon described above: self- play with nonlinear TD(λ) coupled to a deep convolutional ANN. But the DeepMind team took a different approach that held more promise for a game as complex as Go. They divided the process of training the value network into two stages. In the first stage, they created the best policy they could by using reinforcement learning to train an RL policy network. This was a deep convolutional ANN with the same structure as the SL policy network. It was initialized with the final weights of the SL policy network that were learned via supervised learning, and then policy- gradient reinforcement learning was used to improve upon the SL policy. In the second stage of training the value network,

the team used Monte Carlo policy evaluation on data obtained from a large number of simulated self- play games with moves selected by the RL policy network.

Figure 16.6 illustrates the networks used by AlphaGo and the steps taken to train them in what the DeepMind team called the "AlphaGo pipeline." All these networks were trained before any live game play took place, and their weights remained fixed throughout live play.

![](images/8c13507da073545eda837f8db57038ea356b691f7e0393fdbb7b682979b60f62.jpg)  
Figure 16.6: AlphaGo pipeline. Adapted with permission from Macmillan Publishers Ltd: Nature, vol. 529(7587), p. 485, ©2016.

Here is some more detail about AlphaGo's ANNs and their training. The identically- structured SL and RL policy networks were similar to DQN's deep convolutional network described in Section 16.5 for playing Atari games, except that they had 13 convolutional layers with the final layer consisting of a soft- max unit for each point on the  \(19 \times 19\)  Go board. The networks' input was a  \(19 \times 19 \times 48\)  image stack in which each point on the Go board was represented by the values of 48 binary or integer- valued features. For example, for each point, one feature indicated if the point was occupied by one of AlphaGo's stones, one of its opponent's stones, or was unoccupied, thus providing the "raw" representation of the board configuration. Other features were based on the rules of Go, such as the number of adjacent points that were empty, the number of opponent stones that would be captured by placing a stone there, the number of turns since a stone was placed there, and other features that the design team considered to be important.

Training the SL policy network took approximately 3 weeks using a distributed implementation of stochastic gradient ascent on 50 processors. The network achieved 57% accuracy, where the best accuracy achieved by other groups at the time of publication was 44.4%. Training the RL policy network was done by policy gradient reinforcement learning over simulated games between the RL policy network's current policy and opponents using policies randomly selected from policies produced by earlier iterations of the learning algorithm. Playing against a randomly selected collection of opponents

prevented overfitting to the current policy. The reward signal was  \(+1\)  if the current policy won,  \(- 1\)  if it lost, and zero otherwise. These games directly pitted the two policies against one another without involving MCTS. By simulating many games in parallel on 50 processors, the DeepMind team trained the RL policy network on a million games in a single day. In testing the final RL policy, they found that it won more than  \(80\%\)  of games played against the SL policy, and it won  \(85\%\)  of games played against a Go program using MCTS that simulated 100,000 games per move.

The value network, whose structure was similar to that of the SL and RL policy networks except for its single output unit, received the same input as the SL and RL policy networks with the exception that there was an additional binary feature giving the current color to play. Monte Carlo policy evaluation was used to train the network from data obtained from a large number of self- play games played using the RL policy. To avoid overfitting and instability due to the strong correlations between positions encountered in self- play, the DeepMind team constructed a data set of 30 million positions each chosen randomly from a unique self- play game. Then training was done using 50 million mini- batches each of 32 positions drawn from this data set. Training took one week on 50 GPUs.

The rollout policy was learned prior to play by a simple linear network trained by supervised learning from a corpus of 8 million human moves. The rollout policy network had to output actions quickly while still being reasonably accurate. In principle, the SL or RL policy networks could have been used in the rollouts, but the forward propagation through these deep networks took too much time for either of them to be used in rollout simulations, a great many of which had to be carried out for each move decision during live play. For this reason, the rollout policy network was less complex than the other policy networks, and its input features could be computed more quickly than the features used for the policy networks. The rollout policy network allowed approximately 1,000 complete game simulations per second to be run on each of the processing threads that AlphaGo used.

One may wonder why the SL policy was used instead of the better RL policy to select actions in the expansion phase of APV- MCTS. These policies took the same amount of time to compute because they used the same network architecture. The team actually found that AlphaGo played better against human opponents when APV- MCTS used as the SL policy instead of the RL policy. They conjectured that the reason for this was that the latter was tuned to respond to optimal moves rather than to the broader set of moves characteristic of human play. Interestingly, the situation was reversed for the value function used by APV- MCTS. They found that when APV- MCTS used the value function derived from the RL policy, it performed better than if it used the value function derived from the SL policy.

Several methods worked together to produce AlphaGo's impressive playing skill. The DeepMind team evaluated different versions of AlphaGo in order to assess the contributions made by these various components. The parameter  \(\eta\)  in (16.4) controlled the mixing of game state evaluations produced by the value network and by rollouts. With  \(\eta = 0\) , AlphaGo used just the value network without rollouts, and with  \(\eta = 1\) , evaluation relied just on rollouts. They found that AlphaGo using just the value network played

better than the rollout- only AlphaGo, and in fact played better than the strongest of all other Go programs existing at the time. The best play resulted from setting  \(\eta = 0.5\) , indicating that combining the value network with rollouts was particularly important to AlphaGo's success. These evaluation methods complemented one another: the value network evaluated the high- performance RL policy that was too slow to be used in live play, while rollouts using the weaker but much faster rollout policy were able to add precision to the value network's evaluations for specific states that occurred during games.

Overall, AlphaGo's remarkable success fueled a new round of enthusiasm for the promise of artificial intelligence, specifically for systems combining reinforcement learning with deep ANNs, to address problems in other challenging domains.

# 16.6.2 AlphaGo Zero

Building upon the experience with AlphaGo, a DeepMind team developed AlphaGo Zero (Silver et al. 2017a). In contrast to AlphaGo, this program used no human data or guidance beyond the basic rules of the game (hence the Zero in its name). It learned exclusively from self- play reinforcement learning, with input giving just "raw" descriptions of the placements of stones on the Go board. AlphaGo Zero implemented a form of policy iteration (Section 4.3), interleaving policy evaluation with policy improvement. Figure 16.7 is an overview of AlphaGo Zero's algorithm. A significant difference between AlphaGo Zero and AlphaGo is that AlphaGo Zero used MCTS to select moves throughout self- play reinforcement learning, whereas AlphaGo used MCTS for live play after—but not during—learning. Other differences besides not using any human data or human- crafted features are that AlphaGo Zero used only one deep convolutional ANN and used a simpler version of MCTS.

AlphaGo Zero's MCTS was simpler than the version used by AlphaGo in that it did not include rollouts of complete games, and therefore did not need a rollout policy. Each iteration of AlphaGo Zero's MCTS ran a simulation that ended at a leaf node of the current search tree instead of at the terminal position of a complete game simulation. But as in AlphaGo, each iteration of MCTS in AlphaGo Zero was guided by the output of a deep convolutional network, labeled  \(f_{\theta}\)  in Figure 16.7, where  \(\theta\)  is the network's weight vector. The input to the network, whose architecture we describe below, consisted of raw representations of board positions, and its output had two parts: a scalar value,  \(v\) , an estimate of the probability that the current player will win from from the current board position, and a vector,  \(\mathbf{p}\) , of move probabilities, one for each possible stone placement on the current board, plus the pass, or resign, move.

Instead of selecting self- play actions according to the probabilities  \(\mathbf{p}\) , however, AlphaGo Zero used these probabilities, together with the network's value output, to direct each execution of MCTS, which returned new move probabilities, shown in Figure 16.7 as the policies  \(\pi_{i}\) . These policies benefitted from the many simulations that MCTS conducted each time it executed. The result was that the policy actually followed by AlphaGo Zero was an improvement over the policy given by the network's outputs  \(\mathbf{p}\) . Silver et al. (2017a) wrote that "MCTS may therefore be viewed as a powerful policy improvement operator."

![](images/9e6ca0cab8e458a8e4ac665b00cf35d30b1b352670a2463c6f5f046dfb3941cb.jpg)  
Figure 16.7: AlphaGo Zero self-play reinforcement learning. a) The program played many games against itself, one shown here as a sequence of board positions  \(s_i\) ,  \(i = 1,2,\ldots ,T\) , with moves  \(a_i\) ,  \(i = 1,2,\ldots ,T\) , and winner  \(z\) . Each move  \(a_i\)  was determined by action probabilities  \(\pi\) , returned by MCTS executed from root node  \(s_i\)  and guided by a deep convolutional network, here labeled  \(f_\theta\) , with latest weights  \(\theta\) . Shown here for just one position  \(s\)  but repeated for all  \(s_i\) , the network's inputs were raw representations of board positions  \(s_i\)  (together with several past positions, though not shown here), and its outputs were vectors  \(\mathbf{p}\)  of move probabilities that guided MCTS's forward searches, and scalar values  \(v\)  that estimated the probability of the current player winning from each position  \(s_i\) . b) Deep convolutional network training. Training examples were randomly sampled steps from recent self-play games. Weights  \(\theta\)  were updated to move the policy vector  \(\mathbf{p}\)  toward the probabilities  \(\pi\)  returned by MCTS, and to include the winners  \(z\)  in the estimated win probability  \(v\) . Reprinted from draft of Silver et al. (2017a) with permission of the authors and DeepMind.

Here is more detail about AlphaGo Zero's ANN and how it was trained. The network took as input a  \(19 \times 19 \times 17\)  image stack consisting of 17 binary feature planes. The first 8 feature planes were raw representations of the positions of the current player's stones in the current and seven past board configurations: a feature value was 1 if a player's stone was on the corresponding point, and was 0 otherwise. The next 8 feature planes similarly coded the positions of the opponent's stones. A final input feature plane had a constant value indicating the color of the current play: 1 for black; 0 for white. Because repetition is not allowed in Go and one player is given some number of "compensation points" for not getting the first move, the current board position is not a Markov state of Go. This is why features describing past board positions and the color feature were needed.

The network was "two- headed," meaning that after a number of initial layers, the network split into two separate "heads" of additional layers that separately fed into two sets of output units. In this case, one head fed 362 output units producing  \(19^2 + 1\)  move

probabilities  \(\mathbf{p}\) , one for each possible stone placement plus pass; the other head fed just one output unit producing the scalar  \(v\) , an estimate of the probability that the current player will win from the current board position. The network before the split consisted of 41 convolutional layers, each followed by batch normalization, and with skip connections added to implement residual learning by pairs of layers (see Section 9.7). Overall, move probabilities and values were computed by 43 and 44 layers respectively.

Starting with random weights, the network was trained by stochastic gradient descent (with momentum, regularization, and step- size parameter decreasing as training continues) using batches of examples sampled uniformly at random from all the steps of the most recent 500,000 games of self- play with the current best policy. Extra noise was added to the network's output  \(\mathbf{p}\)  to encourage exploration of all possible moves. At periodic checkpoints during training, which Silver et al. (2017a) chose to be at every 1,000 training steps, the policy output by the ANN with the latest weights was evaluated by simulating 400 games (using MCTS with 1,600 iterations to select each move) against the current best policy. If the new policy won (by a margin set to reduce noise in the outcome), then it became the best policy to be used in subsequent self- play. The network's weights were updated to make the network's policy output  \(\mathbf{p}\)  more closely match the policy returned by MCTS, and to make its value output,  \(v\) , more closely match the probability that the current best policy wins from the board position represented by the network's input.

The DeepMind team trained AlphaGo Zero over 4.9 million games of self- play, which took about 3 days. Each move of each game was selected by running MCTS for 1,600 iterations, taking approximately 0.4 second per move. Network weights were updated over 700,000 batches each consisting of 2,048 board configurations. They then ran tournaments with the trained AlphaGo Zero playing against the version of AlphaGo that defeated Fan Hui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1. They used the Elo rating system to evaluate the relative performances of the programs. The difference between two Elo ratings is meant to predict the outcome of games between the players. The Elo ratings of AlphaGo Zero, the version of AlphaGo that played against Fan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144, and 3,739. The gaps in these Elo ratings translate into predictions that AlphaGo Zero would defeat these other programs with probabilities very close to one. In a match of 100 games between AlphaGo Zero, trained as described, and the exact version of AlphaGo that defeated Lee Sedol held under the same conditions that were used in that match, AlphaGo Zero defeated AlphaGo in all 100 games.

The DeepMind team also compared AlphaGo Zero with a program using an ANN with the same architecture but trained by supervised learning to predict human moves in a data set containing nearly 30 million positions from 160,000 games. They found that the supervised- learning player initially played better than AlphaGo Zero, and was better at predicting human expert moves, but played less well after AlphaGo Zero was trained for a day. This suggested that AlphaGo Zero had discovered a strategy for playing that was different from how humans play. In fact, AlphaGo Zero discovered, and came to prefer, some novel variations of classical move sequences.

Final tests of AlphaGo Zero's algorithm were conducted with a version having a larger ANN and trained over 29 million self- play games, which took about 40 days, again starting

with random weights. This version achieved an Elo rating of 5,185. The team pitted this version of AlphaGo Zero against a program called AlphaGo Master, the strongest program at the time, that was identical to AlphaGo Zero but, like AlphaGo, used human data and features. AlphaGo Master's Elo rating was 4,858, and it had defeated the strongest human professional players 60 to 0 in online games. In a 100 game match, AlphaGo Zero with the larger network and more extensive learning defeated AlphaGo Master 89 games to 11, thus providing a convincing demonstration of the problem- solving power of AlphaGo Zero's algorithm.

AlphaGo Zero soundly demonstrated that superhuman performance can be achieved by pure reinforcement learning, augmented by a simple version of MCTS, and deep ANNs with very minimal knowledge of the domain and no reliance on human data or guidance. We will surely see systems inspired by the DeepMind accomplishments of both AlphaGo and AlphaGo Zero applied to challenging problems in other domains.

Recently, yet a better program, AlphaZero, was described by Silver et al. (2017b) that does not even incorporate knowledge of Go. AlphaZero is a general reinforcement learning algorithm that improves over the world's hitherto best programs in the diverse games of Go, chess, and shogi.

# 16.7 Personalized Web Services

Personalizing web services such as the delivery of news articles or advertisements is one approach to increasing users' satisfaction with a website or to increase the yield of a marketing campaign. A policy can recommend content considered to be the best for each particular user based on a profile of that user's interests and preferences inferred from their history of online activity. This is a natural domain for machine learning, and in particular, for reinforcement learning. A reinforcement learning system can improve a recommendation policy by making adjustments in response to user feedback. One way to obtain user feedback is by means of website satisfaction surveys, but for acquiring feedback in real time it is common to monitor user clicks as indicators of interest in a link.

A method long used in marketing called  \(A / B\)  testing is a simple type of reinforcement learning used to decide which of two versions, A or B, of a website users prefer. Because it is non- associative, like a two- armed bandit problem, this approach does not personalize content delivery. Adding context consisting of features describing individual users and the content to be delivered allows personalizing service. This has been formalized as a contextual bandit problem (or an associative reinforcement learning problem, Section 2.9) with the objective of maximizing the total number of user clicks. Li, Chu, Langford, and Schapire (2010) applied a contextual bandit algorithm to the problem of personalizing the Yahoo! Front Page Today webpage (one of the most visited pages on the internet at the time of their research) by selecting the news story to feature. Their objective was to maximize the click- through rate (CTR), which is the ratio of the total number of clicks all users make on a webpage to the total number of visits to the page. Their contextual bandit algorithm improved over a standard non- associative bandit algorithm by  \(12.5\%\) .

Theocharous, Thomas, and Ghavamzadeh (2015) argued that better results are possible

by formulating personalized recommendation as a Markov decision problem (MDP) with the objective of maximizing the total number of clicks users make over repeated visits to a website. Policies derived from the contextual bandit formulation are greedy in the sense that they do not take long- term effects of actions into account. These policies effectively treat each visit to a website as if it were made by a new visitor uniformly sampled from the population of the website's visitors. By not using the fact that many users repeatedly visit the same websites, greedy policies do not take advantage of possibilities provided by long- term interactions with individual users.

As an example of how a marketing strategy might take advantage of long- term user interaction, Theocharous et al. contrasted a greedy policy with a longer- term policy for displaying ads for buying a product, say a car. The ad displayed by the greedy policy might offer a discount if the user buys the car immediately. A user either takes the offer or leaves the website, and if they ever return to the site, they would likely see the same offer. A longer- term policy, on the other hand, can transition the user "down a sales funnel" before presenting the final deal. It might start by describing the availability of favorable financing terms, then praise an excellent service department, and then, on the next visit, offer the final discount. This type of policy can result in more clicks by a user over repeated visits to the site, and if the policy is suitably designed, more eventual sales.

Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments to see if policies designed to maximize clicks over the long term could in fact improve over short- term greedy policies. The Adobe Marketing Cloud, a set of tools that many companies use to run digital marketing campaigns, provides infrastructure for automating user- targeted advertising and fund- raising campaigns. Actually deploying novel policies using these tools entails significant risk because a new policy may end up performing poorly. For this reason, the research team needed to assess what a policy's performance would be if it were to be actually deployed, but to do so on the basis of data collected under the execution of other policies. A critical aspect of this research, then, was off- policy evaluation. Further, the team wanted to do this with high confidence to reduce the risk of deploying a new policy. Although high confidence off- policy evaluation was a central component of this research (see also Thomas, 2015; Thomas, Theocharous, and Ghavamzadeh, 2015), here we focus only on the algorithms and their results.

Theocharous et al. compared the results of two algorithms for learning ad recommendation policies. The first algorithm, which they called greedy optimization, had the goal of maximizing only the probability of immediate clicks. As in the standard contextual bandit formulation, this algorithm did not take the long- term effects of recommendations into account. The other algorithm, a reinforcement learning algorithm based on an MDP formulation, aimed at improving the number of clicks users made over multiple visits to a website. They called this latter algorithm life- time value (LTV) optimization. Both algorithms faced challenging problems because the reward signal in this domain is very sparse because users usually do not click on ads, and user clicking is very random so that returns have high variance.

Data sets from the banking industry were used for training and testing these algorithms. The data sets consisted of many complete trajectories of customer interaction with a bank's website that showed each customer one out of a collection of possible offers. If

a customer clicked, the reward was 1, and otherwise it was 0. One data set contained approximately 200,000 interactions from a month of a bank's campaign that randomly offered one of 7 offers. The other data set from another bank's campaign contained 4,000,000 interactions involving 12 possible offers. All interactions included customer features such as the time since the customer's last visit to the website, the number of their visits so far, the last time the customer clicked, geographic location, one of a collection of interests, and features giving demographic information.

Greedy optimization was based on a mapping estimating the probability of a click as a function of user features. The mapping was learned via supervised learning from one of the data sets by means of a random forest (RF) algorithm (Breiman, 2001). RF algorithms have been widely used for large- scale applications in industry because they are effective predictive tools that tend not to overfit and are relatively insensitive to outliers and noise. Theocharous et al. then used the mapping to define an  \(\epsilon\) - greedy policy that selected with probability  \(1 - \epsilon\)  the offer predicted by the RF algorithm to have the highest probability of producing a click, and otherwise selected from the other offers uniformly at random.

LTV optimization used a batch- mode reinforcement learning algorithm called fitted  \(Q\)  iteration (FQI). It is a variant of fitted value iteration (Gordon, 1999) adapted to Q- learning. Batch mode means that the entire data set for learning is available from the start, as opposed to the online mode of the algorithms we focus on in this book in which data are acquired sequentially while the learning algorithm executes. Batch- mode reinforcement learning algorithms are sometimes necessary when online learning is not practical, and they can use any batch- mode supervised learning regression algorithm, including algorithms known to scale well to high- dimensional spaces. The convergence of FQI depends on properties of the function approximation algorithm (Gordon, 1999). For their application to LTV optimization, Theocharous et al. used the same RF algorithm they used for the greedy optimization approach. Because in this case FQI convergence is not monotonic, Theocharous et al. kept track of the best FQI policy by off- policy evaluation using a validation training set. The final policy for testing the LTV approach was the  \(\epsilon\) - greedy policy based on the best policy produced by FQI with the initial action- value function set to the mapping produced by the RF for the greedy optimization approach.

To measure the performance of the policies produced by the greedy and LTV approaches, Theocharous et al. used the CTR metric and a metric they called the LTV metric. These metrics are similar, except that the LTV metric critically distinguishes between individual website visitors:

$$
\mathrm{CTR} = \frac{\mathrm{Total} \# \mathrm{of} \mathrm{Clicks}}{\mathrm{Total} \# \mathrm{of} \mathrm{Visits}},
$$

$$
\mathrm{LTV} = \frac{\mathrm{Total} \# \mathrm{of} \mathrm{Clicks}}{\mathrm{Total} \# \mathrm{of} \mathrm{Visitors}}.
$$

Figure 16.8 illustrates how these metrics differ. Each circle represents a user visit to the site; black circles are visits at which the user clicks. Each row represents visits by a particular user. By not distinguishing between visitors, the CTR for these sequences is 0.35, whereas the LTV is 1.5. Because LTV is larger than CTR to the extent that

individual users revisit the site, it is an indicator of how successful a policy is in encouraging users to engage in extended interactions with the site.

![](images/8f6ff0b7557671bd32758d4e2eb259c20ca8943a486a9bb2babb61871e5a0b73.jpg)  
Figure 16.8: Click through rate (CTR) versus life-time value (LTV). Each circle represents a user visit; black circles are visits at which the user clicks. Adapted from Theocharous et al. (2015).

Testing the policies produced by the greedy and LTV approaches was done using a high confidence off- policy evaluation method on a test data set consisting of real- world interactions with a bank website served by a random policy. As expected, results showed that greedy optimization performed best as measured by the CTR metric, while LTV optimization performed best as measured by the LTV metric. Furthermore—although we have omitted its details—the high confidence off- policy evaluation method provided probabilistic guarantees that the LTV optimization method would, with high probability, produce policies that improve upon policies currently deployed. Assured by these probabilistic guarantees, Adobe announced in 2016 that the new LTV algorithm would be a standard component of the Adobe Marketing Cloud so that a retailer could issue a sequence of offers following a policy likely to yield higher return than a policy that is insensitive to long- term results.

# 16.8 Thermal Soaring

Birds and gliders take advantage of upward air currents—thermals—to gain altitude in order to maintain flight while expending little, or no, energy. Thermal soaring, as this behavior is called, is a complex skill requiring responding to subtle environmental cues to increase altitude by exploiting a rising column of air for as long as possible. Reddy, Celani, Sejnowski, and Vergassola (2016) used reinforcement learning to investigate thermal soaring policies that are effective in the strong atmospheric turbulence usually accompanying rising air currents. Their primary goal was to provide insight into the cues birds sense and how they use them to achieve their impressive thermal soaring performance, but the results also contribute to technology relevant to autonomous gliders. Reinforcement learning had previously been applied to the problem of navigating efficiently to the vicinity of a thermal updraft (Woodbury, Dunn, and Valasek, 2014) but not to the more challenging problem of soaring within the turbulence of the updraft itself.

Reddy et al. modeled the soaring problem as a continuing MDP with discounting. The agent interacted with a detailed model of a glider flying in turbulent air. They

devoted significant effort toward making the model generate realistic thermal soaring conditions, including investigating several different approaches to atmospheric modeling. For the learning experiments, air flow in a three- dimensional box with one kilometer sides, one of which was at ground level, was modeled by a sophisticated physics- based set of partial differential equations involving air velocity, temperature, and pressure. Introducing small random perturbations into the numerical simulation caused the model to produce analogs of thermal updrafts and accompanying turbulence (Figure 16.9 Left) Glider flight was modeled by aerodynamic equations involving velocity, lift, drag, and other factors governing powerless flight of a fixed- wing aircraft. Maneuvering the glider involved changing its angle of attack (the angle between the glider's wing and the direction of air flow) and its bank angle (Figure 16.9 Right).

![](images/6242fc06dd246e5adc7fe4e052e5b23c7da81821ea9f9256c56f3cc767149476.jpg)  
Figure 16.9: Thermal soaring model: Left: snapshot of the vertical velocity field of the simulated cube of air: in red (blue) is a region of large upward (downward) flow. Right: diagram of powerless flight showing bank angle  \(\mu\)  and angle of attack  \(\alpha\) . Adapted with permission From PNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in Turbulent Environments.

The interface between the agent and the environment required defining the agent's actions, the state information the agent receives from the environment, and the reward signal. By experimenting with various possibilities, Reddy et al. decided that three actions each for the angle of attack and the bank angle were enough for their purposes: increment or decrement the current bank angle and angle of attack by  \(5^{\circ}\)  and  \(2.5^{\circ}\) , respectively, or leave them unchanged. This resulted in  \(3^{2}\)  possible actions. The bank angle was bounded to remain between  \(- 15^{\circ}\)  and  \(+15^{\circ}\) .

Because a goal of their study was to try to determine what minimal set of sensory cues are necessary for effective soaring, both to shed light on the cues birds might use for soaring and to minimize the sensing complexity required for automated glider soaring, the authors tried various sets of signals as input to the reinforcement learning agent. They started by using state aggregation (Section 9.3) of a four- dimensional state space with dimensions giving local vertical wind speed, local vertical wind acceleration, torque depending on the difference between the vertical wind velocities at the left and right wing tips, and the local temperature. Each dimension was discretized into three bins: positive

high, negative high, and small. Results, described below, showed that only two of these dimensions were critical for effective soaring behavior.

The overall objective of thermal soaring is to gain as much altitude as possible from each rising column of air. Reddy et al. tried a straightforward reward signal that rewarded the agent at the end of each episode based on the altitude gained over the episode, a large negative reward signal if the glider touched the ground, and zero otherwise. They found that learning was not successful with this reward signal for episodes of realistic duration and that eligibility traces did not help. By experimenting with various reward signals, they found that learning was best with a reward signal that at each time step linearly combined the vertical wind velocity and vertical wind acceleration observed on the previous time step.

Learning was by one- step Sarsa, with actions selected according to a soft- max distribution based on normalized action values. Specifically, the action probabilities were computed according to (13.2) with action preferences:

$$
h(s,a,\pmb {\theta}) = \frac{\hat{q}(s,a,\pmb{\theta}) - \min_b\hat{q}(s,b,\pmb{\theta})}{\tau\big(\max_b\hat{q}(s,b,\pmb{\theta}) - \min_b\hat{q}(s,b,\pmb{\theta})\big)},
$$

where  \(\pmb{\theta}\)  is a parameter vector with one component for each action and aggregated group of states, and  \(\hat{q} (s,a,\pmb {\theta})\)  merely returned the component corresponding to  \(s,a\)  in the usual way for state aggregation methods. The above equation forms the action preferences by normalizing the approximate action values to the interval  \([0,1]\)  then dividing by  \(\tau\) , a positive "temperature parameter."3 As  \(\tau\)  increases, the probability of selecting an action becomes less dependent on its preference; as  \(\tau\)  decreases toward zero, the probability of selecting the most highly- preferred action approaches one, making the policy approach the greedy policy. The temperature parameter  \(\tau\)  was initialized to 2.0 and incrementally decreased to 0.2 during learning. Action preferences were computed from the current estimates of the action values: the action with the maximum estimated action value was given preference  \(1 / \tau\) , the action with the minimum estimated action value was given preference 0, and the preferences of the other actions were scaled between these extremes. The step- size and discount- rate parameters were fixed at 0.1 and 0.98 respectively.

Each learning episode took place with the agent controlling simulated flight in an independently generated period of simulated turbulent air currents. Each episode lasted 2.5 minutes simulated with a 1 second time step. Learning effectively converged after a few hundred episodes. The left panel of Figure 16.10 shows a sample trajectory before learning where the agent selects actions randomly. Starting at the top of the volume shown, the glider's trajectory is in the direction indicated by the arrow and quickly loses altitude. Figure 16.10's right panel is a trajectory after learning. The glider starts at the same place (here appearing at the bottom of the volume) and gains altitude by spiraling within the rising column of air. Although Reddy et al. found that performance varied widely over different simulated periods of air flow, the number of times the glider touched the ground consistently decreased to nearly zero as learning progressed.

After experimenting with different sets of features available to the learning agent, it turned out that the combination of just vertical wind acceleration and torques worked

![](images/1ac09b9838c6ac463a1d49c6379c16b4b88246de1ec7f854cb042d13f3e6290c.jpg)  
Figure 16.10: Sample thermal soaring trajectories, with arrows showing the direction of flight from the same starting point (note that the altitude scales are shifted). Left: before learning: the agent selects actions randomly and the glider descends. Right: after learning: the glider gains altitude by following a spiral trajectory. Adapted with permission from PNAS vol. 113(22), p. E4879, 2016. Reddy, Celani, Sejnowski, and Vergassola. Learning to Soar in Turbulent Environments.

best. The authors conjectured that because these features give information about the gradient of vertical wind velocity in two different directions, they allow the controller to select between turning by changing the bank angle or continuing along the same course by leaving the bank angle alone. This allows the glider to stay within a rising column of air. Vertical wind velocity is indicative of the strength of the thermal but does not help in staying within the flow. They found that sensitivity to temperature was of little help. They also found that controlling the angle of attack is not helpful in staying within a particular thermal, being useful instead for traveling between thermals when covering large distances, as in cross- country gliding and bird migration.

Due to the fact that soaring in different levels of turbulence requires different policies, training was done in conditions ranging from weak to strong turbulence. In strong turbulence the rapidly changing wind and glider velocities allowed less time for the controller to react. This reduced the amount of control possible compared to what was possible for maneuvering when fluctuations were weak. Reddy et al. examined the policies Sarsa learned under these different conditions. Common to policies learned in all regimes were these features: when sensing negative wind acceleration, bank sharply in the direction of the wing with the higher lift; when sensing large positive wind acceleration and no torque, do nothing. However, different levels of turbulence led to policy differences. Policies learned in strong turbulence were more conservative in that they preferred small bank angles, whereas in weak turbulence, the best action was to turn as much as possible by banking sharply. Systematic study of the bank angles preferred by the policies learned under the different conditions led the authors to suggest that by detecting when vertical

wind acceleration crosses a certain threshold the controller can adjust its policy to cope with different turbulence regimes.

Reddy et al. also conducted experiments to investigate the effect of the discount- rate parameter  \(\gamma\)  on the performance of the learned policies. They found that the altitude gained in an episode increased as  \(\gamma\)  increased, reaching a maximum for  \(\gamma = .99\) , suggesting that effective thermal soaring requires taking into account long- term effects of control decisions.

This computational study of thermal soaring illustrates how reinforcement learning can further progress toward different kinds of objectives. Learning policies having access to different sets of environmental cues and control actions contributes to both the engineering objective of designing autonomous gliders and the scientific objective of improving understanding of the soaring skills of birds. In both cases, hypotheses resulting from the learning experiments can be tested in the field by instrumenting real gliders<sup>4</sup> and by comparing predictions with observed bird soaring behavior.
