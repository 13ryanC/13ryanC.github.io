---
date: "2025-06-30"
title: "(Part 2.2) Personal Notes on the Foundations of Reinforcement Learning"
summary: "Aim to provide more insight on RL foundations for beginners"
category: "Tutorial"
series: ["RL Theory"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
sources:
  - title: "Lecture 1 (2022-01-05)"
    url: "http://www.youtube.com/watch?v=rjwxqcVrVws"
  - title: "Lecture 1 (2021-01-12)"
    url: "http://www.youtube.com/watch?v=0oJmSULoj3I"
---

Below is a **selfâ€‘contained compendium** arranged in the sevenâ€‘level hermeneutic order

$$
\boxed{ğ’_{1}\;\leá´´\;ğ’_{2}\;\leá´´\;\dots\;\leá´´\;ğ’_{7}}
$$

so that every concept presupposes only material in earlier clusters.  Formal definitions appear in **blue boxes**, theorems in **green boxes**, and key proofs are sketched in compressed but complete form.  Wherever several viewpoints coexist, they are itemised explicitly.  All sets are finite unless stated Borelâ€‘measurable, which lets us sideâ€‘step technicalities without losing generality.

---

## ğ’â‚Â â€”Â Foundational MDP primitives

### 1.1Â Probability space of trajectories

$$
(\Omega,\mathscr F,\mathbf P)=\left(S\times A\times\mathbb R\right)^{\infty},\qquad
\mathscr F=\sigma\!\bigl(\text{cylinder sets}\bigr).
$$

Random coâ€‘ordinate projections give $S_t,A_t,R_{t+1}$.

---

### 1.2Â **DefinitionÂ (MDP)**

$$
\boxed{\mathcal M=(S,A,T,R,\gamma)},\qquad\gamma\in[0,1).
$$

* $S$: state spaceÂ Â Â Â Â Â Â Â Â Â \* $A$: action space
* $T(s,a,\cdot)$: transition kernel (probability measure on $S$).
* $R(s,a)$: reward, either deterministic or the mean of a bounded distribution.
* $\gamma$: discount factor ensuring $|G_t|<\infty$.

---

### 1.3Â Histories & policies

*Full history* at timeâ€¯t:

$$
H_t=(S_0,A_0,R_1,\ldots,S_t).
$$

$$
\boxed{\pi_t(\,\cdot\mid H_t):A\to[0,1]}\quad
\text{is a policy kernel (measurable in }H_t\text{).}
$$

> **Memoryless / stationary**
> $\pi_t(\cdot\mid H_t)=\pi(\cdot\mid S_t)$.

### 1.4Â Return

$$
G_t=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1},
\qquad
|G_t|\le\frac{R_{\max}}{1-\gamma}.
$$

*Boundedness* follows from geometric summation plus bounded rewards.

---

## ğ’â‚‚Â â€”Â Valueâ€‘centric objectives

### 2.1Â **DefinitionÂ (Value functions)**

$$
V^{\pi}(s) = \mathbf E^{\pi}_s [G_0],
$$

$$
Q^{\pi}(s,a)=\mathbf E^{\pi}_{s,a}[G_0].
$$

### 2.2Â Optimality & Îµâ€‘optimality

$$
V^{\star}(s)=\sup_\pi V^{\pi}(s),\qquad
\pi\text{ is }\varepsilon\text{-optimal}\iff 
V^{\pi}(s)\ge V^{\star}(s)-\varepsilon\;\forall s.
$$

*Why â€œsupâ€ not â€œmaxâ€*Â â€”Â the space of all historyâ€‘dependent policies is (uncountably) infinite; continuity does not guarantee an attained maximum but completeness of â„ guarantees a supremum.

### 2.3Â Initialâ€‘state distribution is optional

For discounted tasks

$$
\mathbf E_{\mu_0}^\pi[G_0]
= (1-\gamma)\sum_{s,a}d^{\pi}_\gamma(s,a)R(s,a),
$$

so $\mu_0$ merely **rescales** the occupancy measure; optimisation is stateâ€‘wise.

---

## ğ’â‚ƒÂ â€”Â Structural sufficiency: occupancy measures & Fundamental Theorem

### 3.1Â **DefinitionÂ (Discounted occupancy measure)**

$$
d_{\gamma}^{\pi} (s,a)  =  \mathbf{E}^{\pi} \bigl[\sum_{t=0}^{\infty} \gamma^{t} \mathbf{1} \lbrace S_t=s,A_t=a \rbrace \bigr]
$$

Properties

* total mass $=1/(1-\gamma)$;
* **linear value identity**
  $V^{\pi}(s_0)=\sum_{s,a}d^{\pi}_\gamma(s,a)R(s,a)$.

### 3.2Â LP formulation

Maximise $\sum_{s,a}d(s,a)R(s,a)$ subject to linear **balance constraints**

$$
\sum_a d(s,a)-(1-\gamma)\mu_0(s)=
\gamma \sum_{s',a'}d(s',a')T(s',a',s),\quad d\ge0.
$$

Feasible set is a compact, convex polytope $ğ’Ÿ$.

### 3.3Â Extremal structure

**Lemma.** An extreme point of $ğ’Ÿ$ corresponds to a **deterministic stationary** policy.

*Proof sketch.*
Eliminate $d$ to $(|S|\times|A|)$ variables.  Every basic feasible solution has at most $|S|$ nonâ€‘zero stateâ€‘action pairs â‡’ each state chooses exactly **one** action â‡’ deterministic & stationary.

---

### 3.4Â **Fundamental Theorem (discounted MDPs)**

$$
\boxed{\exists\;\pi^\star\in A^{S}\text{ deterministic stationary with }V^{\pi^\star}=V^\star.}
$$

*Proof.* Objective is linear, so optimum is at an extreme point of $\mathcal{D}$ â‡’ deterministic stationary $\pi^{\ast}$ (Lemma). â–¡

---

### 3.5Â MemorylessÂ â‰ Â Greedy

*Memoryless* merely means $\pi(s)$ depends only on $s$.
Greedy w.r.t $V$ means

$$
\pi(s)=\arg\max_{a} \lbrace R(s,a)+\gamma\sum_{s'}T(s,a,s')V(s') \rbrace.
$$

A memoryless policy can be nonâ€‘greedy if the argument is not evaluated on the value it induces.  **Example:** twoâ€‘state chain where Ï€ picks a subâ€‘optimum action consistently.

---

## ğ’â‚„Â â€”Â Computational mechanisms

### 4.1Â Bellman operators

$$
(T^\pi V)(s)= R(s,\pi(s))+\gamma\!\!\sum_{s'}T(s,\pi(s),s')V(s'),\qquad
(T^\star V)(s)=\max_a\{R+\gamma \cdot T V\}.
$$

### 4.2Â **Theorem (Contraction)**

$$
\|T^\star V_1-T^\star V_2\|_\infty\le\gamma\|V_1-V_2\|_\infty.
$$

*Proof.* Follows from max of affine maps and Î³<1.

### 4.3Â Value Iteration

```
Vâ‚€ arbitrary
repeat  V_{k+1} â† Tâ˜… V_k
until   â€–V_{k+1}âˆ’V_kâ€–âˆ < Îµ(1âˆ’Î³)/2Î³
Ï€_k(s)=argmax_a {...}
```

**Iteration bound**

$$
k\;\ge\;\frac{\log\!\bigl(\tfrac{2R_{\max}}{(1-\gamma)\varepsilon}\bigr)}
           {\log\!\bigl(\tfrac1\gamma\bigr)}.
$$

### 4.4Â Policy enumeration complexity

$$
|\Pi_{\text{detâ€‘stat}}|=|A|^{|S|},\qquad
|\Pi_{\text{full}}|\approx(|A|^{|S|})^{\mathbb N}\;\text{(uncountable)}.
$$

Policyâ€‘iteration / VI avoid this blowâ€‘up; nevertheless, exact optimal control for undiscounted infiniteâ€‘horizon is **PPADâ€‘complete** (Etessamiâ€¯&â€¯YannakakisÂ â€™10).

---

### 4.5Â Geometry of value space

Attainable value vectors form a **convex polytope** in $\mathbb R^{|S|}$.
Value Iteration traces a Î³â€‘contractive trajectory that spirals onto the **upper surface** (Paretoâ€‘optimal frontier).  For |S|â€¯=â€¯3 one can visualise this as a 3â€‘D polyhedron; successive $T^\star$ images move the point outward until convergence at the extreme point representing Ï€â˜….

---

## ğ’â‚…Â â€”Â Measureâ€‘theoretic foundations

* **Ïƒâ€‘algebras** guarantee every infiniteâ€‘length event we care about (e.g. â€œvisit s infinitely oftenâ€) is measurable.
* **Dominated convergence** lets us swap limit â†” expectation when proving convergence of $G_t$.
* **Radonâ€“Nikodym** derivatives underpin conditional values $V^{\pi}(s)=\mathbf E[G_t\mid S_t=s]$.

### Stâ€¯Petersburg paradox (why Î³â€¯<â€¯1)

With payoff $2^{N}$ on first heads,
$\mathbf E[2^{N}]=\infty$.  Discounting with Î³<Â½ yields finite expectation
$\sum_{n\ge0}Î³^{n}2^{n}<\infty$.  This motivates insisting on Î³<1 or bounded horizon.

### Weak vs. strong Markov

* **Strong Markov property**: state at a stopping time Ï„ is independent of past given $S_\tau$.
* **Weak Markov**: independence only for fixed times.
  Strong â‡’ Weak â‡’ sufficiency of stationary policies; equality holds for timeâ€‘homogeneous kernels.

---

## ğ’â‚†Â â€”Â Horizon & reward variants

| Setting                   | Policy form                                    | Bellman equation                      | Issues                                          |
| ------------------------- | ---------------------------------------------- | ------------------------------------- | ----------------------------------------------- |
| Finite horizon T          | timeâ€‘indexed Ï€â‚œ(s)                             | backward recursion over t             | Stationarity lost.                              |
| Undiscounted total reward | stationary Ï€ may suffice but $G_t$ may diverge | no contraction â‡’ different algorithms | Requires positive recurrence or absorbing goal. |
| Average reward            | stationary Ï€, bias functions h(s)              | $Ï + h(s) = \max_a[R + \sum T h]$     | Needs gainâ€‘bias VI or relativeâ€‘value PI.        |

**Goal states & deadlines**
Augment state with clock $t$ or absorbing â€œterminalâ€; Markov property restored, algorithms unchanged.

---

## ğ’â‚‡Â â€”Â Metaâ€‘model reflections

### Brittleness & Robust MDPs

Introduce an **ambiguity set**

$$
\mathcal T=\{\,\tilde T:\| \tilde T - T\|_1\le \delta \,\}.
$$

Robust value solves

$$
V(s)=\max_a\min_{\tilde T\in\mathcal T}
\bigl\{R(s,a)+\gamma\sum_{s'}\tilde T(s,a,s')V(s')\bigr\}.
$$

### Leaky abstraction

Hidden variables violate Markov; remedy via **feature engineering** or **POMDPs**.
Nonâ€‘stationary exploration policies used during learning purposely break optimalâ€‘stationary dogma.

### Timeâ€‘outs & soft deadlines

*Hard*Â â€” add clock dimension.
*Soft*Â â€” reward shaping $R'(s,a)=R(s,a)-\lambda$ each step imposes implicit discount.

### Nonâ€‘memoryless policies with unmatched value profile

Example: 2â€‘state MDP where Ï€Â¹ alternates actions (depends on parity of t), achieving identical Vâ˜… but *stateâ€‘wise* returns unmatched by any stationary policy at intermediate horizons; underscores why we *define* policies broadly before proving sufficiency.

---

## Closing synthesis

The lattice

$$
ğ’_{1}\toğ’_{2}\toğ’_{3}\toğ’_{4}\toğ’_{5}\toğ’_{6}\toğ’_{7}
$$

first **builds the syntax** of MDPs, then **semantics of preference**, discovers a **structural theorem** that shrinks the search space, develops **algorithms** exploiting that structure, erects the **measureâ€‘theoretic scaffolding** needed for infinite trajectories, surveys **alternative objective framings**, and finally reflects on **model robustness and abstraction limits**.  Each idea is justified, formally defined, and embedded in its proper interpretive stratum, completing the requested comprehensive passage.
