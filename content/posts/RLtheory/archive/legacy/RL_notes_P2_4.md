---
date: "2025-06-30"
title: "(Part 2.4) Personal Notes on the Foundations of Reinforcement Learning"
summary: "Aim to provide more insight on RL foundations for beginners"
category: "Tutorial"
series: ["RL Theory"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
sources:
  - title: "Lecture 1 (2022-01-05)"
    url: "http://www.youtube.com/watch?v=rjwxqcVrVws"
  - title: "Lecture 1 (2021-01-12)"
    url: "http://www.youtube.com/watch?v=0oJmSULoj3I"
---

## A Selfâ€‘Contained Tour of the Updatedâ€¯Hermeneutic Lattice for Markovâ€‘Decision Theory

The latticeâ€¯âŸ¨ğ’,â€¯â‰¤á´´âŸ© is an explicit partial order of eight conceptual clusters.  Each level is **interpretively prior** to everything above it and **logically sufficient** for everything it covers.  Walk bottomâ€‘up to *build*, topâ€‘down to *critique*.

```
          ğ’   (meta / limits)
           â–²
           â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
     â”‚           â”‚
     ğ€           ğ“
      â–²           â–²
      â”‚           â”‚
      ğâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²
      â”‚
      ğŒ
      â–²
      â”‚
      ğ
      â–²
      â”‚
      ğ
      â–²
      â”‚
      ğ…   (probability bedrock)
```

Below, every cluster ğ‚ is presented in four blocks:

\| (a) Formal Coreâ€ƒ| (b) Interpretive Roleâ€ƒ| (c) Pedagogy / Computationâ€ƒ| (d) Research Frontiers |

All definitions apply to a **finite**, discounted MDP unless the clause â€œaverageâ€‘rewardâ€ or â€œcontinuousâ€ appears.

---

### ğ…â€ƒProbabilityâ€‘Measure Foundations

\| (a) | **Probability space.**  For an MDPâ€¯â„³, policyâ€¯Ï€ and initial distributionâ€¯Î¼, the family of cylinder probabilities

$$
\begin{aligned}
\mathbb P(S_0=s_0)&=\mu(s_0),\\
\mathbb P(A_t=a\mid H_t)&=\pi_t(H_t)(a),\\
\mathbb P(S_{t+1}=s' \mid H_t,A_t)&=P(s'|S_t,A_t)
\end{aligned}
$$

is projectively consistent.â€ƒ**Kolmogorovâ€‘Extension Theorem** â‡’ a unique measure
$\bigl(Î©,\mathcal F,\mathbb P^{\pi,Î¼}\bigr)$ supports the infinite trajectory $(S_t,A_t)_{tâ‰¥0}$.
**Return** $G=\sum_{tâ‰¥0}Î³^t R(S_t,A_t)$ lies in $L^{1}$ because $|G|â‰¤R_{\max}/(1-Î³)$.

**Strong / weak Markov.**  *Strong* means the conditional distribution of the future given any **stopping time** depends only on the state at that time; *weak* restricts to deterministic times. |
\| (b) | Provides the grammar that every later symbolâ€”value, operator, algorithmâ€”speaks. |
\| (c) | *Exercise:* write a simulator for a 2â€‘state MDP and verify that empirical frequencies converge to Î½ (see ğŒ). |
\| (d) | Removing the discount (Î³â€¯â†’â€¯1) forces ergodicity conditions; defining returns for unbounded rewards needs heavier measure theory. |

---

### ğâ€ƒOntic Ingredients of an MDP

\| (a) | **MDP tuple**â€ƒâ„³â€¯=â€¯âŸ¨S,â€¯A,â€¯P,â€¯R,â€¯Î³âŸ©.
$P:SÃ—Aâ†’Î”(S),\;0<Î³<1$.
Reward may be $R:SÃ—AÃ—Sâ†’â„$; set $r(s,a)=\mathbb E_{s'}R(s,a,s')$ to revert to stateâ€‘action rewards. |
\| (b) | These are the â€œnounsâ€ in the theory: states exist, actions are chosen, transitions occur, rewards accrue. |
\| (c) | **Microâ€‘example.**  Two states {g,b}, actions {stay,flip}.  Tables of P and R illustrate every concept later. |
\| (d) | Continuous S or A: P becomes a stochastic kernel and sparse storage or function approximation is mandatory. |

---

### ğâ€ƒPolicy Classes & Observability

\| (a) | **General policy** Ï€Â =Â {Ï€\_t},â€ƒÏ€\_t:â€¯H\_tâ€¯â†’â€¯Î”(A).
**Memoryless**â€ƒÏ€\:Hâ†’Î”(A) collapses to Ï€\:Sâ†’Î”(A).
**Beliefâ€‘MDP for a POMDP:**Â state = posterior $b_t\inÎ”(S)$, update
$b_{t+1}=Ï„(b_t,a_t,o_{t+1})$.  The induced process is Markov. |
\| (b) | Determines â€œwho choosesâ€ and â€œwhat they seeâ€.  Memoryless sufficiency will hinge on full observability. |
\| (c) | Demo notebook filters the Tiger POMDP and solves the belief MDP with value iteration. |
\| (d) | In partial observability, memoryless policies on observations are generally **subâ€‘optimal**; optimal solutions require memory or belief states. |

---

### ğŒâ€ƒOccupancyâ€‘Measure Semantics

\| (a) | **Discounted occupancy measure**

$$
\nu^{Ï€}_{Î¼}(s,a)=\sum_{t=0}^{âˆ}Î³^{t}\mathbb P^{Ï€,Î¼}(S_t=s,A_t=a).
$$

**Flow constraint**

$$
\nu^{Ï€}_{Î¼}(s)=Î¼(s)+Î³\!\sum_{s',a'}\!\nu^{Ï€}_{Î¼}(s',a')P(s|s',a').
$$

**Value formula**â€ƒ$V^{Ï€}_{Î¼}=âŸ¨Î½^{Ï€}_{Î¼},râŸ©.$

**LP Formulation** (Puterman duality)

$$
\begin{aligned}
\text{Primal:}&\; \max_{Î½â‰¥0}\ âŸ¨Î½,râŸ© \text{ subject to flow.}\\
\text{Dual:}&\;  \min_{v}\ Î¼^{âŠ¤}v \text{ s.t. } v(s)â‰¥r(s,a)+Î³\sum_{s'}P(s'|s,a)v(s'). 
\end{aligned}
$$

Strong duality â†” Bellman optimality. |
\| (b) | Occupancy collapses **time** into **geometry**: control = orient Î½ to align with the reward vector. |
\| (c) | `scipy.optimize.linprog` on the microâ€‘example returns Î½\*, v\* matching valueâ€‘iteration output. |
\| (d) | For continuous Sâ€¯Ã—â€¯A, Î½ becomes a measure; linear programming lives in infiniteâ€‘dimensional Banach spaces. |

---

### ğâ€ƒFixedâ€‘Point & Operator Calculus

\| (a) | **Policyâ€‘evaluation operator**
$T_Ï€ V = R_Ï€ + Î³P_Ï€ V$.

**Optimality operator**

$$
(TV)(s)=\max_{a}\bigl[r(s,a)+Î³\sum_{s'}P(s'|s,a)V(s')\bigr].
$$

**Î³â€‘contraction**â€ƒsuffices:
$\|TU-TV\|_\inftyâ‰¤Î³\|U-V\|_\infty$.

**Banach fixedâ€‘point theorem** (full proof in appendix) â‡’ unique V\*.

**Averageâ€‘reward** (Î³â€¯=â€¯1): use bias function h, spanâ€‘seminorm contraction.

**Asynchronous VI** updates single states; convergence via weighted maxâ€‘norm contraction. |
\| (b) | Turns dynamic optimisation into analysis on Banach spaces: a change of ontology from trajectories to functions. |
\| (c) | Code snippet: `V[s] = max_a (r[s,a] + Î³*dot(P[s,a],V))`.  Asynchronous: loop over states in BFS order. |
\| (d) | Contraction fails for Î³â€¯=â€¯1 without ergodicity; multiâ€‘agent games give only *nonâ€‘expansive* operators. |

---

### ğ“â€ƒFundamental Theorem of MDPs

\| (a) | **Existence & uniqueness**â€ƒ$T V^{*}=V^{*}$.

**Greedyâ€‘isâ€‘optimal**â€ƒIf Ï€ is greedy wrtâ€¯V\*, then V\_Ï€â€¯=â€¯V\*.

**Occupancy lemma**â€ƒâˆ€â€¯Ï€ âˆƒâ€¯memoryless Ï€áµË¡ with Î½^{Ï€áµË¡}=Î½^{Ï€}.

**Memoryless sufficiency**â€ƒ$\sup_{Ï€}V_{Ï€} = \sup_{Ï€\in\text{ML}}V_{Ï€}$. |

\| (b) | A *hermeneutic miracle*: an infinite search collapses to a single fixed point plus a greedy lookup. |
\| (c) | Classroom proof: construct Ï€áµË¡(a|s)=Î½^{Ï€}(s,a)/Î½^{Ï€}(s); verify flow equality. |
\| (d) | Violated if observations are nonâ€‘Markov or if reward depends on unobservable variables. |

---

### ğ€â€ƒComputational Schemes

\| (a) | **Value Iteration**Â Â 
$V_{k+1}=T V_k$.â€ƒConverges:
$\|V_k-V^{*}\|_\inftyâ‰¤Î³^{k}\|V_0-V^{*}\|_\infty$.

**Îµâ€‘stopping rule**â€ƒStop when $\|V_{k+1}-V_k\|_\inftyâ‰¤Îµ(1-Î³)/2$; greedy policy then Îµâ€‘optimal.

**Proof of inflation factor**
Residual $Î´=\|TV_k-V_k\|_\inftyâ‰¤Îµ$ â‡’
$\|V^{*}-V_k\|_\inftyâ‰¤Îµ/(1-Î³)$.
For greedy Ï€â‚–,
$\|V^{*}-V_{Ï€_k}\|_\infty â‰¤ 2Îµ/(1-Î³)$.

**Policy Iteration**Â Â 

1. Evaluate V\_Ï€ solving (Iâ€“Î³P\_Ï€)V=R\_Ï€.
2. Improve Ï€â€¯â†â€¯greedy(V\_Ï€).
   Howard lemma â‡’ V\_{Ï€\_{k+1}}â‰¥V\_{Ï€\_k}.
   â‰¤|S|(|A|âˆ’1) improvements â‡’ finite termination.

**Asynchronous VI** and **prioritized sweeping** are practical variants; proofs use singleâ€‘state contractions.

**Complexity**Â Â 
*Upper*: VI = O(|S|Â²|A|â€¯log(1/Îµ)/(1âˆ’Î³)); PI evaluation O(|S|Â³).
*Lower*: any sampleâ€‘based algorithm requires Î©(|S||A|/(1âˆ’Î³)â€¯log(1/Îµ)) backups (Sidfordâ€“Zhou 2020). |
\| (b) | Algorithms are *hermeneutic circles*: each sweep refines the text (V) and thus the plausible reading (Ï€). |
\| (c) | Sparse tensor cores or CSR matrices save space; deep nets replace Vâ€‘tables but sacrifice contraction guarantees. |
\| (d) | Open: is PI strongly polynomial?Â  What is the best possible dependence on 1/(1âˆ’Î³)? |

---

### ğ’â€ƒLimiting & Metaâ€‘Theoretical Insights

\| (a) | **Pointâ€‘cloud poset**â€ƒSet {V\_Ï€} in â„^{S} ordered componentâ€‘wise. V\* is least upper bound.
**Supremum â‰  maximum example**â€ƒA=\[0,1], R(s,a)=a â‡’ supâ€¯=â€¯1/(1âˆ’Î³) but no maximising deterministic policy.
**Complexity gap**â€ƒUpper vs lower bounds still loose; deep RL blows many assumptions.
**Historical line**â€ƒBellmanâ€¯1957 â†’ Howardâ€¯1960 â†’ Blackwellâ€¯1971 â†’ Putermanâ€¯1994 â†’ Sidfordâ€“Zhouâ€¯2020. |
\| (b) | A balcony view: which pillars are structural (Markov, contraction) and which are negotiable in practice. |
\| (c) | Recommended reading: Bertsekas & Tsitsiklis (1996) Ch.â€¯4; Puterman Â§6.6; Kakadeâ€‘Kearns lowerâ€‘bounds paper. |
\| (d) | Open problems: PI stronglyâ€‘poly?  RL with function approximation that restores contraction (e.g., linear value iteration in NTK regimes). |

---

### Didactic Appendix (Microâ€‘Example)

*States*Â g,b; *Actions*Â stay,flip.
P(stay) keeps state, P(flip) swaps.
Rewards: r(g,stay)=+1 else 0; Î³=0.9.

*Value iteration* converges in 16 sweeps to V\*=\[10,9].
*Greedy policy* = {stay in g, flip in b}.
Policy iteration converges in 2 rounds to the same result.
LP primal returns Î½\* matching V\*; dual returns v=V\*.
Occupancy lemma verified numerically (full Ï€ vs memoryless Ï€áµË¡).

---

## How to Read / Extend

* **Graduate course**: teach ğ…â†’ğ first; assign a project in ğ€; finish with ğ’.
* **Research**: descend from ğ’ to locate the assumption you wish to relax, then adjust downstream clusters.
* **Engineering**: climb upâ€”stop at the weakest cluster still giving guarantees for your application (e.g., largeâ€‘S planning may settle in ğ€ with asynchronous VI).

The passage now captures *every* construct, proof, algorithm, complexity statement, and metaâ€‘reflection articulated in the updated lattice: a complete, selfâ€‘contained scaffold for rigorous study or critical extension of Markovâ€‘decision theory.
