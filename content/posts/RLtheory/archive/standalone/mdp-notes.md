A Measure-Theoretic Tutorial on the Foundations of Discounted Markov Decision ProcessesThis tutorial provides a rigorous, self-contained development of the theory of discounted Markov Decision Processes (MDPs), the mathematical framework that underpins modern reinforcement learning (RL). Our objective is to build the theory from first principles, starting with the measure-theoretic foundations necessary for a robust treatment of stochastic processes over infinite horizons. We proceed systematically, establishing the existence of a probability law over trajectories, defining value functions and optimality, and culminating in the Fundamental Theorem of MDPs, which guarantees the existence of optimal policies and justifies the core algorithms of dynamic programming.This document is intended for graduate students and researchers in reinforcement learning, control theory, operations research, and related fields who seek a deeper understanding of the mathematical machinery behind the algorithms they use. A background in real analysis and basic probability theory is assumed.1. Measure-Theoretic FoundationsTo analyze sequential decision-making in potentially continuous spaces over infinite time, we must move beyond elementary probability and establish a more powerful mathematical language. This language is measure theory, which provides a rigorous way to define the probability of complex events, such as the set of all trajectories that converge to a goal state. This section lays that essential groundwork.Measurable Spaces and σ-AlgebrasIn probability theory, we speak of assigning probabilities to events. An event is simply a set of possible outcomes. The collection of all events we wish to measure must have a specific structure to ensure consistency. This structure is the σ-algebra.Definition 1.1 (Measurable Space). A measurable space is a pair (Ω,F), where Ω is a non-empty set, called the sample space, and F is a σ-algebra (or σ-field) on Ω. A σ-algebra is a collection of subsets of Ω satisfying three properties 1:Ω∈F (the entire sample space is an event).If E∈F, then its complement Ec=Ω∖E is also in F (if an event can occur, its non-occurrence is also an event).If E1​,E2​,… is a countable sequence of sets in F, then their union ⋃i=1∞​Ei​ is also in F (a countable union of events is an event).The elements of F are called measurable sets or events.Intuitive Explanation. The sample space Ω contains all possible outcomes of an experiment (e.g., all possible infinite sequences of states and actions in RL). The σ-algebra F is the set of "questions" we can ask about the outcome to which we can assign a well-defined probability. For finite or countable Ω, we can often take F to be the power set (the set of all subsets), but for uncountable spaces like the real numbers, this leads to paradoxes. The σ-algebra formalism restricts us to a well-behaved collection of events, ensuring mathematical consistency.3Probability MeasuresOnce we have a space of events, we can define a function that assigns probabilities to them.Definition 1.2 (Probability Measure). Given a measurable space (Ω,F), a probability measure P is a function $P: \mathcal{F} \to $ that satisfies 2:P(Ω)=1.(Countable Additivity) For any countable collection of pairwise disjoint sets E1​,E2​,… in F,P(⋃i=1∞​Ei​)=∑i=1∞​P(Ei​).The triple (Ω,F,P) is called a probability space.Product Spaces and TrajectoriesIn RL, we are interested in infinite sequences of states and actions, known as trajectories. A trajectory τ=(s0​,a0​,s1​,a1​,…) is an element of the infinite product space H=(S×A)∞. To define probabilities on this space, we need a corresponding product σ-algebra.Definition 1.3 (Product σ-Algebra). Let {(Ωn​,Fn​)}n=0∞​ be a sequence of measurable spaces. The product space is the Cartesian product Ω=∏n=0∞​Ωn​. The product σ-algebra, denoted F=⨂n=0∞​Fn​, is the smallest σ-algebra on Ω that makes all coordinate projection maps πn​:Ω→Ωn​ measurable, where πn​(ω0​,ω1​,…)=ωn​.This σ-algebra is generated by the collection of measurable cylinders, which are sets of the form ∏n=0∞​En​ where En​∈Fn​ and En​=Ωn​ for all but a finite number of indices n.4Intuitive Explanation. A measurable cylinder is a set of infinite trajectories that are constrained on only a finite number of time steps. For example, the set of all trajectories starting in state s0​ and taking action a0​ is a cylinder set. The product σ-algebra contains all events that can be constructed by applying countable unions, intersections, and complements to these basic cylinder sets. This ensures that any event that can be described by conditions on a countable number of time steps is measurable.The Ionescu-Tulcea Extension TheoremThe central challenge in formalizing an MDP is to construct a single, coherent probability measure on the infinite space of trajectories from the local, one-step transition probabilities provided by the MDP and a policy. The Ionescu-Tulcea theorem provides the tool to do precisely this.5 It generalizes the construction of product measures to cases where the probability of the next event can depend on all previous outcomes.First, we need the concept of a stochastic kernel, which formalizes a conditional probability distribution.Definition 1.4 (Stochastic Kernel). A stochastic kernel (or Markov kernel) κ from a measurable space (X,FX​) to another measurable space (Y,FY​) is a mapping $\kappa: X \times \mathcal{F}_Y \to $ such that 6:For each fixed x∈X, the mapping B↦κ(x,B) is a probability measure on (Y,FY​).For each fixed set B∈FY​, the mapping x↦κ(x,B) is a measurable function on (X,FX​).Theorem 1.1 (Ionescu-Tulcea Extension Theorem). Let (Ω0​,F0​) be a measurable space and let μ be a probability measure on it. For each n≥1, let (Ωn​,Fn​) be a measurable space and let κn​ be a stochastic kernel from (∏i=0n−1​Ωi​,⨂i=0n−1​Fi​) to (Ωn​,Fn​). Then there exists a unique probability measure P on the product space (Ω,F)=(∏n=0∞​Ωn​,⨂n=0∞​Fn​) such that for any n≥0 and any measurable sets E0​∈F0​,…,En​∈Fn​,P(E0​×⋯×En​×i=n+1∏∞​Ωi​)=∫E0​​μ(dω0​)∫E1​​κ1​(ω0​,dω1​)⋯∫En​​κn​(ω0​,…,ωn−1​,dωn​)(Ionescu Tulcea 1949).5Proof Sketch. The proof relies on Carathéodory's Extension Theorem, which states that a finitely additive measure on an algebra of sets can be uniquely extended to a measure on the σ-algebra generated by it, provided the measure is countably additive.7Define a measure on cylinder sets: Let C be the algebra of all finite-dimensional cylinder sets. For a cylinder set C=E0​×⋯×En​×∏i=n+1∞​Ωi​, define the set function P(C) using the integral expression from the theorem statement.Show finite additivity: Show that this set function P is finitely additive on the algebra C.Show countable additivity: This is the crucial step. It is sufficient to show that P is continuous from above at the empty set, i.e., if Ck​∈C is a decreasing sequence of cylinders with Ck​↓∅, then P(Ck​)→0. This step is typically proven by contradiction, using the properties of the kernels and a telescoping argument or the dominated convergence theorem to show that if P(Ck​) does not go to zero, then the intersection cannot be empty.6Apply Carathéodory's Theorem: Since P is a countably additive measure on the algebra C, Carathéodory's theorem guarantees its unique extension to a measure on F=σ(C). Uniqueness follows from the fact that two measures agreeing on a generating algebra that is closed under finite intersections must be identical.62. Discounted Markov Decision Processes (MDPs)With the mathematical foundations in place, we now formally define the environment model for a sequential decision-making problem under uncertainty. The Markov Decision Process is the canonical model for this setting.8Definition 2.1 (Markov Decision Process). A discounted Markov Decision Process (MDP) is a 5-tuple (S,A,P,r,γ), where 10:S is a set of states, endowed with a σ-algebra FS​. The pair (S,FS​) is the state space.A is a set of actions, endowed with a σ-algebra FA​. The pair (A,FA​) is the action space.$P: (\mathcal{S} \times \mathcal{A}) \times \mathcal{F_S} \to $ is the transition probability kernel. For any state-action pair (s,a) and any measurable set of next states S′∈FS​, P(S′∣s,a) is the probability that the next state St+1​ lies in S′, given the current state is St​=s and the action taken is At​=a.r:S×A→R is the reward function, which is a measurable function specifying the immediate reward received for taking action a in state s.$\gamma \inKey AssumptionsWhile the above definition is general, much of the foundational theory of MDPs is developed under simplifying assumptions. For this tutorial, we primarily adopt the following:Assumption 2.1 (Finiteness). The state space S and action space A are finite sets.Under this assumption, the σ-algebras FS​ and FA​ are simply the power sets 2S and 2A. The transition kernel P simplifies to a function $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to $, where P(s′∣s,a) is the probability of transitioning to state s′ from state s after taking action a. Similarly, the reward function becomes a lookup table r:S×A→R.Assumption 2.2 (Bounded Rewards). The reward function is uniformly bounded. That is, there exists a constant Rmax​<∞ such that ∣r(s,a)∣≤Rmax​ for all (s,a)∈S×A.This assumption, combined with γ<1, ensures that the total discounted return over an infinite horizon is finite, which is essential for value functions to be well-defined.13 Specifically, the total return is bounded in the range $$.A note on reward function definitions: some literature defines the reward as a function of the state, action, and next state, r(s,a,s′). For the purpose of the theory developed here, this is equivalent to defining an expected reward function rˉ(s,a)=Es′∼P(⋅∣s,a)​[r(s,a,s′)]. Since all subsequent Bellman equations rely on this expected immediate reward, the two formulations lead to identical theoretical results.10The Effective HorizonThe discount factor γ makes an infinite-horizon problem mathematically tractable. It also carries a practical interpretation by defining an "effective" planning horizon. Rewards far in the future are discounted to the point of being negligible, so the agent's decisions are primarily influenced by rewards within a finite, albeit soft, time window.Definition 2.2 (Effective Horizon). For a given discount factor γ∈[0,1) and a small tolerance ε>0, the effective horizon Hγ,ε​ is the number of time steps beyond which the discounted sum of maximum possible future rewards is less than ε. A common and simpler heuristic for the effective horizon is given by 14:Hγ​=1−γ1​This value represents the time scale on which the cumulative discounted reward is roughly equivalent to an undiscounted sum of rewards. For instance, the sum of discounted rewards up to this horizon, assuming a constant reward of 1, is ∑t=0Hγ​−1​γt≈1−γ1​=Hγ​. The discount factor at this horizon, γHγ​=(1−(1−γ))1/(1−γ), is approximately e−1≈0.37 for γ close to 1. This means that rewards at the effective horizon are still weighted significantly, but their influence decays rapidly thereafter.14Intuitive Explanation. The discount factor γ can be seen as encoding a preference for immediate gratification or a constant probability 1−γ of the process terminating at any step.15 The effective horizon Hγ​ translates this abstract parameter into a more concrete timescale. An agent with γ=0.99 has an effective horizon of 100 steps, meaning it is "far-sighted," while an agent with γ=0.5 has an effective horizon of 2 steps and is very "myopic." This concept helps bridge the gap between the infinite-horizon mathematical formulation and the finite-horizon nature of many practical problems.3. Policies and the Agent–Environment LoopThe MDP defines the environment. The policy defines the agent. A policy is a rule that specifies how the agent chooses its actions. The interaction between the agent and the environment unfolds in a loop: at each discrete time step t=0,1,2,…, the agent observes the current state St​, selects an action At​ according to its policy, receives a reward Rt+1​, and the environment transitions to a new state St+1​.17To formally define a policy, we must first define the information available to the agent.Definition 3.1 (History). The history at time t is the sequence of states and actions observed up to that time, Ht​=(S0​,A0​,S1​,A1​,…,At−1​,St​). The space of all possible histories of length t is denoted Ht​=(S×A)t×S.The most general type of policy can make decisions based on this entire history.Definition 3.2 (History-Dependent Policy). A general history-dependent randomized policy is a sequence of stochastic kernels π=(πt​)t≥0​, where for each time step t, πt​ is a kernel from the history space Ht​ to the action space (A,FA​). That is, for any history ht​∈Ht​, πt​(⋅∣ht​) is a probability distribution over the set of actions A.18This class of policies is immensely complex and generally intractable. Fortunately, the structure of MDPs allows us to focus on much simpler classes of policies without loss of optimality.Definition 3.3 (Memoryless/Markov Policy). A policy π is memoryless or Markov if the action choice at time t depends only on the current state St​. That is, for any two histories ht​,ht′​∈Ht​ that end in the same state, St​(ht​)=St​(ht′​)=st​, we have πt​(⋅∣ht​)=πt​(⋅∣ht′​). Such a policy can be represented as a sequence of kernels πt​:S→P(A), where P(A) is the set of probability distributions on A.18Definition 3.4 (Stationary Policy). A policy is stationary if its decision rule does not change over time. A stationary Markov policy is defined by a single stochastic kernel π:S→P(A), where π(⋅∣s) is the probability distribution over actions to be used in state s at any time step t.19Definition 3.5 (Deterministic Policy). A policy is deterministic if for any given input (history or state), it specifies a single action with probability 1. A stationary deterministic Markov policy is a measurable function π:S→A that maps each state to a single action.20The relationship between these classes is a hierarchy of generality:History-Dependent ⊃ Markov ⊃ Stationary Markov ⊃ Stationary Deterministic Markov.A central goal of MDP theory is to demonstrate that for the discounted, infinite-horizon criterion, an optimal policy can always be found within the simplest class: stationary, deterministic, and Markov. This is a remarkable result that makes finding optimal policies a tractable problem. The subsequent sections will build the machinery needed to prove this claim.4. Interconnection of a Policy with an MDPHaving defined the environment (MDP) and the agent (policy), we now formally combine them to construct a single stochastic process. This crucial step uses the Ionescu-Tulcea theorem to guarantee the existence of a unique probability measure over the space of all possible infinite trajectories, thereby providing a solid foundation for defining expected values.Let an MDP (S,A,P,r,γ), an initial state distribution μ (a probability measure on (S,FS​)), and a general history-dependent policy π=(πt​)t≥0​ be given. The interaction generates a trajectory τ=(S0​,A0​,S1​,A1​,…), which is an element of the trajectory space H=(S×A)∞. This space is endowed with the product σ-algebra FH​=(FS​⊗FA​)∞.The stochastic process unfolds as follows:The initial state S0​ is drawn according to μ.The first action A0​ is drawn according to the policy's distribution for time 0, π0​(⋅∣S0​).The next state S1​ is drawn according to the MDP's transition kernel, P(⋅∣S0​,A0​).The next action A1​ is drawn according to the policy's distribution for time 1, π1​(⋅∣S0​,A0​,S1​).This process continues, with St+1​∼P(⋅∣St​,At​) and At​∼πt​(⋅∣Ht​).This sequence of conditional sampling steps can be formalized as a sequence of stochastic kernels, allowing us to invoke the Ionescu-Tulcea theorem.Theorem 4.1 (Existence of the Trajectory Probability Measure). For any MDP, any initial state distribution μ, and any policy π, there exists a unique probability measure Pμπ​ on the trajectory space (H,FH​) that is consistent with the sequential generation process described above. That is, for all s∈S,a∈A,t≥0, and any history ht​∈Ht​:Pμπ​(S0​∈B)=μ(B) for any B∈FS​.Pμπ​(At​∈C∣Ht​=ht​)=πt​(C∣ht​) for any C∈FA​.Pμπ​(St+1​∈B∣Ht​=ht​,At​=at​)=P(B∣st​,at​) for any B∈FS​, where st​ and at​ are the last state and action in ht​.Proof Sketch. The proof is a direct application of Theorem 1.1. We construct a sequence of measurable spaces and kernels. Let (Ω0​,F0​)=(S,FS​), (Ω1​,F1​)=(A,FA​), (Ω2​,F2​)=(S,FS​), and so on, alternating between the state and action spaces.The initial measure is μ on (Ω0​,F0​).The first kernel, κ1​, maps from Ω0​=S to Ω1​=A and is defined by the policy at time 0: κ1​(s0​,C)=π0​(C∣s0​).The second kernel, κ2​, maps from Ω0​×Ω1​=S×A to Ω2​=S and is defined by the MDP transition kernel: κ2​((s0​,a0​),B)=P(B∣s0​,a0​).In general, for t≥0, the kernel κ2t+1​ (for action At​) is defined by πt​, and the kernel κ2t+2​ (for state St+1​) is defined by P.With this sequence of spaces and kernels, the Ionescu-Tulcea theorem directly guarantees the existence and uniqueness of the measure Pμπ​ on the product space H.185. Probabilities over Trajectories & ReturnsNow that we have established the existence of a probability measure Pμπ​ over trajectories, we can derive explicit expressions for the probabilities of specific events, particularly finite prefixes of trajectories. These expressions are fundamental to many algorithms, especially in model-based RL and policy gradient methods.For the remainder of this tutorial, unless otherwise specified, we will focus on stationary Markov policies, which we denote simply by π(a∣s). This is the most common and important class of policies in the study of discounted MDPs.Theorem 5.1 (Probability of a Finite Trajectory). For a stationary Markov policy π and an initial state distribution μ, the probability (or probability density for continuous spaces) of a finite trajectory prefix ht​=(s0​,a0​,s1​,a1​,…,st​,at​) is given by 12:Pμπ​(S0​=s0​,A0​=a0​,…,At​=at​)=μ(s0​)π(a0​∣s0​)P(s1​∣s0​,a0​)π(a1​∣s1​)…π(at​∣st​)P(st+1​∣st​,at​)where P(st+1​∣st​,at​) is shorthand for the probability of the event {St+1​=st+1​} given {St​=st​,At​=at​}. A more compact form is:Pμπ​(Sk​=sk​,Ak​=ak​ for 0≤k≤t)=μ(s0​)k=0∏t​π(ak​∣sk​)P(sk+1​∣sk​,ak​)Proof. The proof is a direct application of the chain rule of probability, simplified by the Markov properties of the system. Let the event be E={S0​=s0​,A0​=a0​,…,St​=st​,At​=at​}.\begin{align*}P_\mu^\pi(E) &= P_\mu^\pi(S_0=s_0) \times P_\mu^\pi(A_0=a_0 | S_0=s_0) \times P_\mu^\pi(S_1=s_1 | S_0=s_0, A_0=a_0) \times \dots \&= \mu(s_0) \times \pi(a_0|s_0) \times P(s_1|s_0,a_0) \times \pi(a_1|s_1) \times \dots \&= \mu(s_0) \prod_{k=0}^{t} \pi(a_k|s_k) P(s_{k+1}|s_k, a_k)\end{align*}Each step simplifies because the policy is Markov (depends only on the current state) and the environment is Markov (the next state depends only on the current state-action pair).12The Return as a Random VariableThe ultimate goal of the agent is to maximize the cumulative rewards it receives. This quantity, called the return, is a random variable whose value depends on the specific trajectory realized.Definition 5.1 (The Return). The return from time step t, denoted Gt​, is the sum of discounted rewards from that point forward:Gt​=k=0∑∞​γkRt+k+1​=Rt+1​+γRt+2​+γ2Rt+3​+…where Rt+1​=r(St​,At​). Since the states and actions are random variables defined on the probability space (H,FH​,Pμπ​), the rewards Rt​ and the return Gt​ are also measurable functions on this space, i.e., they are well-defined random variables.The explicit formula for the probability of a finite trajectory is the engine behind many practical algorithms. For example, in policy gradient methods, it is used to compute the likelihood of a trajectory, which is needed to calculate the gradient of the expected return. In off-policy evaluation using importance sampling, it allows calculating the ratio of probabilities of a trajectory under two different policies.6. Value Functions and Optimality CriteriaWith the stochastic process fully defined, we can now introduce the central objects for evaluating the performance of a policy: the value functions. Value functions are expectations of the return, and they provide a way to quantify how "good" it is to be in a certain state, or to take a certain action in a state, under a given policy.23State-Value and Action-Value FunctionsDefinition 6.1 (State-Value Function). For a given policy π, the state-value function, denoted vπ:S→R, maps each state s to the expected return when starting in s and subsequently following policy π 23:$$v^\pi(s) \doteq \mathbb{E}^\pi \left = \mathbb{E}^\pi \left$$The expectation Eπ is taken with respect to the probability measure Pδs​π​, where δs​ is the initial distribution that places all mass on state s.Definition 6.2 (Action-Value Function). For a given policy π, the action-value function, denoted qπ:S×A→R, maps each state-action pair (s,a) to the expected return when starting in state s, taking action a, and thereafter following policy π 23:$$q^\pi(s, a) \doteq \mathbb{E}^\pi \left = \mathbb{E}^\pi \left$$The action-value function is often called the Q-function, and its values are called Q-values.These two functions are related: the value of a state is the expected value of the actions taken from it, averaged over the policy's choices:vπ(s)=Ea∼π(⋅∣s)​[qπ(s,a)]Optimality CriteriaThe goal of reinforcement learning is to find the "best" policy. Value functions provide a way to formalize this by inducing a partial ordering on the space of policies.Definition 6.3 (Policy Ordering and Optimality).A policy π is defined to be better than or equal to a policy π′, denoted π≥π′, if vπ(s)≥vπ′(s) for all states s∈S.An optimal policy, denoted π∗, is a policy such that π∗≥π for all policies π∈Π, where Π is the set of all policies.27It is a cornerstone result of MDP theory that for any finite MDP, at least one such optimal policy exists. Furthermore, all optimal policies share the same value functions, which we call the optimal value functions.Definition 6.4 (Optimal Value Functions). The optimal state-value function, v∗, and the optimal action-value function, q∗, are defined as the maximum value achievable from any state or state-action pair 28:v∗(s)≐π∈Πsup​vπ(s)for all s∈Sq∗(s,a)≐π∈Πsup​qπ(s,a)for all (s,a)∈S×AFor any optimal policy π∗, it holds that vπ∗=v∗ and qπ∗=q∗. The optimal value functions are related by:v∗(s)=a∈Amax​q∗(s,a)The existence of an optimal policy that is uniformly best for all starting states is a powerful and simplifying feature of the standard discounted MDP framework. In more complex settings, such as constrained MDPs where an agent must satisfy certain safety constraints, the optimal policy may depend on the initial state.29 The unconstrained formulation ensures that the goal is universal, and as we will see, so is the optimal solution.7. Memoryless vs. General PoliciesWe previously defined a hierarchy of policy classes, from the most general history-dependent policies to the simplest stationary, deterministic, Markov policies. A fundamental question is whether we lose performance by restricting our search to these simpler classes. For standard discounted MDPs, the answer is no.Theorem 7.1 (Sufficiency of Stationary Markov Policies). For any discounted MDP with finite state and action spaces, and for any initial state s∈S:For any history-dependent policy π, there exists a stationary Markov policy π′ such that vπ′(s)≥vπ(s).There exists an optimal policy π∗ that is stationary and Markov.Proof Sketch. A full proof of this result is surprisingly elegant and can be constructed using the concept of occupancy measures (Section 10). An alternative, more direct proof proceeds by construction and induction.30Fix an arbitrary history-dependent policy π and a state s.Construct a new stationary Markov policy π′ by setting, for each state s′∈S, π′(s′) to be a greedy action with respect to the action-value function of the original policy π:π′(s′)∈arga∈Amax​qπ(s′,a)Show that this new policy π′ is an improvement over or equal to π. The key insight is that for any state s′, by definition of vπ and the greedy choice of π′, we have:vπ(s′)=Ea∼π(⋅∣s′)​[qπ(s′,a)]≤a∈Amax​qπ(s′,a)=qπ(s′,π′(s′))Unrolling this inequality recursively shows that vπ′(s)≥vπ(s) for all s.Since for any policy, we can find a stationary Markov policy that is at least as good, the supremum over all policies (which defines v∗) must be achieved by a policy within the stationary Markov class.When History Matters: The Case of Partial ObservabilityThe sufficiency of Markov policies hinges critically on the assumption that the agent can fully observe the state st​. When this assumption is violated, history becomes essential for optimal decision-making. This is the domain of Partially Observable Markov Decision Processes (POMDPs).31In a POMDP, the agent does not observe the state st​. Instead, it receives an observation ot​, which is probabilistically related to the underlying state by an observation function O(ot​∣st​,at​). Since different states can produce the same observation (a phenomenon called perceptual aliasing), the agent is uncertain about the true state of the environment.32Example: Consider a robot navigating a building. There are two rooms, s1​ and s2​, that look identical. The robot's observation is simply "in a room". If the robot needs to find an object located only in s2​, this observation is insufficient. However, by remembering its history of actions—e.g., "I was in the corridor, turned left, and moved forward"—the robot can infer that it is more likely to be in s1​ than s2​. An optimal policy must leverage this history to decide whether to search for the object or to leave the room and try another path.31In a POMDP, the history is necessary to form a better estimate of the true state. This estimate is called the belief state, bt​, which is a probability distribution over the state space S, where bt​(s)=P(St​=s∣Htobs​), with Htobs​ being the history of observations and actions. The belief state bt​ is a sufficient statistic for the history; the optimal policy for a POMDP is Markov with respect to the belief state.31 Since the belief state itself is a function of the entire history, the resulting policy is necessarily history-dependent with respect to the raw observations.8. Stochasticity vs. DeterminismRandomness in an MDP can arise from two distinct sources: the environment and the agent's policy.Stochastic Dynamics (Environment): The transition kernel P(s′∣s,a) may be probabilistic. For example, a robot attempting to move forward may have a 90% chance of succeeding and a 10% chance of slipping and staying in place. This randomness is inherent to the environment and is outside the agent's control.9 An MDP with a transition function where for each (s,a), the next state s′ is uniquely determined, is said to have deterministic dynamics.8Stochastic Policy (Agent): The policy π(a∣s) may be a probability distribution over actions. For example, in a game of Rock-Paper-Scissors, the optimal strategy is to choose each action with probability 1/3 to be unpredictable.34 This randomness is a choice made by the agent.A key result in the theory of standard discounted MDPs is that, despite the generality of stochastic policies, we do not need them to achieve optimality.Theorem 8.1 (Sufficiency of Deterministic Policies). For any discounted MDP with finite state and action spaces, there exists an optimal policy π∗ that is stationary, Markov, and deterministic.Proof Sketch. The proof relies on the structure of the optimal value functions. Recall that the optimal state-value function is related to the optimal action-value function by v∗(s)=maxa∈A​q∗(s,a). This maximization implies that for any state s, there is at least one action a∗ that achieves this maximum. A deterministic policy can be constructed by simply selecting one such maximizing action for each state:π∗(s)=arga∈Amax​q∗(s,a)Any policy constructed this way is called a greedy policy with respect to q∗. The Fundamental Theorem of MDPs (Section 15) will formally establish that any such policy is optimal.The reason a stochastic policy offers no advantage for optimality in this setting is that the value of a state is a linear combination of the Q-values of the actions taken from it: vπ(s)=∑a​π(a∣s)qπ(s,a). A convex combination of numbers can never exceed the maximum of those numbers. Therefore, concentrating all probability mass on the action with the highest Q-value is always an optimal strategy.35Stochastic policies are still critically important in reinforcement learning, but for reasons other than achieving a higher optimal value in a known MDP. Their primary roles are:Exploration: During learning, when the optimal values are unknown, stochasticity (e.g., in an ϵ-greedy policy) allows the agent to try actions that currently seem suboptimal, which may lead to the discovery of better strategies.21Partial Observability: As seen with POMDPs, if states are aliased, a stochastic policy can sometimes perform better than any deterministic one by hedging against uncertainty.Function Approximation: When using function approximation for policies (e.g., in policy gradient methods), stochastic policies often have smoother optimization landscapes, making them easier to train with gradient-based methods.379. Objective Functions / Reward FormulationsThe choice of how to aggregate rewards over time is a fundamental modeling decision that defines the nature of the problem and the behavior of the resulting optimal policy. The most common formulations are the discounted infinite-horizon, finite-horizon, and average-reward criteria.38Discounted Infinite-HorizonObjective: $J(\pi) = \mathbb{E}_\mu^\pi \left$, with $\gamma \in More importantly, it makes the Bellman operators contraction mappings (as we will see in Section 12), which guarantees the existence of a unique optimal value function and the convergence of standard dynamic programming algorithms. However, the choice of γ can be seen as artificial and can bias the policy towards short-term gains, which may not be appropriate for all applications.39Finite-HorizonObjective: $J(\pi) = \mathbb{E}_\mu^\pi \left$, for a fixed, known horizon H.Discussion: This formulation is natural for episodic tasks where the interaction has a definite end point (e.g., a game with a fixed number of turns). The primary analytical difference is that the optimal policy is generally non-stationary; the best action in a state s depends on how many time steps are remaining, trem​=H−t. The solution is typically found via backward induction from the final time step.40Average-RewardObjective: $J(\pi) = \lim_{H \to \infty} \frac{1}{H} \mathbb{E}_\mu^\pi \left$.Discussion: This criterion is well-suited for continuing tasks where there is no natural notion of discounting and the goal is to optimize long-term, steady-state performance (e.g., managing a supply chain or a power grid).39 It avoids the selection of an artificial discount factor. However, the analysis is significantly more complex. The associated Bellman operators are not contractions, and proofs of convergence for learning algorithms often require stronger assumptions on the structure of the MDP, such as ergodicity (i.e., that the Markov chain induced by any policy has a single recurrent class).43The following table summarizes the key trade-offs:CriterionObjective FunctionHorizonKey PropertiesTypical Use CaseDiscounted$\mathbb{E}\left$InfiniteMathematically tractable; stationary optimal policy; Bellman operator is a contraction.Standard RL setting, problems with preference for near-term rewards.Finite-Horizon$\mathbb{E}\left$Finite (H)Optimal policy is non-stationary; solved with backward induction.Episodic tasks with a known, fixed length.Average-Reward$\lim_{H\to\infty} \frac{1}{H}\mathbb{E}\left$InfiniteNo discount factor; analysis is more complex, often requires ergodicity assumptions.Continuing tasks, optimizing for steady-state performance.10. Discounted Occupancy MeasuresValue functions characterize a policy by the expected return it generates. An alternative and powerful characterization is the discounted occupancy measure, which describes the frequency with which a policy visits each state-action pair.Definition 10.1 (Discounted Occupancy Measure). Given an MDP, an initial state distribution μ, and a policy π, the (unnormalized) discounted state-action occupancy measure uμπ​ is a measure on S×A defined for any measurable set C⊆S×A as:$$u_\mu^\pi(C) = \mathbb{E}_\mu^\pi \left$$where 1{⋅} is the indicator function. For finite spaces, this simplifies to a vector where each component is 22:uμπ​(s,a)=t=0∑∞​γtPμπ​(St​=s,At​=a)The total mass of this measure is ∑s,a​uμπ​(s,a)=∑t=0∞​γt=1/(1−γ).Intuitive Explanation. The occupancy measure uμπ​(s,a) represents the expected total discounted time that the agent spends in state s taking action a when starting from distribution μ and following policy π.Properties and SignificanceThe occupancy measure provides a powerful alternative perspective on policy optimization due to its properties.1. Value as a Linear Functional: The expected return (value) of a policy can be expressed as a simple inner product between the occupancy measure and the reward function.22$$v^\pi(\mu) = \mathbb{E}_\mu^\pi \left = \sum_{s,a} r(s,a) \sum_{t=0}^\infty \gamma^t P_\mu^\pi(S_t=s, A_t=a) = \sum_{s,a} r(s,a) u_\mu^\pi(s,a) = \langle u_\mu^\pi, r \rangle$$This recasts the problem of maximizing value as finding a policy whose occupancy measure has the largest projection onto the reward vector.2. Policy Optimization as a Linear Program: The set of all achievable occupancy measures for stationary Markov policies forms a convex set (a polytope for finite MDPs). The mapping from policies to occupancy measures is complex, but the set of valid occupancy measures itself is defined by a set of linear constraints known as the Bellman flow constraints.44 This means that the problem of finding the optimal policy can be transformed from a potentially non-convex optimization over policies into a linear program over occupancy measures:umax​s,a∑​u(s,a)r(s,a)subject to u being a valid occupancy measure.This transformation is a cornerstone of theoretical RL and provides a powerful tool for analysis and algorithm design.453. Sufficiency of Markov Policies: Occupancy measures provide an elegant way to prove the sufficiency of Markov policies (Theorem 7.1). It can be shown that for any history-dependent policy π and initial distribution μ, one can construct a stationary Markov policy π′ that generates the exact same occupancy measure, uμπ′​=uμπ​.22 Since the value depends only on the occupancy measure, this implies vπ′(μ)=vπ(μ). Thus, nothing is lost by restricting the search to Markov policies.11. Bellman Operators for a Fixed PolicyThe recursive nature of value functions gives rise to a set of fundamental equations known as the Bellman equations. These equations can be expressed compactly using Bellman operators, which are central to the theory and algorithms of dynamic programming. We begin with the operator for a fixed policy π.Definition 11.1 (Bellman Expectation Operator). The Bellman expectation operator for a policy π, denoted Tπ, is a mapping from the space of value functions to itself. For any value function v:S→R, the updated value function Tπv is defined for each state s∈S as 47:(Tπv)(s)≐Ea∼π(⋅∣s)​[r(s,a)+γEs′∼P(⋅∣s,a)​[v(s′)]]This operator computes the expected value of a one-step lookahead. It takes the current value estimates for all states, v, and produces a new set of estimates by considering the immediate reward and the discounted value of the next state, averaged over the policy and the environment dynamics.For finite state and action spaces, this operator can be written in vector form:Tπv=rπ+γPπvwhere v∈R∣S∣ is the value function vector, rπ∈R∣S∣ is the vector of expected rewards under policy π (rπ(s)=Ea∼π(⋅∣s)​[r(s,a)]), and Pπ is the ∣S∣×∣S∣ transition probability matrix induced by the policy (Pπ(s′∣s)=Ea∼π(⋅∣s)​[P(s′∣s,a)]).49The Policy Evaluation EquationThe Bellman operator provides a concise way to state the self-consistency condition that the true value function vπ must satisfy.Theorem 11.1 (Bellman Equation for a Policy). The state-value function vπ for a policy π is the unique fixed point of the Bellman expectation operator Tπ. That is, vπ is the unique solution to the equation:vπ=TπvπThis is the policy evaluation equation, a system of ∣S∣ linear equations for finite MDPs 50:vπ(s)=a∈A∑​π(a∣s)(r(s,a)+γs′∈S∑​P(s′∣s,a)vπ(s′))∀s∈SThe existence and uniqueness of this fixed point are guaranteed by the properties of the operator Tπ, which we establish in the next section. This theorem is fundamental because it provides a way to compute the value of any given policy, a sub-problem known as policy evaluation.12. γ-Contraction & Fixed-Point IterationThe reason the policy evaluation equation has a unique solution, and the reason iterative algorithms converge to it, is due to a key property of the Bellman operator: it is a contraction mapping. This property allows us to bring the powerful tools of functional analysis, specifically the Banach fixed-point theorem, to bear on the problem.Theorem 12.1 (Banach Fixed-Point Theorem). Let (X,d) be a non-empty complete metric space. Let T:X→X be a contraction mapping, i.e., there exists a constant $\gamma \inTo apply this theorem, we consider the space of bounded real-valued functions on S, denoted B(S), equipped with the infinity norm ∥v∥∞​=sups∈S​∣v(s)∣. This forms a complete metric space (a Banach space).Theorem 12.2 (Tπ is a γ-Contraction). The Bellman expectation operator Tπ is a γ-contraction mapping on B(S) with respect to the infinity norm.Proof. Let u,v∈B(S) be two value functions. We examine the distance between their images under Tπ:\begin{align*}|T^\pi u - T^\pi v|\infty &= \sup{s \in \mathcal{S}} \left| (T^\pi u)(s) - (T^\pi v)(s) \right| \&= \sup_{s \in \mathcal{S}} \left| \left( \mathbb{E}\pi \right) - \left( \mathbb{E}\pi \right) \right| \&= \sup_{s \in \mathcal{S}} \left| \gamma , \mathbb{E}\pi \right| \&\le \gamma \sup{s \in \mathcal{S}} \mathbb{E}\pi \quad \text{(by Jensen's inequality)} \&\le \gamma \sup{s \in \mathcal{S}} \mathbb{E}\pi \&= \gamma \sup{s \in \mathcal{S}} \mathbb{E}\pi \&= \gamma |u - v|\infty\end{align*}The expectation of the constant ∥u−v∥∞​ is just the constant itself. Thus, Tπ is a γ-contraction.52Convergence of Policy Evaluation and Error BoundsThe contraction property of Tπ has immediate and powerful consequences for the policy evaluation algorithm, which iteratively applies this operator: vk+1​←Tπvk​.Corollary 12.1 (Convergence of Policy Evaluation). For any finite MDP and any initial value function v0​, the sequence of value functions {vk​} generated by iterative policy evaluation converges to the unique true value function vπ.This result justifies the use of iterative methods to solve the Bellman equation, which is often more computationally efficient than direct matrix inversion, especially for large state spaces. The rate of convergence is geometric, as given by the following error bounds.Theorem 12.3 (Error Bounds for Policy Evaluation). Let {vk​} be the sequence of value functions generated by iterative policy evaluation starting from v0​. The error at iteration k is bounded by:∥vk​−vπ∥∞​≤γk∥v0​−vπ∥∞​∥vk​−vπ∥∞​≤1−γγk​∥v1​−v0​∥∞​Proof Sketch. The first bound follows directly from repeated application of the contraction property: ∥vk​−vπ∥∞​=∥(Tπ)kv0​−(Tπ)kvπ∥∞​≤γk∥v0​−vπ∥∞​. The second bound, which is more practical as it does not depend on the unknown vπ, can be derived using the triangle inequality and the sum of a geometric series.5513. Bellman Optimality OperatorWhile the Bellman expectation operator Tπ is used to evaluate a given policy, the Bellman optimality operator T is used to find the optimal value function v∗ directly. It captures the core logic of Bellman's principle of optimality: an optimal policy must make an optimal choice at the first step, leading to a situation from which an optimal policy is followed thereafter.57Definition 13.1 (Bellman Optimality Operator). The Bellman optimality operator, denoted T, maps a value function v to a new value function Tv. For any value function v:S→R, the updated value function Tv is defined for each state s∈S as 49:(Tv)(s)≐a∈Amax​{r(s,a)+γEs′∼P(⋅∣s,a)​[v(s′)]}In vector form for finite spaces, this is (Tv)(s)=maxa∈A​{ra​(s)+γ(Pa​v)(s)}, where Pa​ is the transition matrix for action a.The key difference between T and Tπ is the replacement of the policy-weighted average over actions (Ea∼π(⋅∣s)​) with a maximization (maxa∈A​). This makes the Bellman optimality operator non-linear. Despite this, it retains the crucial properties of monotonicity and contraction.Theorem 13.1 (Properties of the Bellman Optimality Operator). The Bellman optimality operator T has the following properties:Monotonicity: If u≤v (i.e., u(s)≤v(s) for all s), then Tu≤Tv.γ-Contraction: T is a γ-contraction in the infinity norm: ∥Tu−Tv∥∞​≤γ∥u−v∥∞​.Proof of Contraction. Let u,v∈B(S). For any state s, let a∗ be an action that achieves the maximum for (Tu)(s). Then:\begin{align*}(Tu)(s) - (Tv)(s) &= (r(s,a^) + \gamma \mathbb{E}) - \max_{a} (r(s,a) + \gamma \mathbb{E}) \&\le (r(s,a^) + \gamma \mathbb{E}) - (r(s,a^) + \gamma \mathbb{E}) \&= \gamma , \mathbb{E} \&\le \gamma , \mathbb{E}[|u - v|_\infty | s,a^] = \gamma |u - v|\infty\end{align*}By swapping the roles of u and v, we can show $(Tv)(s) - (Tu)(s) \le \gamma |u - v|\infty$. Combining these gives ∣(Tu)(s)−(Tv)(s)∣≤γ∥u−v∥∞​. Since this holds for all s, taking the supremum over s yields the result.54The algorithm of Value Iteration is simply the repeated application of this operator: vk+1​←Tvk​. The contraction property guarantees that this process converges to a unique fixed point, which, as we will see, is the optimal value function v∗.14. Greedy Policies w.r.t. a Value FunctionThe Bellman operators provide a way to evaluate policies and find optimal values. The concept of a greedy policy provides the mechanism to improve policies and connect value functions back to actions.Definition 14.1 (Greedy Policy). A policy π is said to be greedy with respect to a value function v:S→R if, for every state s∈S, it deterministically selects an action that maximizes the one-step lookahead value 22:π(s)∈arga∈Amax​{r(s,a)+γs′∈S∑​P(s′∣s,a)v(s′)}In operator notation, a policy π is greedy with respect to v if it selects actions that achieve the maximum in the definition of the Bellman optimality operator T.This leads to a crucial characterization that connects the two types of Bellman operators.Proposition 14.1 (Characterization of Greedy Policies). A stationary Markov policy π is greedy with respect to a value function v if and only if its Bellman expectation operator, when applied to v, is equal to the Bellman optimality operator applied to v. That is:Tπv=TvIntuitive Explanation. A greedy policy is one that acts with one-step foresight. It looks at the current value estimates v(s′) for all possible next states, and chooses the action that leads to the best combination of immediate reward and discounted future value. This is an act of pure exploitation of the current value function.36 The central idea behind policy iteration is that acting greedily with respect to the value function of a suboptimal policy will always lead to a new policy that is strictly better (or equally good), until the optimal policy is reached.15. Fundamental Theorem of MDPs & MonotonicityWe now arrive at the culminating result of this tutorial, the Fundamental Theorem of MDPs. This theorem ties together the concepts of optimal value functions, Bellman optimality operators, and greedy policies, providing the theoretical justification for the core dynamic programming algorithms: Value Iteration and Policy Iteration.First, we formally state the monotonicity property of the Bellman operators, which is a key lemma for the main proof.Theorem 15.1 (Monotonicity of Bellman Operators). The Bellman expectation operator Tπ and the Bellman optimality operator T are both monotone. That is, for any two value functions u,v∈B(S), if u(s)≤v(s) for all s∈S, then:(Tπu)(s)≤(Tπv)(s) for all s∈S.(Tu)(s)≤(Tv)(s) for all s∈S.Proof.For Tπ:(Tπv)(s)−(Tπu)(s)=γEπ​. Since v≥u, the term v(s′)−u(s′) is non-negative for all s′. The expectation of a non-negative random variable is non-negative, so (Tπv)(s)−(Tπu)(s)≥0.For T: Let a∗=argmaxa​{r(s,a)+γE}.(Tv)(s)=r(s,a∗)+γE≥r(s,a∗)+γE.And (Tu)(s)=maxa​{r(s,a)+γE}. The action a∗ is a candidate for the max in (Tu)(s), so (Tu)(s)≥r(s,a∗)+γE. This line of reasoning is incomplete. A cleaner proof is:Let u≤v. Then for any s,a, r(s,a)+γE≤r(s,a)+γE. Taking the maximum over a on both sides preserves the inequality: maxa​{…u…}≤maxa​{…v…}, which means (Tu)(s)≤(Tv)(s).60With this property, we can state and prove the main theorem.Theorem 15.2 (Fundamental Theorem of MDPs). For any discounted MDP with finite state and action spaces:There exists a unique optimal value function v∗ which is the unique fixed point of the Bellman optimality operator T. That is, v∗ is the unique solution to the Bellman optimality equation:v∗=Tv∗A policy π is an optimal policy if and only if it is greedy with respect to v∗.There always exists an optimal policy that is stationary and deterministic.Proof.Existence and Uniqueness of the Fixed Point: From Theorem 13.1, the Bellman optimality operator T is a γ-contraction on the Banach space (B(S),∥⋅∥∞​). By the Banach Fixed-Point Theorem (Theorem 12.1), T has a unique fixed point. Let's call this fixed point v~. So, v~=Tv~. We need to show that this fixed point is indeed the optimal value function, i.e., v~=v∗.Let π be any policy. By definition, vπ=Tπvπ. Since for any state s, the maximization in (Tvπ)(s) includes the action (or distribution of actions) taken by π, we have (Tπvπ)(s)≤(Tvπ)(s). Thus, vπ≤Tvπ.Subtracting v~ from both sides and using the contraction property of T: vπ−v~≤Tvπ−Tv~. Taking norms, ∥vπ−v~∥∞​≤∥Tvπ−Tv~∥∞​≤γ∥vπ−v~∥∞​. Since γ<1, this implies ∥vπ−v~∥∞​=0, which means vπ=v~. This seems incorrect. Let's follow the proof from the literature.22Let's prove v~=v∗. First, for any policy π, vπ=Tπvπ≤Tvπ. Applying the operator (I−γPπ)−1 (which is monotone) to both sides of vπ≤Tvπ is not helpful.Let's try another way. Let π∗ be an optimal policy. Then v∗=vπ∗=Tπ∗v∗≤Tv∗. Now let π′ be a policy greedy with respect to v∗. Then Tπ′v∗=Tv∗. Combining these, we get v∗≤Tπ′v∗. By monotonicity of Tπ′, applying it repeatedly gives v∗≤Tπ′v∗≤(Tπ′)2v∗≤…. As k→∞, (Tπ′)kv∗→vπ′. So v∗≤vπ′. But by definition of v∗, vπ′≤v∗. Thus, vπ′=v∗, meaning π′ is an optimal policy. Also, since v∗=vπ′, we have v∗=Tπ′v∗=Tv∗. This shows that v∗ is a fixed point of T. Since the fixed point is unique, v∗ is the unique fixed point. This proves part 1 and the "if" part of part 2 (if a policy is greedy w.r.t. v∗, it is optimal).Proof of Part 2 ("only if"): We need to show that if a policy π is optimal, it must be greedy with respect to v∗.If π is optimal, then vπ=v∗.We know vπ=Tπvπ. Substituting v∗, we get v∗=Tπv∗.We also know from part 1 that v∗=Tv∗.So, Tπv∗=Tv∗. By the characterization of greedy policies (Proposition 14.1), this means π is greedy with respect to v∗.Proof of Part 3: Since v∗ exists, we can always construct a greedy policy by taking the argmax at each state. Since the action space is finite, such a maximum always exists. This constructs a stationary deterministic policy. By part 2, this policy is optimal.Table of NotationSymbolDefinitionFirst Used(Ω,F,P)A probability space, with sample space Ω, σ-algebra F, and probability measure P.Section 1(S,FS​)The measurable state space.Section 2(A,FA​)The measurable action space.Section 2$P(s's,a)$Transition probability kernel of the MDP.r(s,a)Reward function.Section 2γDiscount factor, γ∈[0,1).Section 2Ht​History of states and actions up to time t.Section 3πA policy, a mapping from histories or states to distributions over actions.Section 3Pμπ​The unique probability measure over trajectories induced by initial distribution μ and policy π.Section 4Gt​The return (discounted sum of future rewards) from time t.Section 5vπ(s)The state-value function for policy π.Section 6qπ(s,a)The action-value function for policy π.Section 6v∗(s),q∗(s,a)The optimal state-value and action-value functions.Section 6π∗An optimal policy.Section 6uμπ​(s,a)The discounted state-action occupancy measure.Section 10TπThe Bellman expectation operator for policy π.Section 11TThe Bellman optimality operator.Section 13$\|\cdot\|_\inf$The infinity norm (or sup norm) for functions, $|f|_\infty = \sup_xf(x)
