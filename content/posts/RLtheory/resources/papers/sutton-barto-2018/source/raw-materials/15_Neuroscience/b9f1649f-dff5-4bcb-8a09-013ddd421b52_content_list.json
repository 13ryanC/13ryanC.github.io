[
    {
        "type": "text",
        "text": "Chapter 15",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Neuroscience",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily functions; control behavior; change over time as a result of development, learning, and aging; and how cellular and molecular mechanisms make these functions possible. One of the most exciting aspects of reinforcement learning is the mounting evidence from neuroscience that the nervous systems of humans and many other animals implement algorithms that correspond in striking ways to reinforcement learning algorithms. The main objective of this chapter is to explain these parallels and what they suggest about the neural basis of reward- related learning in animals.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The most remarkable point of contact between reinforcement learning and neuroscience involves dopamine, a chemical deeply involved in reward processing in the brains of mammals. Dopamine appears to convey temporal- difference (TD) errors to brain structures where learning and decision making take place. This parallel is expressed by the reward prediction error hypothesis of dopamine neuron activity, a hypothesis that resulted from the convergence of computational reinforcement learning and results of neuroscience experiments. In this chapter we discuss this hypothesis, the neuroscience findings that led to it, and why it is a significant contribution to understanding brain reward systems. We also discuss parallels between reinforcement learning and neuroscience that are less striking than this dopamine/TD- error parallel but that provide useful conceptual tools for thinking about reward- based learning in animals. Other elements of reinforcement learning have the potential to impact the study of nervous systems, but their connections to neuroscience are still relatively undeveloped. We discuss several of these evolving connections that we think will grow in importance over time.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As we outlined in the history section of this book's introductory chapter (Section 1.7), many aspects of reinforcement learning were influenced by neuroscience. A second objective of this chapter is to acquaint readers with ideas about brain function that have contributed to our approach to reinforcement learning. Some elements of reinforcement learning are easier to understand when seen in light of theories of brain function. This is particularly true for the idea of the eligibility trace, one of the basic mechanisms of reinforcement learning, that originated as a conjectured property of synapses, the structures by which nerve cells—neurons—communicate with one another.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this chapter we do not delve very deeply into the enormous complexity of the neural systems underlying reward- based learning in animals: this chapter is too short, and we are not neuroscientists. We do not try to describe—or even to name—the very many brain structures and pathways, or any of the molecular mechanisms, believed to be involved in these processes. We also do not do justice to hypotheses and models that are alternatives to those that align so well with reinforcement learning. It should not be surprising that there are differing views among experts in the field. We can only provide a glimpse into this fascinating and developing story. We hope, though, that this chapter convinces you that a very fruitful channel has emerged connecting reinforcement learning and its theoretical underpinnings to the neuroscience of reward- based learning in animals.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Many excellent publications cover links between reinforcement learning and neuroscience, some of which we cite in this chapter's final section. Our treatment differs from most of these because we assume familiarity with reinforcement learning as presented in the earlier chapters of this book, but we do not assume knowledge of neuroscience. We begin with a brief introduction to the neuroscience concepts needed for a basic understanding of what is to follow.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "15.1 Neuroscience Basics",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Some basic information about nervous systems is helpful for following what we cover in this chapter. Terms that we refer to later are italicized. Skipping this section will not be a problem if you already have an elementary knowledge of neuroscience.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Neurons, the main components of nervous systems, are cells specialized for processing and transmitting information using electrical and chemical signals. They come in many forms, but a neuron typically has a cell body, dendrites, and a single axon. Dendrites are structures that branch from the cell body to receive input from other neurons (or to also receive external signals in the case of sensory neurons). A neuron's axon is a fiber that carries the neuron's output to other neurons (or to muscles or glands). A neuron's output consists of sequences of electrical pulses called action potentials that travel along the axon. Action potentials are also called spikes, and a neuron is said to fire when it generates a spike. In models of neural networks it is common to use real numbers to represent a neuron's firing rate, the average number of spikes per some unit of time.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A neuron's axon can branch widely so that the neuron's action potentials reach many targets. The branching structure of a neuron's axon is called the neuron's axonal arbor. Because the conduction of an action potential is an active process, not unlike the burning of a fuse, when an action potential reaches an axonal branch point it \"lights up\" action potentials on all of the outgoing branches (although propagation to a branch can sometimes fail). As a result, the activity of a neuron with a large axonal arbor can influence many target sites.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A synapse is a structure generally at the termination of an axon branch that mediates the communication of one neuron to another. A synapse transmits information from the presynaptic neuron's axon to a dendrite or cell body of the postsynaptic neuron. With a few exceptions, synapses release a chemical neurotransmitter upon the arrival of an action potential from the presynaptic neuron. (The exceptions are cases of direct electric coupling between neurons, but these will not concern us here.) Neurotransmitter molecules released from the presynaptic side of the synapse diffuse across the synaptic cleft, the very small space between the presynaptic ending and the postsynaptic neuron, and then bind to receptors on the surface of the postsynaptic neuron to excite or inhibit its spike- generating activity, or to modulate its behavior in other ways. A particular neurotransmitter may bind to several different types of receptors, with each producing a different effect on the postsynaptic neuron. For example, there are at least five different receptor types by which the neurotransmitter dopamine can affect a postsynaptic neuron. Many different chemicals have been identified as neurotransmitters in animal nervous systems.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "A neuron's background activity is its level of activity, usually its firing rate, when the neuron does not appear to be driven by synaptic input related to the task of interest to the experimenter, for example, when the neuron's activity is not correlated with a stimulus delivered to a subject as part of an experiment. Background activity can be irregular due to input from the wider network, or due to noise within the neuron or its synapses. Sometimes background activity is the result of dynamic processes intrinsic to the neuron. A neuron's phasic activity, in contrast to its background activity, consists of bursts of spiking activity usually caused by synaptic input. Activity that varies slowly and often in a graded manner, whether as background activity or not, is called a neuron's tonic activity.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The strength or effectiveness by which the neurotransmitter released at a synapse influences the postsynaptic neuron is the synapse's efficacy. One way a nervous system can change through experience is through changes in synaptic efficacies as a result of combinations of the activities of the presynaptic and postsynaptic neurons, and sometimes by the presence of a neuromodulator, which is a neurotransmitter having effects other than, or in addition to, direct fast excitation or inhibition.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Brains contain several different neuromodulation systems consisting of clusters of neurons with widely branching axonal arbors, with each system using a different neurotransmitter. Neuromodulation can alter the function of neural circuits, mediate motivation, arousal, attention, memory, mood, emotion, sleep, and body temperature. Important here is that a neuromodulatory system can distribute something like a scalar signal, such as a reinforcement signal, to alter the operation of synapses in widely distributed sites critical for learning.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The ability of synaptic efficacies to change is called synaptic plasticity. It is one of the primary mechanisms responsible for learning. The parameters, or weights, adjusted by learning algorithms correspond to synaptic efficacies. As we detail below, modulation of synaptic plasticity via the neuromodulator dopamine is a plausible mechanism for how the brain might implement learning algorithms like many of those described in this book.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "15.2 Reward Signals, Reinforcement Signals, Values, and Prediction Errors",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Links between neuroscience and computational reinforcement learning begin as parallels between signals in the brain and signals playing prominent roles in reinforcement learning theory and algorithms. In Chapter 3 we said that any problem of learning goal- directed behavior can be reduced to the three signals representing actions, states, and rewards. However, to explain links that have been made between neuroscience and reinforcement learning, we have to be less abstract than this and consider other reinforcement learning signals that correspond, in certain ways, to signals in the brain. In addition to reward signals, these include reinforcement signals (which we argue are different from reward signals), value signals, and signals conveying prediction errors. When we label a signal by its function in this way, we are doing it in the context of reinforcement learning theory in which the signal corresponds to a term in an equation or an algorithm. On the other hand, when we refer to a signal in the brain, we mean a physiological event such as a burst of action potentials or the secretion of a neurotransmitter. Labeling a neural signal by its function, for example calling the phasic activity of a dopamine neuron a reinforcement signal, means that the neural signal behaves like, and is conjectured to function like, the corresponding theoretical signal.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Uncovering evidence for these correspondences involves many challenges. Neural activity related to reward processing can be found in nearly every part of the brain, and it is difficult to interpret results unambiguously because representations of different reward- related signals tend to be highly correlated with one another. Experiments need to be carefully designed to allow one type of reward- related signal to be distinguished with any degree of certainty from others- or from an abundance of other signals not related to reward processing. Despite these difficulties, many experiments have been conducted with the aim of reconciling aspects of reinforcement learning theory and algorithms with neural signals, and some compelling links have been established. To prepare for examining these links, in the rest of this section we remind the reader of what various reward- related signals mean according to reinforcement learning theory.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "In our Comments on Terminology at the end of the previous chapter, we said that  $R_{t}$  is like a reward signal in an animal's brain and not an object or event in the animal's environment. In reinforcement learning, the reward signal (along with an agent's environment) defines the problem a reinforcement learning agent is trying to solve. In this respect,  $R_{t}$  is like a signal in an animal's brain that distributes primary reward to sites throughout the brain. But it is unlikely that a unitary master reward signal like  $R_{t}$  exists in an animal's brain. It is best to think of  $R_{t}$  as an abstraction summarizing the overall effect of a multitude of neural signals generated by many systems in the brain that assess the rewarding or punishing qualities of sensations and states.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Reinforcement signals in reinforcement learning are different from reward signals. The function of a reinforcement signal is to direct the changes a learning algorithm makes in an agent's policy, value estimates, or environment models. For a TD method, for instance, the reinforcement signal at time  $t$  is the TD error  $\\delta_{t - 1} = R_{t} + \\gamma V(S_{t}) - V(S_{t - 1})$ . The",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "reinforcement signal for some algorithms could be just the reward signal, but for most of the algorithms we consider the reinforcement signal is the reward signal adjusted by other information, such as the value estimates in TD errors.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Estimates of state values or of action values, that is,  $V$  or  $Q$ , specify what is good or bad for the agent over the long run. They are predictions of the total reward an agent can expect to accumulate over the future. Agents make good decisions by selecting actions leading to states with the largest estimated state values, or by selecting actions with the largest estimated action values.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Prediction errors measure discrepancies between expected and actual signals or sensations. Reward prediction errors (RPEs) specifically measure discrepancies between the expected and the received reward signal, being positive when the reward signal is greater than expected, and negative otherwise. TD errors like (6.5) are special kinds of RPEs that signal discrepancies between current and earlier expectations of reward over the long- term. When neuroscientists refer to RPEs they generally (though not always) mean TD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter, a TD error is generally one that does not depend on actions, as opposed to TD errors used in learning action- values by algorithms like Sarsa and Q- learning. This is because the most well- known links to neuroscience are stated in terms of action- free TD errors, but we do not mean to rule out possible similar links involving action- dependent TD errors. (TD errors for predicting signals other than rewards are useful too, but that case will not concern us here. See, for example, Modayil, White, and Sutton, 2014. )",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "One can ask many questions about links between neuroscience data and these theoretically- defined signals. Is an observed signal more like a reward signal, a value signal, a prediction error, a reinforcement signal, or something altogether different? And if it is an error signal, is it an RPE, a TD error, or a simpler error like the Rescorla- Wagner error (14.3)? And if it is a TD error, does it depend on actions like the TD error of Q- learning or Sarsa? As indicated above, probing the brain to answer questions like these is extremely difficult. But experimental evidence suggests that one neurotransmitter, specifically the neurotransmitter dopamine, signals RPEs, and further, that the phasic activity of dopamine- producing neurons in fact conveys TD errors (see Section 15.1 for a definition of phasic activity). This evidence led to the reward prediction error hypothesis of dopamine neuron activity, which we describe next.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "15.3 The Reward Prediction Error Hypothesis",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The reward prediction error hypothesis of dopamine neuron activity proposes that one of the functions of the phasic activity of dopamine- producing neurons in mammals is to deliver an error between an old and a new estimate of expected future reward to target areas throughout the brain. This hypothesis (though not in these exact words) was first explicitly stated by Montague, Dayan, and Sejnowski (1996), who showed how the TD error concept from reinforcement learning accounts for many features of the phasic",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "activity of dopamine neurons in mammals. The experiments that led to this hypothesis were performed in the 1980s and early 1990s in the laboratory of neuroscientist Wolfram Schultz. Section 15.5 describes these influential experiments, Section 15.6 explains how the results of these experiments align with TD errors, and the Bibliographical and Historical Remarks section at the end of this chapter includes a guide to the literature surrounding the development of this influential hypothesis.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Montague et al. (1996) compared the TD errors of the TD model of classical conditioning with the phasic activity of dopamine- producing neurons during classical conditioning experiments. Recall from Section 14.2 that the TD model of classical conditioning is basically the semi- gradient- descent TD( $\\lambda$ ) algorithm with linear function approximation. Montague et al. made several assumptions to set up this comparison. First, because a TD error can be negative but neurons cannot have a negative firing rate, they assumed that the quantity corresponding to dopamine neuron activity is  $\\delta_{t - 1} + b_t$ , where  $b_t$  is the background firing rate of the neuron. A negative TD error corresponds to a drop in a dopamine neuron's firing rate below its background rate.<sup>2</sup>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "A second assumption was needed about the states visited in each classical conditioning trial and how they are represented as inputs to the learning algorithm. This is the same issue we discussed in Section 14.2.4 for the TD model. Montague et al. show a complete serial compound (CSC) representation as shown in the left column of Figure 14.1, but where the sequence of short- duration internal signals continues until the onset of the US, which here is the arrival of a non- zero reward signal. This representation allows the TD error to mimic the fact that dopamine neuron activity not only predicts a future reward, but that it is also sensitive to when after a predictive cue that reward is expected to arrive. There has to be some way to keep track of the time between sensory cues and the arrival of reward. If a stimulus initiates a sequence of internal signals that continues after the stimulus ends, and if there is a different signal for each time step following the stimulus, then each time step after the stimulus is represented by a distinct state. Thus, the TD error, being state- dependent, can be sensitive to the timing of events within a trial.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In simulated trials with these assumptions about background firing rate and input representation, TD errors of the TD model are remarkably similar to dopamine neuron phasic activity. Previewing our description of details about these similarities in Section 15.5 below, the TD errors parallel the following features of dopamine neuron activity: (1) the phasic response of a dopamine neuron only occurs when a rewarding event is unpredicted; (2) early in learning, neutral cues that precede a reward do not cause substantial phasic dopamine responses, but with continued learning these cues gain predictive value and come to elicit phasic dopamine responses; (3) if an even earlier cue reliably precedes a cue that has already acquired predictive value, the phasic dopamine response shifts to the earlier cue, ceasing for the later cue; and (4) if after learning, the predicted rewarding event is omitted, a dopamine neuron's response decreases below its baseline level shortly after the expected time of the rewarding event.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Although not every dopamine neuron monitored in the experiments of Schultz and colleagues behaved in all of these ways, the striking correspondence between the activities of most of the monitored neurons and TD errors lends strong support to the reward prediction error hypothesis. There are situations, however, in which predictions based on the hypothesis do not match what is observed in experiments. The choice of input representation is critical to how closely TD errors match some of the details of dopamine neuron activity, particularly details about the timing of dopamine neuron responses. Different ideas, some of which we discuss below, have been proposed about input representations and other features of TD learning to make the TD errors fit the data better, though the main parallels appear with the CSC representation that Montague et al. used. Overall, the reward prediction error hypothesis has received wide acceptance among neuroscientists studying reward- based learning, and it has proven to be remarkably resilient in the face of accumulating results from neuroscience experiments.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "To prepare for our description of the neuroscience experiments supporting the reward prediction error hypothesis, and to provide some context so that the significance of the hypothesis can be appreciated, we next present some of what is known about dopamine, the brain structures it influences, and how it is involved in reward- based learning.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "15.4 Dopamine",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Dopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly in two clusters of neurons in the midbrain of mammals: the substantia nigra pars compacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essential roles in many processes in the mammalian brain. Prominent among these are motivation, learning, action- selection, most forms of addiction, and the disorders schizophrenia and Parkinson's disease. Dopamine is called a neuromodulator because it performs many functions other than direct fast excitation or inhibition of targeted neurons. Although much remains unknown about dopamine's functions and details of its cellular effects, it is clear that it is fundamental to reward processing in the mammalian brain. Dopamine is not the only neuromodulator involved in reward processing, and its role in aversive situations—punishment—remains controversial. Dopamine also can function differently in non- mammals. But no one doubts that dopamine is essential for reward- related processes in mammals, including humans.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "An early, traditional view is that dopamine neurons broadcast a reward signal to multiple brain regions implicated in learning and motivation. This view followed from a famous 1954 paper by James Olds and Peter Milner that described the effects of electrical stimulation on certain areas of a rat's brain. They found that electrical stimulation to particular regions acted as a very powerful reward in controlling the rat's behavior: \"...the control exercised over the animal's behavior by means of this reward is extreme, possibly exceeding that exercised by any other reward previously used in animal experimentation\" (Olds and Milner, 1954). Later research revealed that the sites at which stimulation was most effective in producing this rewarding effect excited dopamine pathways, either directly or indirectly, that ordinarily are excited by natural rewarding stimuli. Effects similar to these were also observed with human subjects. These observations strongly suggested that dopamine neuron activity signals reward.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "But if the reward prediction error hypothesis is correct—even if it accounts for only some features of a dopamine neuron's activity—this traditional view of dopamine neuron activity is not entirely correct: phasic responses of dopamine neurons signal reward prediction errors, not reward itself. In reinforcement learning's terms, a dopamine neuron's phasic response at a time  $t$  corresponds to  $\\delta_{t - 1} = R_t + \\gamma V(S_t) - V(S_{t - 1})$ , not to  $R_t$ .",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Reinforcement learning theory and algorithms help reconcile the reward- prediction- error view with the conventional notion that dopamine signals reward. In many of the algorithms we discuss in this book,  $\\delta$  functions as a reinforcement signal, meaning that it is the main driver of learning. For example,  $\\delta$  is the critical factor in the TD model of classical conditioning, and  $\\delta$  is the reinforcement signal for learning both a value function and a policy in an actor- critic architecture (Sections 13.5 and 15.7). Action- dependent forms of  $\\delta$  are reinforcement signals for  $Q$ - learning and Sarsa. The reward signal  $R_t$  is a crucial component of  $\\delta_{t - 1}$ , but it is not the complete determinant of its reinforcing effect in these algorithms. The additional term  $\\gamma V(S_t) - V(S_{t - 1})$  is the higher- order reinforcement part of  $\\delta_{t - 1}$ , and even if reward occurs ( $R_t \\neq 0$ ), the TD error can be silent if the reward is fully predicted (which is fully explained in Section 15.6 below).",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "A closer look at Olds' and Milner's 1954 paper, in fact, reveals that it is mainly about the reinforcing effect of electrical stimulation in an instrumental conditioning task. Electrical stimulation not only energized the rats' behavior—through dopamine's effect on motivation—it also led to the rats quickly learning to stimulate themselves by pressing a lever, which they would do frequently for long periods of time. The activity of dopamine neurons triggered by electrical stimulation reinforced the rats' lever pressing.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "More recent experiments using optogenetic methods clinch the role of phasic responses of dopamine neurons as reinforcement signals. These methods allow neuroscientists to precisely control the activity of selected neuron types at a millisecond timescale in awake behaving animals. Optogenetic methods introduce light- sensitive proteins into selected neuron types so that these neurons can be activated or silenced by means of flashes of laser light. The first experiment using optogenetic methods to study dopamine neurons showed that optogenetic stimulation producing phasic activation of dopamine neurons in mice was enough to condition the mice to prefer the side of a chamber where they received this stimulation as compared to the chamber's other side where they received no, or lower- frequency, stimulation (Tsai et al. 2009). In another example, Steinberg et al. (2013) used optogenetic activation of dopamine neurons to create artificial bursts of dopamine neuron activity in rats at the times when rewarding stimuli were expected but omitted—times when dopamine neuron activity normally pauses. With these pauses replaced by artificial bursts, responding was sustained when it would ordinarily decrease due to lack of reinforcement (in extinction trials), and learning was enabled when it would ordinarily be blocked due to the reward being already predicted (the blocking paradigm; Section 14.2.1).",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Additional evidence for the reinforcing function of dopamine comes from optogenetic experiments with fruit flies, except in these animals dopamine's effect is the opposite of its effect in mammals: optically triggered bursts of dopamine neuron activity act just like electric foot shock in reinforcing avoidance behavior, at least for the population",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "of dopamine neurons activated (Claridge- Chang et al. 2009). Although none of these optogenetic experiments showed that phasic dopamine neuron activity is specifically like a TD error, they convincingly demonstrated that phasic dopamine neuron activity acts just like  $\\delta$  acts (or perhaps like minus  $\\delta$  acts in fruit flies) as the reinforcement signal in algorithms for both prediction (classical conditioning) and control (instrumental conditioning).",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Dopamine neurons are particularly well suited to broadcasting a reinforcement signal to many areas of the brain. These neurons have huge axonal arbors, each releasing dopamine at 100 to 1,000 times more synaptic sites than reached by the axons of typical neurons. Shown to the right is the axonal arbor of a single dopamine neuron whose cell body is in the SNpc of a rat's brain. Each axon of a SNpc or VTA dopamine neuron makes roughly 500,000 synaptic contacts on the dendrites of neurons in targeted brain areas.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "If dopamine neurons broadcast a reinforcement signal like reinforcement learning's  $\\delta$ , then because this is a scalar signal, i.e., a single number, all dopamine neurons in both the SNpc and VTA would be expected to activate more or- less identically so that they would act in near synchrony to send the same signal to all of the sites their axons target. Although it has been a common belief that dopamine neurons do act together like this, modern evidence is pointing to the more complicated picture that different subpopulations of dopamine neurons respond to input differently depending on the structures to which they send their signals and the different Axonal arbor of a single neuron producing dopamine as a neurotransmitter. These axons make synaptic contacts with a huge number of dendrites of neurons in targeted brain areas.",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/547eb771406b0075cfb6b654dec5945f654d651496d4f6fe50c557eeafeb0671.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Adapted from The Journal of Neuroscience, Matsuda, Furuta, Nakamura, Hioki, Fujiyama, Arai, and Kaneko, volume 29, 2009, page 451.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "ways these signals act on their target structures. Dopamine has functions other than signaling RPEs, and even for dopamine neurons that do signal RPEs, it can make sense to send different RPEs to different structures depending on the roles these structures play in producing reinforced behavior. This is beyond what we treat in any detail in this book, but vector- valued RPE signals make sense from the perspective of reinforcement learning when decisions can be decomposed into separate sub- decisions, or more generally, as a way to address the structural version of the credit assignment problem: How do you distribute credit for success (or blame for failure) of a decision among the many component structures that could have been involved in producing it? We say a bit more about this in Section 15.10 below.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The axons of most dopamine neurons make synaptic contact with neurons in the frontal cortex and the basal ganglia, areas of the brain involved in voluntary movement, decision making, learning, and cognitive functions such as planning. Because most ideas relating",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "dopamine to reinforcement learning focus on the basal ganglia, and the connections from dopamine neurons are particularly dense there, we focus on the basal ganglia here. The basal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain. The main input structure of the basal ganglia is called the striatum. Essentially all of the cerebral cortex, among other structures, provides input to the striatum. The activity of cortical neurons conveys a wealth of information about sensory input, internal states, and motor activity. The axons of cortical neurons make synaptic contacts on the dendrites of the main input/output neurons of the striatum, called medium spiny neurons. Output from the striatum loops back via other basal ganglia nuclei and the thalamus to frontal areas of cortex, and to motor areas, making it possible for the striatum to influence movement, abstract decision processes, and reward processing. Two main subdivisions of the striatum are important for reinforcement learning: the dorsal striatum, primarily implicated in influencing action selection, and the ventral striatum, thought to be critical for different aspects of reward processing, including the assignment of affective value to sensations.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The dendrites of medium spiny neurons are covered with spines on whose tips the axons of neurons in the cortex make synaptic contact. Also making synaptic contact with these spines—in this case contracting the spine stems—are axons of dopamine neurons (Figure 15.1). This arrangement brings together presynaptic activity of cortical neurons,",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/7d5f023a0d976eecf8d6f4a896a39878cc598381599eaa1cad485a4adbcdb90e.jpg",
        "img_caption": [
            "Figure 15.1: Spine of a striatal neuron showing input from both cortical and dopamine neurons. Axons of cortical neurons influence striatal neurons via corticostriatal synapses releasing the neurotransmitter glutamate at the tips of spines covering the dendrites of striatal neurons. An axon of a VTA or SNpc dopamine neuron is shown passing by the spine (from the lower right). \"Dopamine varicosities\" on this axon release dopamine at or near the spine stem, in an arrangement that brings together presynaptic input from cortex, postsynaptic activity of the striatal neuron, and dopamine, making it possible that several types of learning rules govern the plasticity of corticostriatal synapses. Each axon of a dopamine neuron makes synaptic contact with the stems of roughly 500,000 spines. Some of the complexity omitted from our discussion is shown here by other neurotransmitter pathways and multiple receptor types, such as D1 an D2 dopamine receptors by which dopamine can produce different effects at spines and other postsynaptic sites. From Journal of Neurophysiology, W. Schultz, vol. 80, 1998, page 10."
        ],
        "img_footnote": [],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "postsynaptic activity of medium spiny neurons, and input from dopamine neurons. What actually occurs at these spines is complex and not completely understood. Figure 15.1 hints at the complexity by showing two types of receptors for dopamine, receptors for glutamate—the neurotransmitter of the cortical inputs—and multiple ways that the various signals can interact. But evidence is mounting that changes in the efficacies of the synapses on the pathway from the cortex to the striatum, which neuroscientists call corticostriatal synapses, depend critically on appropriately- timed dopamine signals.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "15.5 Experimental Support for the Reward Prediction Error Hypothesis",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Dopamine neurons respond with bursts of activity to intense, novel, or unexpected visual and auditory stimuli that trigger eye and body movements, but very little of their activity is related to the movements themselves. This is surprising because degeneration of dopamine neurons is a cause of Parkinson's disease, whose symptoms include motor disorders, particularly deficits in self- initiated movement. Motivated by the weak relationship between dopamine neuron activity and stimulus- triggered eye and body movements, Romo and Schultz (1990) and Schultz and Romo (1990) took the first steps toward the reward prediction error hypothesis by recording the activity of dopamine neurons and muscle activity while monkeys moved their arms.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "They trained two monkeys to reach from a resting hand position into a bin containing a bit of apple, a piece of cookie, or a raisin, when the monkey saw and heard the bin's door open. The monkey could then grab and bring the food to its mouth. After a monkey became good at this, it was trained on two additional tasks. The purpose of the first task was to see what dopamine neurons do when movements are self- initiated. The bin was left open but covered from above so that the monkey could not see inside but could reach in from below. No triggering stimuli were presented, and after the monkey reached for and ate the food morsel, the experimenter usually (though not always), silently and unseen by the monkey, replaced food in the bin by sticking it onto a rigid wire. Here too, the activity of the dopamine neurons Romo and Schultz monitored was not related to the monkey's movements, but a large percentage of these neurons produced phasic responses whenever the monkey first touched a food morsel. These neurons did not respond when the monkey touched just the wire or explored the bin when no food was there. This was good evidence that the neurons were responding to the food and not to other aspects of the task.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The purpose of Romo and Schultz's second task was to see what happens when movements are triggered by stimuli. This task used a different bin with a movable cover. The sight and sound of the bin opening triggered reaching movements to the bin. In this case, Romo and Schultz found that after some period of training, the dopamine neurons no longer responded to the touch of the food but instead responded to the sight and sound of the opening cover of the food bin. The phasic responses of these neurons had shifted from the reward itself to stimuli predicting the availability of the reward. In a followup study, Romo and Schultz found that most of the dopamine neurons whose activity they",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "monitored did not respond to the sight and sound of the bin opening outside the context of the behavioral task. These observations suggested that the dopamine neurons were responding neither to the initiation of a movement nor to the sensory properties of the stimuli, but were rather signaling an expectation of reward.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Schultz's group conducted many additional studies involving both SNpc and VTA dopamine neurons. A particular series of experiments was influential in suggesting that the phasic responses of dopamine neurons correspond to TD errors and not to simpler errors like those in the Rescorla- Wagner model (14.3). In the first of these experiments (Ljungberg, Apicella, and Schultz, 1992), monkeys were trained to depress a lever after a light was illuminated as a 'trigger cue' to obtain a drop of apple juice. As Romo and Schultz had observed earlier, many dopamine neurons initially responded to the reward—the drop of juice (Figure 15.2, top panel). But many of these neurons lost that reward response as training continued and developed responses instead to the illumination of the light that predicted the reward (Figure 15.2, middle panel). With continued training, lever pressing became faster while the number of dopamine neurons responding to the trigger cue decreased.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Following this study, the same monkeys were trained on a new task (Schultz, Apicella, and Ljungberg, 1993). Here the monkeys faced two levers, each with a light above it. Illuminating one of these lights was an 'instruction cue' indicating which of the two levers",
        "page_idx": 11
    },
    {
        "type": "image",
        "img_path": "images/b45cde8f9a22255da04a858f8fdd08d09bbb6f6e2c005f7f8211daca6a3b7ecd.jpg",
        "img_caption": [
            "Figure 15.2: The response of dopamine neurons shifts from initial responses to primary reward to earlier predictive stimuli. These are plots of the number of action potentials produced by monitored dopamine neurons within small time intervals, averaged over all the monitored dopamine neurons (ranging from 23 to 44 neurons for these data). Top: dopamine neurons are activated by the unpredicted delivery of drop of apple juice. Middle: with learning, dopamine neurons developed responses to the reward-predicting trigger cue and lost responsiveness to the delivery of reward. Bottom: with the addition of an instruction cue preceding the trigger cue by 1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction cue. From Schultz et al. (1995), MIT Press."
        ],
        "img_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "would produce a drop of apple juice. In this task, the instruction cue preceded the trigger cue of the previous task by a fixed interval of 1 second. The monkeys learned to withhold reaching until seeing the trigger cue, and dopamine neuron activity increased, but now the responses of the monitored dopamine neurons occurred almost exclusively to the earlier instruction cue and not to the trigger cue (Figure 15.2, bottom panel). Here again the number of dopamine neurons responding to the instruction cue was much reduced when the task was well learned. During learning across these tasks, dopamine neuron activity shifted from initially responding to the reward to responding to the earlier predictive stimuli, first progressing to the trigger stimulus then to the still earlier instruction cue. As responding moved earlier in time it disappeared from the later stimuli. This shifting of responses to earlier reward predictors, while losing responses to later predictors is a hallmark of TD learning (see, for example, Figure 14.2).",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "The task just described revealed another property of dopamine neuron activity shared with TD learning. The monkeys sometimes pressed the wrong key, that is, the key other than the instructed one, and consequently received no reward. In these trials, many of the dopamine neurons showed a sharp decrease in their firing rates below baseline shortly after the reward's usual time of delivery, and this happened without the availability of any external cue to mark the usual time of reward delivery (Figure 15.3). Somehow the",
        "page_idx": 12
    },
    {
        "type": "image",
        "img_path": "images/072ea006ddb12bbfa7d7807bc35d1a597148521b562964d81822f685c2be9ad3.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Figure 15.3: The response of dopamine neurons drops below baseline shortly after the time when an expected reward fails to occur. Top: dopamine neurons are activated by the unpredicted delivery of a drop of apple juice. Middle: dopamine neurons respond to a conditioned stimulus (CS) that predicts reward and do not respond to the reward itself. Bottom: when the reward predicted by the CS fails to occur, the activity of dopamine neurons drops below baseline shortly after the time the reward is expected to occur. At the top of each of these panels is shown the average number of action potentials produced by monitored dopamine neurons within small time intervals around the indicated times. The raster plots below show the activity patterns of the individual dopamine neurons that were monitored; each dot represents an action potential. From Schulz, Dayan, and Montague, A Neural Substrate of Prediction and Reward, Science, vol. 275, issue 5306, pages 1593- 1598, March 14, 1997. Reprinted with permission from AAAS.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "monkeys were internally keeping track of the timing of the reward. (Response timing is one area where the simplest version of TD learning needs to be modified to account for some of the details of the timing of dopamine neuron responses. We consider this issue in the following section.)",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The observations from the studies described above led Schultz and his group to conclude that dopamine neurons respond to unpredicted rewards, to the earliest predictors of reward, and that dopamine neuron activity decreases below baseline if a reward, or a predictor of reward, does not occur at its expected time. Researchers familiar with reinforcement learning were quick to recognize that these results are strikingly similar to how the TD error behaves as the reinforcement signal in a TD algorithm. The next section explores this similarity by working through a specific example in detail.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "15.6 TD Error/Dopamine Correspondence",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "This section explains the correspondence between the TD error  $\\delta$  and the phasic responses of dopamine neurons observed in the experiments just described. We examine how  $\\delta$  changes over the course of learning in a task something like the one described above where a monkey first sees an instruction cue and then a fixed time later has to respond correctly to a trigger cue in order to obtain reward. We use a simple idealized version of this task, but we go into a lot more detail than is usual because we want to emphasize the theoretical basis of the parallel between TD errors and dopamine neuron activity.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The first simplifying assumption is that the agent has already learned the actions required to obtain reward. Then its task is just to learn accurate predictions of future reward for the sequence of states it experiences. This is then a prediction task, or more technically, a policy- evaluation task: learning the value function for a fixed policy (Sections 4.1 and 6.1). The value function to be learned assigns to each state a value that predicts the return that will follow that state if the agent selects actions according to the given policy, where the return is the (possibly discounted) sum of all the future rewards. This is unrealistic as a model of the monkey's situation because the monkey would likely learn these predictions at the same time that it is learning to act correctly (as would a reinforcement learning algorithm that learns policies as well as value functions, such as an actor- critic algorithm), but this scenario is simpler to describe than one in which a policy and a value function are learned simultaneously.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Now imagine that the agent's experience divides into multiple trials, in each of which the same sequence of states repeats, with a distinct state occurring on each time step during the trial. Further imagine that the return being predicted is limited to the return over a trial, which makes a trial analogous to a reinforcement learning episode as we have defined it. In reality, of course, the returns being predicted are not confined to single trials, and the time interval between trials is an important factor in determining what an animal learns. This is true for TD learning as well, but here we assume that returns do not accumulate over multiple trials. Given this, then, a trial in experiments like those conducted by Schultz and colleagues is equivalent to an episode of reinforcement learning. (Though in this discussion, we will use the term trial instead of episode to relate better to the experiments.)",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "As usual, we also need to make an assumption about how states are represented as inputs to the learning algorithm, an assumption that influences how closely the TD error corresponds to dopamine neuron activity. We discuss this issue later, but for now we assume the same CSC representation used by Montague et al. (1996) in which there is a separate internal stimulus for each state visited at each time step in a trial. This reduces the process to the tabular case covered in the first part of this book. Finally, we assume that the agent uses  $\\mathrm{TD}(0)$  to learn a value function,  $V$ , stored in a lookup table initialized to be zero for all the states. We also assume that this is a deterministic task and that the discount factor,  $\\gamma$ , is very nearly one so that we can ignore it.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Figure 15.4 shows the time courses of  $R$ ,  $V$ , and  $\\delta$  at several stages of learning in this policy- evaluation task. The time axes represent the time interval over which a sequence of states is visited in a trial (where for clarity we omit showing individual states). The reward signal is zero throughout each trial except when the agent reaches the rewarding state, shown near the right end of the time line, when the reward signal becomes some positive number, say  $R^{\\star}$ . The goal of TD learning is to predict the return for each state visited in a trial, which in this undiscounted case and given our assumption that predictions are confined to individual trials, is simply  $R^{\\star}$  for each state.",
        "page_idx": 14
    },
    {
        "type": "image",
        "img_path": "images/17757ea8631c4256839ca33cb5f196648563528380672cdcd12cc5545f8ed2bc.jpg",
        "img_caption": [
            "Figure 15.4: The behavior of the TD error  $\\delta$  during TD learning is consistent with features of the phasic activation of dopamine neurons. (Here  $\\delta$  is the TD error available at time  $t$ , i.e.,  $\\delta_{t - 1}$ ). Top: a sequence of states, shown as an interval of regular predictors, is followed by a non-zero reward  $R^{\\star}$ . Early in learning: the initial value function,  $V$ , and initial  $\\delta$ , which at first is equal to  $R^{\\star}$ . Learning complete: the value function accurately predicts future reward,  $\\delta$  is positive at the earliest predictive state, and  $\\delta = 0$  at the time of the non-zero reward.  $R^{\\star}$  omitted: at the time the predicted reward is omitted,  $\\delta$  becomes negative. See text for a complete explanation of why this happens."
        ],
        "img_footnote": [],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Preceding the rewarding state is a sequence of reward- predicting states, with the earliest reward- predicting state shown near the left end of the time line. This is like the state near the start of a trial, for example like the state marked by the instruction cue in a trial of the monkey experiment of Schultz et al. (1993) described above. It is the first state in a trial that reliably predicts that trial's reward. (Of course, in reality states visited on preceding trials are even earlier reward- predicting states, but because we are confining predictions to individual trials, these do not qualify as predictors of this trial's reward. Below we give a more satisfactory, though more abstract, description of an earliest reward- predicting state.) The latest reward- predicting state in a trial is the state immediately preceding the trial's rewarding state. This is the state near the far right end of the time line in Figure 15.4. Note that the rewarding state of a trial does not predict the return for that trial: the value of this state would come to predict the return over all the following trials, which here we are assuming to be zero in this episodic formulation.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Figure 15.4 shows the first- trial time courses of  $V$  and  $\\delta$  as the graphs labeled 'early in learning.' Because the reward signal is zero throughout the trial except when the rewarding state is reached, and all the  $V$ - values are zero, the TD error is also zero until it becomes  $R^{\\star}$  at the rewarding state. This follows because  $\\delta_{t - 1} = R_{t} + V_{t} - V_{t - 1} = R_{t} + 0 - 0 = R_{t}$ , which is zero until it equals  $R^{*}$  when the reward occurs. Here  $V_{t}$  and  $V_{t - 1}$  are respectively the estimated values of the states visited at times  $t$  and  $t - 1$  in a trial. The TD error at this stage of learning is analogous to a dopamine neuron responding to an unpredicted reward (e.g., a drop of apple juice) at the start of training.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Throughout this first trial and all successive trials,  $\\mathrm{TD}(0)$  updates occur at each state transition as described in Chapter 6. This successively increases the values of the reward- predicting states, with the increases spreading backwards from the rewarding state, until the values converge to the correct return predictions. In this case (because we are assuming no discounting) the correct predictions are equal to  $R^{\\star}$  for all the reward- predicting states. This can be seen in Figure 15.4 as the graph of  $V$  labeled 'learning complete' where the values of all the states from the earliest to the latest reward- predicting states all equal  $R^{\\star}$ . The values of the states preceding the earliest reward- predicting state remain low (which Figure 15.4 shows as zero) because they are not reliable predictors of reward.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "When learning is complete, that is, when  $V$  attains its correct values, the TD errors associated with transitions from any reward- predicting state are zero because the predictions are now accurate. This is because for a transition from a reward- predicting state to another reward- predicting state, we have  $\\delta_{t - 1} = R_{t} + V_{t} - V_{t - 1} = 0 + R^{\\star} - R^{\\star} = 0$ , and for the transition from the latest reward- predicting state to the rewarding state, we have  $\\delta_{t - 1} = R_{t} + V_{t} - V_{t - 1} = R^{\\star} + 0 - R^{*} = 0$ . On the other hand, the TD error on a transition from any state to the earliest reward- predicting state is positive because of the mismatch between this state's low value and the larger value of the following reward- predicting state. Indeed, if the value of a state preceding the earliest reward- predicting state were zero, then after the transition to the earliest reward- predicting state, we would have that  $\\delta_{t - 1} = R_{t} + V_{t} - V_{t - 1} = 0 + R^{\\star} - 0 = R^{\\star}$ . The 'learning complete' graph of  $\\delta$  in Figure 15.4 shows this positive value at the earliest reward- predicting state, and zeros everywhere else.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The positive TD error upon transitioning to the earliest reward- predicting state is analogous to the persistence of dopamine responses to the earliest stimuli predicting reward. By the same token, when learning is complete, a transition from the latest reward- predicting state to the rewarding state produces a zero TD error because the latest reward- predicting state's value, being correct, cancels the reward. This parallels the observation that fewer dopamine neurons generate a phasic response to a fully predicted reward than to an unpredicted reward.",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "After learning, if the reward is suddenly omitted, the TD error goes negative at the usual time of reward because the value of the latest reward- predicting state is then too high:  $\\delta_{t - 1} = R_t + V_t - V_{t - 1} = 0 + 0 - R^\\star = - R^\\star$ , as shown at the right end of the  $R$  omitted graph of  $\\delta$  in Figure 15.4. This is like dopamine neuron activity decreasing below baseline at the time an expected reward is omitted as seen in the experiment of Schultz et al. (1993) described above and shown in Figure 15.3.",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "The idea of an earliest reward- predicting state deserves more attention. In the scenario described above, because experience is divided into trials, and we assumed that predictions are confined to individual trials, the earliest reward- predicting state is always the first state of a trial. Clearly this is artificial. A more general way to think of an earliest reward- predicting state is that it is an unpredicted predictor of reward, and there can be many such states. In an animal's life, many different states may precede an earliest reward- predicting state. However, because these states are more often followed by other states that do not predict reward, their reward- predicting powers, that is, their values, remain low. A TD algorithm, if operating throughout the animal's life, would update the values of these states too, but the updates would not consistently accumulate because, by assumption, none of these states reliably precedes an earliest reward- predicting state. If any of them did, they would be reward- predicting states as well. This might explain why with overtraining, dopamine responses decrease to even the earliest reward- predicting stimulus in a trial. With overtraining one would expect that even a formerly- unpredicted predictor state would become predicted by stimuli associated with earlier states: the animal's interaction with its environment both inside and outside of an experimental task would become commonplace. Upon breaking this routine with the introduction of a new task, however, one would see TD errors reappear, as indeed is observed in dopamine neuron activity.",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "The example described above explains why the TD error shares key features with the phasic activity of dopamine neurons when the animal is learning in a task similar to the idealized task of our example. But not every property of the phasic activity of dopamine neurons coincides so neatly with properties of  $\\delta$ . One of the most troubling discrepancies involves what happens when a reward occurs earlier than expected. We have seen that the omission of an expected reward produces a negative prediction error at the reward's expected time, which corresponds to the activity of dopamine neurons decreasing below baseline when this happens. If the reward arrives later than expected, it is then an unexpected reward and generates a positive prediction error. This happens with both TD errors and dopamine neuron responses. But when reward arrives earlier than expected, dopamine neurons do not do what the TD error does—at least with the CSC representation used by Montague et al. (1996) and by us in our example. Dopamine",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "neurons do respond to the early reward, which is consistent with a positive TD error because the reward is not predicted to occur then. However, at the later time when the reward is expected but omitted, the TD error is negative whereas, in contrast to this prediction, dopamine neuron activity does not drop below baseline in the way the TD model predicts (Hollerman and Schultz, 1998). Something more complicated is going on in the animal's brain than simply TD learning with a CSC representation.",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Some of the mismatches between the TD error and dopamine neuron activity can be addressed by selecting suitable parameter values for the TD algorithm and by using stimulus representations other than the CSC representation. For instance, to address the early- reward mismatch just described, Suri and Schultz (1999) proposed a CSC representation in which the sequences of internal signals initiated by earlier stimuli are cancelled by the occurrence of a reward. Another proposal by Daw, Courville, and Touretzky (2006) is that the brain's TD system uses representations produced by statistical modeling carried out in sensory cortex rather than simpler representations based on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning with a microstimulus (MS) representation (Figure 14.1) fits the activity of dopamine neurons in the early- reward and other situations better than when a CSC representation is used. Pan, Schmidt, Wickerus, and Hyland (2005) found that even with the CSC representation, prolonged eligibility traces improve the fit of the TD error to some aspects of dopamine neuron activity. In general, many fine details of TD- error behavior depend on subtle interactions between eligibility traces, discounting, and stimulus representations. Findings like these elaborate and refine the reward prediction error hypothesis without refuting its core claim that the phasic activity of dopamine neurons is well characterized as signaling TD errors.",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "On the other hand, there are other discrepancies between the TD theory and experimental data that are not so easily accommodated by selecting parameter values and stimulus representations (we mention some of these discrepancies in the Bibliographical and Historical Remarks section at the end of this chapter), and more mismatches are likely to be discovered as neuroscientists conduct ever more refined experiments. But the reward prediction error hypothesis has been functioning very effectively as a catalyst for improving our understanding of how the brain's reward system works. Intricate experiments have been designed to validate or refute predictions derived from the hypothesis, and experimental results have, in turn, led to refinement and elaboration of the TD error/dopamine hypothesis.",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "A remarkable aspect of these developments is that the reinforcement learning algorithms and theory that connect so well with properties of the dopamine system were developed from a computational perspective in total absence of any knowledge about the relevant properties of dopamine neurons—remember, TD learning and its connections to optimal control and dynamic programming were developed many years before any of the experiments were conducted that revealed the TD- like nature of dopamine neuron activity. This unplanned correspondence, despite not being perfect, suggests that the TD error/dopamine parallel captures something significant about brain reward processes.",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "In addition to accounting for many features of the phasic activity of dopamine neurons, the reward prediction error hypothesis links neuroscience to other aspects of reinforcement",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "learning, in particular, to learning algorithms that use TD errors as reinforcement signals. Neuroscience is still far from reaching complete understanding of the circuits, molecular mechanisms, and functions of the phasic activity of dopamine neurons, but evidence supporting the reward prediction error hypothesis, along with evidence that phasic dopamine responses are reinforcement signals for learning, suggest that the brain might implement something like an actor- critic algorithm in which TD errors play critical roles. Other reinforcement learning algorithms are plausible candidates too, but actor- critic algorithms fit the anatomy and physiology of the mammalian brain particularly well, as we describe in the following two sections.",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "15.7 Neural Actor-Critic",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Actor- critic algorithms learn both policies and value functions. The 'actor' is the component that learns policies, and the 'critic' is the component that learns about whatever policy is currently being followed by the actor in order to 'criticize' the actor's action choices. The critic uses a TD algorithm to learn the state- value function for the actor's current policy. The value function allows the critic to critique the actor's action choices by sending TD errors,  $\\delta$ , to the actor. A positive  $\\delta$  means that the action was 'good' because it led to a state with a better- than- expected value; a negative  $\\delta$  means that the action was 'bad' because it led to a state with a worse- than- expected value. Based on these critiques, the actor continually updates its policy.",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Two distinctive features of actor- critic algorithms are responsible for thinking that the brain might implement an algorithm like this. First, the two components of an actor- critic algorithm—the actor and the critic—suggest that two parts of the striatum—the dorsal and ventral subdivisions (Section 15.4), both critical for reward- based learning—may function respectively something like an actor and a critic. A second property of actor- critic algorithms that suggests a brain implementation is that the TD error has the dual role of being the reinforcement signal for both the actor and the critic, though it has a different influence on learning in each of these components. This fits well with several properties of the neural circuitry: axons of dopamine neurons target both the dorsal and ventral subdivisions of the striatum; dopamine appears to be critical for modulating synaptic plasticity in both structures; and how a neuromodulator such as dopamine acts on a target structure depends on properties of the target structure and not just on properties of the neuromodulator.",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Section 13.5 presents actor- critic algorithms as policy gradient methods, but the actor- critic algorithm of Barto, Sutton, and Anderson (1983) was simpler and was presented as an artificial neural network (ANN). Here we describe an ANN implementation something like that of Barto et al., and we follow Takahashi, Schoenbaum, and Niv (2008) in giving a schematic proposal for how this ANN might be implemented by real neural networks in the brain. We postpone discussion of the actor and critic learning rules until Section 15.8, where we present them as special cases of the policy- gradient formulation and discuss what they suggest about how dopamine might modulate synaptic plasticity.",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Figure 15.5a shows an implementation of an actor- critic algorithm as an ANN with component networks implementing the actor and the critic. The critic consists of a single neuron- like unit,  $V$ , whose output activity represents state values, and a component shown as the diamond labeled TD that computes TD errors by combining  $V$ 's output with reward signals and with previous state values (as suggested by the loop from the TD diamond to itself). The actor network has a single layer of  $k$  actor units labeled  $A_{i}$ ,  $i = 1, \\ldots , k$ . The output of each actor unit is a component of a  $k$ - dimensional action vector. An alternative is that there are  $k$  separate actions, one commanded by each actor unit, that compete with one another to be executed, but here we will think of the entire  $A$ - vector as an action.",
        "page_idx": 19
    },
    {
        "type": "image",
        "img_path": "images/6eb78ee330d13db27d5565f4446083a33a27f1215fde30fc7cc433d8d3a518d4.jpg",
        "img_caption": [
            "Figure 15.5: Actor-critic ANN and a hypothetical neural implementation. a) Actor-critic algorithm as an ANN. The actor adjusts a policy based on the TD error  $\\delta$  it receives from the critic; the critic adjusts state-value parameters using the same  $\\delta$ . The critic produces a TD error from the reward signal,  $R$ , and the current change in its estimate of state values. The actor does not have direct access to the reward signal, and the critic does not have direct access to the action. b) Hypothetical neural implementation of an actor-critic algorithm. The actor and the value-learning part of the critic are respectively placed in the dorsal and ventral subdivisions of the striatum. The TD error is transmitted by dopamine neurons located in the VTA and SNpc to modulate changes in synaptic efficacies of input from cortical areas to the ventral and dorsal striatum. Adapted from Frontiers in Neuroscience, vol. 2(1), 2008, Y. Takahashi, G. Schoenbaum, and Y. Niv, Silencing the critics: Understanding the effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an Actor/Critic model."
        ],
        "img_footnote": [],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Both the critic and actor networks receive input consisting of multiple features representing the state of the agent's environment. (Recall from Chapter 1 that the environment of a reinforcement learning agent includes components both inside and outside of the 'organism' containing the agent.) The figure shows these features as the circles labeled  $x_{1}, x_{2}, \\ldots , x_{n}$ , shown twice just to keep the figure simple. A weight representing the efficacy of a synapse is associated with each connection from each feature  $x_{i}$  to the critic unit,  $V$ , and to each of the action units,  $A_{i}$ . The weights in the critic network parameterize the value function, and the weights in the actor network parameterize the policy. The networks learn as these weights change according to the critic and actor learning rules that we describe in the following section.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The TD error produced by circuitry in the critic is the reinforcement signal for changing the weights in both the critic and the actor networks. This is shown in Figure 15.5a by the line labeled 'TD error  $\\delta$ ' extending across all of the connections in the critic and actor networks. This aspect of the network implementation, together with the reward prediction error hypothesis and the fact that the activity of dopamine neurons is so widely distributed by the extensive axonal arbors of these neurons, suggests that an actor- critic network something like this may not be too farfetched as a hypothesis about how reward- related learning might happen in the brain.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Figure 15.5b suggests—very schematically—how the ANN on the figure's left might map onto structures in the brain according to the hypothesis of Takahashi et al. (2008). The hypothesis puts the actor and the value- learning part of the critic respectively in the dorsal and ventral subdivisions of the striatum, the input structure of the basal ganglia. Recall from Section 15.4 that the dorsal striatum is primarily implicated in influencing action selection, and the ventral striatum is thought to be critical for different aspects of reward processing, including the assignment of affective value to sensations. The cerebral cortex, along with other structures, sends input to the striatum conveying information about stimuli, internal states, and motor activity.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "In this hypothetical actor- critic brain implementation, the ventral striatum sends value information to the VTA and SNpc, where dopamine neurons in these nuclei combine it with information about reward to generate activity corresponding to TD errors (though exactly how dopaminergic neurons calculate these errors is not yet understood). The 'TD error  $\\delta$ ' line in Figure 15.5a becomes the line labeled 'Dopamine' in Figure 15.5b, which represents the widely branching axons of dopamine neurons whose cell bodies are in the VTA and SNpc. Referring back to Figure 15.1, these axons make synaptic contact with the spines on the dendrites of medium spiny neurons, the main input/output neurons of both the dorsal and ventral divisions of the striatum. Axons of the cortical neurons that send input to the striatum make synaptic contact on the tips of these spines. According to the hypothesis, it is at these spines where changes in the efficacies of the synapses from cortical regions to the striatum are governed by learning rules that critically depend on a reinforcement signal supplied by dopamine.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "An important implication of the hypothesis illustrated in Figure 15.5b is that the dopamine signal is not the 'master' reward signal like the scalar  $R_{t}$  of reinforcement learning. In fact, the hypothesis implies that one should not necessarily be able to probe the brain and record any signal like  $R_{t}$  in the activity of any single neuron.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Many interconnected neural systems generate reward- related information, with different structures being recruited depending on different types of rewards. Dopamine neurons receive information from many different brain areas, so the input to the SNpc and VTA labeled 'Reward' in Figure 15.5b should be thought of as vector of reward- related information arriving to neurons in these nuclei along multiple input channels. What the theoretical scalar reward signal  $R_{t}$  might correspond to, then, is the net contribution of all reward- related information to dopamine neuron activity. It is the result of a pattern of activity across many neurons in different areas of the brain.",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Although the actor- critic neural implementation illustrated in Figure 15.5b may be correct on some counts, it clearly needs to be refined, extended, and modified to qualify as a full- fledged model of the function of the phasic activity of dopamine neurons. The Historical and Bibliographic Remarks section at the end of this chapter cites publications that discuss in more detail both empirical support for this hypothesis and places where it falls short. We now look in detail at what the actor and critic learning algorithms suggest about the rules governing changes in synaptic efficacies of corticostriatal synapses.",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "15.8 Actor and Critic Learning Rules",
        "text_level": 1,
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "If the brain does implement something like the actor- critic algorithm—and assuming populations of dopamine neurons broadcast a common reinforcement signal to the corticostriatal synapses of both the dorsal and ventral striatum as illustrated in Figure 15.5b (which is likely an oversimplification as we mentioned above)—then this reinforcement signal affects the synapses of these two structures in different ways. The learning rules for the critic and the actor use the same reinforcement signal, the TD error  $\\delta$ , but its effect on learning is different for these two components. The TD error (combined with eligibility traces) tells the actor how to update action probabilities in order to reach higher- valued states. Learning by the actor is like instrumental conditioning using a Law- of- Effect- type learning rule (Section 1.7), the actor works to keep  $\\delta$  as positive as possible. On the other hand, the TD error (when combined with eligibility traces) tells the critic the direction and magnitude in which to change the parameters of the value function in order to improve its predictive accuracy. The critic works to reduce  $\\delta$ 's magnitude to be as close to zero as possible using a learning rule like the TD model of classical conditioning (Section 14.2). The difference between the critic and actor learning rules is relatively simple, but this difference has a profound effect on learning and is essential to how the actor- critic algorithm works. The difference lies solely in the eligibility traces each type of learning rule uses.",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "More than one set of learning rules can be used in actor- critic neural networks like those in Figure 15.5b but, to be specific, here we focus on the actor- critic algorithm for continuing problems with eligibility traces presented in Section 13.6. On each transition from state  $S_{t}$  to state  $S_{t + 1}$ , taking action  $A_{t}$  and receiving reward  $R_{t + 1}$ , that algorithm computes the TD error  $(\\delta)$  and then updates the eligibility trace vectors  $(\\mathbf{z}_{t}^{\\mathbf{w}}$  and  $\\mathbf{z}_{t}^{\\theta}$ ) and",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "the parameters for the critic and actor (  $\\mathbf{w}$  and  $\\pmb \\theta$  ), according to",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "\n$$\n\\begin{array}{r l} & {\\delta_{t} = R_{t + 1} + \\gamma \\hat{v} (S_{t + 1},\\mathbf{w}) - \\hat{v} (S_{t},\\mathbf{w}),}\\\\ & {\\mathbf{z}_{t}^{\\mathbf{w}} = \\gamma \\lambda^{\\mathbf{w}}\\mathbf{z}_{t - 1}^{\\mathbf{w}} + \\nabla \\hat{v} (S_{t},\\mathbf{w}),}\\\\ & {\\mathbf{z}_{t}^{\\pmb \\theta} = \\gamma \\lambda^{\\pmb \\theta}\\mathbf{z}_{t - 1}^{\\pmb \\theta} + \\nabla \\ln \\tau (A_{t}|S_{t},\\pmb \\theta),}\\\\ & {\\mathbf{w}\\leftarrow \\mathbf{w} + \\alpha^{\\mathbf{w}}\\delta_{t}\\mathbf{z}_{t}^{\\mathbf{w}},}\\\\ & {\\pmb \\theta \\leftarrow \\pmb \\theta +\\alpha^{\\pmb \\theta}\\delta_{t}\\mathbf{z}_{t}^{\\pmb \\theta},} \\end{array}\n$$\n",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "where  $\\gamma \\in [0,1)$  is a discount- rate parameter,  $\\lambda^{\\mathbf{w}}\\in [0,1]$  and  $\\lambda^{\\theta}\\in [0,1]$  are bootstrapping parameters for the critic and the actor respectively, and  $\\alpha^{\\mathbf{w}} > 0$  and  $\\alpha^{\\theta} > 0$  are analogous step- size parameters.",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Think of the approximate value function  $\\hat{v}$  as the output of a single linear neuron- like unit, called the critic unit and labeled  $V$  in Figure 15.5a. Then the value function is a linear function of the feature- vector representation of state  $s$ $\\mathbf{x}(s) = (x_{1}(s),\\ldots ,x_{n}(s))^{\\top}$  parameterized by a weight vector  $\\mathbf{w} = (w_{1},\\ldots ,w_{n})^{\\top}$  ..",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "\n$$\n\\hat{v} (s,\\mathbf{w}) = \\mathbf{w}^{\\top}\\mathbf{x}(s). \\tag{15.1}\n$$\n",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Each  $x_{i}(s)$  is like the presynaptic signal to a neuron's synapse whose efficacy is  $w_{i}$  . The weights of the critic are incremented according to the rule above by  $\\alpha^{\\mathbf{w}}\\delta_{t}\\mathbf{z}_{t}^{\\mathbf{w}}$  , where the reinforcement signal,  $\\delta_{t}$  , corresponds to a dopamine signal being broadcast to all of the critic unit's synapses. The eligibility trace vector,  $\\mathbf{z}_{t}^{\\mathbf{w}}$  , for the critic unit is a trace (average of recent values) of  $\\nabla \\hat{v} (S_{t},\\mathbf{w})$  . Because  $\\hat{v} (s,\\mathbf{w})$  is linear in the weights,  $\\nabla \\hat{v} (S_{t},\\mathbf{w}) = \\mathbf{x}(S_{t})$",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "In neural terms, this means that each synapse has its own eligibility trace, which is one component of the vector  $\\mathbf{z}_{t}^{\\mathbf{w}}$  . A synapse's eligibility trace accumulates according to the level of activity arriving at that synapse, that is, the level of presynaptic activity, represented here by the component of the feature vector  $\\mathbf{x}(S_{t})$  arriving at that synapse. The trace otherwise decays toward zero at a rate governed by the fraction  $\\lambda^{\\mathbf{w}}$  . A synapse is eligible for modification as long as its eligibility trace is non- zero. How the synapse's efficacy is actually modified depends on the reinforcement signals that arrive while the synapse is eligible. We call eligibility traces like these of the critic unit's synapses noncontingent eligibility traces because they only depend on presynaptic activity and are not contingent in any way on postsynaptic activity.",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The non- contingent eligibility traces of the critic unit's synapses mean that the critic unit's learning rule is essentially the TD model of classical conditioning described in Section 14.2. With the definition we have given above of the critic unit and its learning rule, the critic in Figure 15.5a is the same as the critic in the ANN actor- critic of Barto et al. (1983). Clearly, a critic like this consisting of just one linear neuron- like unit is the simplest starting point; this critic unit is a proxy for a more complicated neural network able to learn value functions of greater complexity.",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The actor in Figure 15.5a is a one- layer network of  $k$  neuron- like actor units, each receiving at time  $t$  the same feature vector,  $\\mathbf{x}(S_{t})$  , that the critic unit receives. Each actor unit  $j$ $j = 1,\\ldots ,k$  , has its own weight vector,  $\\theta_{j}$  , but because the actor units are all identical, we describe just one of the units and omit the subscript. One way for these",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "units to follow the actor- critic algorithm given in the equations above is for each to be a Bernoulli- logistic unit. This means that the output of each actor unit at each time is a random variable,  $A_{t}$ , taking value 0 or 1. Think of value 1 as the neuron firing, that is, emitting an action potential. The weighted sum,  $\\pmb{\\theta}^{\\top} \\mathbf{x}(S_{t})$ , of a unit's input vector determines the unit's action probabilities via the exponential soft- max distribution (13.2), which for two actions is the logistic function:",
        "page_idx": 23
    },
    {
        "type": "equation",
        "text": "\n$$\n\\pi (1|s,\\pmb {\\theta}) = 1 - \\pi (0|s,\\pmb {\\theta}) = \\frac{1}{1 + \\exp(-\\pmb{\\theta}^{\\top}\\mathbf{x}(s))}. \\tag{15.2}\n$$\n",
        "text_format": "latex",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "The weights of each actor unit are incremented, as above, by:  $\\pmb {\\theta} \\leftarrow \\pmb {\\theta} + \\alpha^{\\pmb{\\theta}} \\delta_{t} \\mathbf{z}_{t}^{\\pmb{\\theta}}$ , where  $\\delta$  again corresponds to the dopamine signal: the same reinforcement signal that is sent to all the critic unit's synapses. Figure 15.5a shows  $\\delta_{t}$  being broadcast to all the synapses of all the actor units (which makes this actor network a team of reinforcement learning agents, something we discuss in Section 15.10 below). The actor eligibility trace vector  $\\mathbf{z}_{t}^{\\theta}$  is a trace (average of recent values) of  $\\nabla \\ln \\pi (A_{t} | S_{t}, \\pmb {\\theta})$ . To understand this eligibility trace refer to Exercise 13.5, which defines this kind of unit and asks you to give a learning rule for it. That exercise asked you to express  $\\nabla \\ln \\pi (a | s, \\pmb {\\theta})$  in terms of  $a, \\mathbf{x}(s)$ , and  $\\pi (a | s, \\pmb {\\theta})$  (for arbitrary state  $s$  and action  $a$ ) by calculating the gradient. For the action and state actually occurring at time  $t$ , the answer is",
        "page_idx": 23
    },
    {
        "type": "equation",
        "text": "\n$$\n\\nabla \\ln \\pi (A_{t} | S_{t}, \\pmb {\\theta}) = \\big(A_{t} - \\pi (1 | S_{t}, \\pmb {\\theta})\\big) \\mathbf{x}(S_{t}). \\tag{15.3}\n$$\n",
        "text_format": "latex",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Unlike the non- contingent eligibility trace of a critic synapse that only accumulates the presynaptic activity  $\\mathbf{x}(S_{t})$ , the eligibility trace of an actor unit's synapse in addition depends on the activity of the actor unit itself. We call this a contingent eligibility trace because it is contingent on this postsynaptic activity. The eligibility trace at each synapse continually decays, but increments or decrements depending on the activity of the presynaptic neuron and whether or not the postsynaptic neuron fires. The factor  $A_{t} - \\pi (1 | S_{t}, \\pmb {\\theta})$  in (15.3) is positive when  $A_{t} = 1$  and negative otherwise. The postsynaptic contingency in the eligibility traces of actor units is the only difference between the critic and actor learning rules. By keeping information about what actions were taken in what states, contingent eligibility traces allow credit for reward (positive  $\\delta$ ), or blame for punishment (negative  $\\delta$ ), to be apportioned among the policy parameters (the efficacies of the actor units' synapses) according to the contributions these parameters made to the units' outputs that could have influenced later values of  $\\delta$ . Contingent eligibility traces mark the synapses as to how they should be modified to alter the units' future responses to favor positive values of  $\\delta$ .",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "What do the critic and actor learning rules suggest about how efficacies of corticostriatal synapses change? Both learning rules are related to Donald Hebb's classic proposal that whenever a presynaptic signal participates in activating the postsynaptic neuron, the synapse's efficacy increases (Hebb, 1949). The critic and actor learning rules share with Hebb's proposal the idea that changes in a synapse's efficacy depend on the interaction of several factors. In the critic learning rule the interaction is between the reinforcement signal  $\\delta$  and eligibility traces that depend only on presynaptic signals. Neuroscientists call this a two- factor learning rule because the interaction is between two signals or",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "quantities. The actor learning rule, on the other hand, is a three- factor learning rule because, in addition to depending on  $\\delta$ , its eligibility traces depend on both presynaptic and postsynaptic activity. Unlike Hebb's proposal, however, the relative timing of the factors is critical to how synaptic efficacies change, with eligibility traces intervening to allow the reinforcement signal to affect synapses that were active in the recent past.",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Some subtleties about signal timing for the actor and critic learning rules deserve closer attention. In defining the neuron- like actor and critic units, we ignored the small amount of time it takes synaptic input to effect the firing of a real neuron. When an action potential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules are released that diffuse across the synaptic cleft to the postsynaptic neuron, where they bind to receptors on the postsynaptic neuron's surface; this activates molecular machinery that causes the postsynaptic neuron to fire (or to inhibit its firing in the case of inhibitory synaptic input). This process can take several tens of milliseconds. According to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces the unit's output. Ignoring activation time like this is common in abstract models of Hebbian- style plasticity in which synaptic efficacies change according to a simple product of simultaneous pre- and postsynaptic activity. More realistic models must take activation time into account.",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Activation time is especially important for a more realistic actor unit because it influences how contingent eligibility traces have to work in order to properly apportion credit for reinforcement to the appropriate synapses. The expression  $\\left(A_{t} - \\pi (1|S_{t},\\pmb {\\theta})\\right)\\mathbf{x}(S_{t})$  defining contingent eligibility traces for the actor unit's learning rule given above includes the postsynaptic factor  $\\left(A_{t} - \\pi (1|S_{t},\\pmb {\\theta})\\right)$  and the presynaptic factor  $\\mathbf{x}(S_{t})$ . This works because by ignoring activation time, the presynaptic activity  $\\mathbf{x}(S_{t})$  participates in causing the postsynaptic activity appearing in  $\\left(A_{t} - \\pi (1|S_{t},\\pmb {\\theta})\\right)$ . To assign credit for reinforcement correctly, the presynaptic factor defining the eligibility trace must be a cause of the postsynaptic factor that also defines the trace. Contingent eligibility traces for a more realistic actor unit would have to take activation time into account. (Activation time should not be confused with the time required for a neuron to receive a reinforcement signal influenced by that neuron's activity. The function of eligibility traces is to span this time interval which is generally much longer than the activation time. We discuss this further in the following section.)",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "There are hints from neuroscience for how this process might work in the brain. Neuroscientists have discovered a form of Hebbian plasticity called spike- timing- dependent plasticity (STDP) that lends plausibility to the existence of actor- like synaptic plasticity in the brain. STDP is a Hebbian- style plasticity, but changes in a synapse's efficacy depend on the relative timing of presynaptic and postsynaptic action potentials. The dependence can take different forms, but in the one most studied, a synapse increases in strength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron fires. If the timing relation is reversed, with a presynaptic spike arriving shortly after the postsynaptic neuron fires, then the strength of the synapse decreases. STDP is a type of Hebbian plasticity that takes the activation time of a neuron into account, which is one of the ingredients needed for actor- like learning.",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "The discovery of STDP has led neuroscientists to investigate the possibility of a three- factor form of STDP in which neuromodulatory input must follow appropriately- timed pre- and postsynaptic spikes. This form of synaptic plasticity, called reward- modulated  $STDP$ , is much like the actor learning rule discussed here. Synaptic changes that would be produced by regular STDP only occur if there is neuromodulatory input within a time window after a presynaptic spike is closely followed by a postsynaptic spike. Evidence is accumulating that reward- modulated STDP occurs at the spines of medium spiny neurons of the dorsal striatum, with dopamine providing the neuromodulatory factor—the sites where actor learning takes place in the hypothetical neural implementation of an actor- critic algorithm illustrated in Figure 15.5b. Experiments have demonstrated reward- modulated STDP in which lasting changes in the efficacies of corticostriatal synapses occur only if a neuromodulatory pulse arrives within a time window that can last up to 10 seconds after a presynaptic spike is closely followed by a postsynaptic spike (Yagishita et al. 2014). Although the evidence is indirect, these experiments point to the existence of contingent eligibility traces having prolonged time courses. The molecular mechanisms producing these traces, as well as the much shorter traces that likely underly STDP, are not yet understood, but research focusing on time- dependent and neuromodulator- dependent synaptic plasticity is continuing.",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The neuron- like actor unit that we have described here, with its Law- of- Effect- style learning rule, appeared in somewhat simpler form in the actor- critic network of Barto et al. (1983). That network was inspired by the \"hedonistic neuron\" hypothesis proposed by physiologist A. H. Klopf (1972, 1982). Not all the details of Klopf's hypothesis are consistent with what has been learned about synaptic plasticity, but the discovery of STDP and the growing evidence for a reward- modulated form of STDP suggest that Klopf's ideas may not have been far off the mark. We discuss Klopf's hedonistic neuron hypothesis next.",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "15.9 Hedonistic Neurons",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "In his hedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual neurons seek to maximize the difference between synaptic input treated as rewarding and synaptic input treated as punishing by adjusting the efficacies of their synapses on the basis of rewarding or punishing consequences of their own action potentials. In other words, individual neurons can be trained with response- contingent reinforcement like an animal can be trained in an instrumental conditioning task. His hypothesis included the idea that rewards and punishments are conveyed to a neuron via the same synaptic input that excites or inhibits the neuron's spike- generating activity. (Had Klopf known what we know today about neuromodulatory systems, he might have assigned the reinforcing role to neuromodulatory input, but he wanted to avoid any centralized source of training information.) Synaptically- local traces of past pre- and postsynaptic activity had the key function in Klopf's hypothesis of making synapses eligible—the term he introduced—for modification by later reward or punishment. He conjectured that these traces are implemented by molecular mechanisms local to each synapse and therefore different from the electrical activity of both the pre- and the postsynaptic neurons. In",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "the Bibliographical and Historical Remarks section of this chapter we bring attention to some similar proposals made by others.",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Klopf specifically conjectured that synaptic efficacies change in the following way. When a neuron fires an action potential, all of its synapses that were active in contributing to that action potential become eligible to undergo changes in their efficacies. If the action potential is followed within an appropriate time period by an increase of reward, the efficacies of all the eligible synapses increase. Symmetrically, if the action potential is followed within an appropriate time period by an increase of punishment, the efficacies of eligible synapses decrease. This is implemented by triggering an eligibility trace at a synapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly, upon pairing of presynaptic activity with the postsynaptic activity that that presynaptic activity participates in causing)—what we call a contingent eligibility trace. This is essentially the three- factor learning rule of an actor unit described in the previous section.",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The shape and time course of an eligibility trace in Klopf's theory reflects the durations of the many feedback loops in which the neuron is embedded, some of which lie entirely within the brain and body of the organism, while others extend out through the organism's external environment as mediated by its motor and sensory systems. His idea was that the shape of a synaptic eligibility trace is like a histogram of the durations of the feedback loops in which the neuron is embedded. The peak of an eligibility trace would then occur at the duration of the most prevalent feedback loops in which that neuron participates. The eligibility traces used by algorithms described in this book are simplified versions of Klopf's original idea, being exponentially (or geometrically) decreasing functions controlled by the parameters  $\\lambda$  and  $\\gamma$ . This simplifies simulations as well as theory, but we regard these simple eligibility traces as a placeholders for traces closer to Klopf's original conception, which would have computational advantages in complex reinforcement learning systems by refining the credit- assignment process.",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Klopf's hedonistic neuron hypothesis is not as implausible as it may at first appear. A well- studied example of a single cell that seeks some stimuli and avoids others is the bacterium Escherichia coli. The movement of this single- cell organism is influenced by chemical stimuli in its environment, behavior known as chemotaxis. It swims in its liquid environment by rotating hairlike structures called flagella attached to its surface. (Yes, it rotates them!) Molecules in the bacterium's environment bind to receptors on its surface. Binding events modulate the frequency with which the bacterium reverses flagellar rotation. Each reversal causes the bacterium to tumble in place and then head off in a random new direction. A little chemical memory and computation causes the frequency of flagellar reversal to decrease when the bacterium swims toward higher concentrations of molecules it needs to survive (attractants) and increase when the bacterium swims toward higher concentrations of molecules that are harmful (repellants). The result is that the bacterium tends to persist in swimming up attractant gradients and tends to avoid swimming up repellant gradients.",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The chemotactic behavior just described is called klinokinesis. It is a kind of trial- and- error behavior, although it is unlikely that learning is involved: the bacterium needs a modicum of short- term memory to detect molecular concentration gradients, but it probably does not maintain long- term memories. Artificial intelligence pioneer Oliver",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Selfridge called this strategy \"run and twiddle,\" pointing out its utility as a basic adaptive strategy: \"keep going in the same way if things are getting better, and otherwise move around\" (Selfridge, 1978, 1984). Similarly, one might think of a neuron \"swimming\" (not literally of course) in a medium composed of the complex collection of feedback loops in which it is embedded, acting to obtain one type of input signal and to avoid others. Unlike the bacterium, however, the neuron's synaptic strengths retain information about its past trial- and- error behavior. If this view of the behavior of a neuron (or just one type of neuron) is plausible, then the closed- loop nature of how the neuron interacts with its environment is important for understanding its behavior, where the neuron's environment consists of the rest of the animal together with the environment with which the animal as a whole interacts.",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Klopf's hedonistic neuron hypothesis extended beyond the idea that individual neurons are reinforcement learning agents. He argued that many aspects of intelligent behavior can be understood as the result of the collective behavior of a population of self- interested hedonistic neurons interacting with one another in an immense society or economic system making up an animal's nervous system. Whether or not this view of nervous systems is useful, the collective behavior of reinforcement learning agents has implications for neuroscience. We take up this subject next.",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "15.10 Collective Reinforcement Learning",
        "text_level": 1,
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The behavior of populations of reinforcement learning agents is deeply relevant to the study of social and economic systems, and if anything like Klopf's hedonistic neuron hypothesis is correct, to neuroscience as well. The hypothesis described above about how an actor- critic algorithm might be implemented in the brain only narrowly addresses the implications of the fact that the dorsal and ventral subdivisions of the striatum, the respective locations of the actor and the critic according to the hypothesis, each contain millions of medium spiny neurons whose synapses undergo change modulated by phasic bursts of dopamine neuron activity.",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The actor in Figure 15.5a is a single- layer network of  $k$  actor units. The actions produced by this network are vectors  $(A_{1},A_{2},\\dots ,A_{k})^{\\top}$  presumed to drive the animal's behavior. Changes in the efficacies of the synapses of all of these units depend on the reinforcement signal  $\\delta$  . Because actor units attempt to make  $\\delta$  as large as possible,  $\\delta$  effectively acts as a reward signal for them (so in this case reinforcement is the same as reward). Thus, each actor unit is itself a reinforcement learning agent a hedonistic neuron if you will. Now, to make the situation as simple as possible, assume that each of these units receives the same reward signal at the same time (although, as indicated above, the assumption that dopamine is released at all the corticostriatal synapses under the same conditions and at the same times is likely an oversimplification).",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "What can reinforcement learning theory tell us about what happens when all members of a population of reinforcement learning agents learn according to a common reward signal? The field of multi- agent reinforcement learning considers many aspects of learning by populations of reinforcement learning agents. Although this field is beyond the scope of this book, we believe that some of its basic concepts and results are relevant to thinking",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "about the brain's diffuse neuromodulatory systems. In multi- agent reinforcement learning (and in game theory), the scenario in which all the agents try to maximize a common reward signal that they simultaneously receive is known as a cooperative game or a team problem.",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "What makes a team problem interesting and challenging is that the common reward signal sent to each agent evaluates the pattern of activity produced by the entire population, that is, it evaluates the collective action of the team members. This means that any individual agent has only limited ability to affect the reward signal because any single agent contributes just one component of the collective action evaluated by the common reward signal. Effective learning in this scenario requires addressing a structural credit assignment problem: which team members, or groups of team members, deserve credit for a favorable reward signal, or blame for an unfavorable reward signal? It is a cooperative game, or a team problem, because the agents are united in seeking to increase the same reward signal: there are no conflicts of interest among the agents. The scenario would be a competitive game if different agents receive different reward signals, where each reward signal again evaluates the collective action of the population, and the objective of each agent is to increase its own reward signal. In this case there might be conflicts of interest among the agents, meaning that actions that are good for some agents are bad for others. Even deciding what the best collective action should be is a non- trivial aspect of game theory. This competitive setting might be relevant to neuroscience too (for example, to account for heterogeneity of dopamine neuron activity), but here we focus only on the cooperative, or team, case.",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "How can each reinforcement learning agent in a team learn to \"do the right thing\" so that the collective action of the team is highly rewarded? An interesting result is that if each agent can learn effectively despite its reward signal being corrupted by a large amount of noise, and despite its lack of access to complete state information, then the population as a whole will learn to produce collective actions that improve as evaluated by the common reward signal, even when the agents cannot communicate with one another. Each agent faces its own reinforcement learning task in which its influence on the reward signal is deeply buried in the noise created by the influences of other agents. In fact, for any agent, all the other agents are part of its environment because its input, both the part conveying state information and the reward part, depends on how all the other agents are behaving. Furthermore, lacking access to the actions of the other agents, indeed lacking access to the parameters determining their policies, each agent can only partially observe the state of its environment. This makes each team member's learning task very difficult, but if each uses a reinforcement learning algorithm able to increase a reward signal even under these difficult conditions, teams of reinforcement learning agents can learn to produce collective actions that improve over time as evaluated by the team's common reward signal.",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "If the team members are neuron- like units, then each unit has to have the goal of increasing the amount of reward it receives over time, as the actor unit does that we described in Section 15.8. Each unit's learning algorithm has to have two essential features. First, it has to use contingent eligibility traces. Recall that a contingent eligibility trace, in neural terms, is initiated (or increased) at a synapse when its presynaptic input",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "participates in causing the postsynaptic neuron to fire. A non- contingent eligibility trace, in contrast, is initiated or increased by presynaptic input independently of what the postsynaptic neuron does. As explained in Section 15.8, by keeping information about what actions were taken in what states, contingent eligibility traces allow credit for reward, or blame for punishment, to be apportioned to an agent's policy parameters according to the contribution the values of these parameters made in determining the agent's action. By similar reasoning, a team member must remember its recent action so that it can either increase or decrease the likelihood of producing that action according to the reward signal that is subsequently received. The action component of a contingent eligibility trace implements this action memory. Because of the complexity of the learning task, however, contingent eligibility is merely a preliminary step in the credit assignment process: the relationship between a single team member's action and changes in the team's reward signal is a statistical correlation that has to be estimated over many trials. Contingent eligibility is an essential but preliminary step in this process.",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Learning with non- contingent eligibility traces does not work at all in the team setting because it does not provide a way to correlate actions with consequent changes in the reward signal. Non- contingent eligibility traces are adequate for learning to predict, as the critic component of the actor- critic algorithm does, but they do not support learning to control, as the actor component must do. The members of a population of critic- like agents may still receive a common reinforcement signal, but they would all learn to predict the same quantity (which in the case of an actor- critic method, would be the expected return for the current policy). How successful each member of the population would be in learning to predict the expected return would depend on the information it receives, which could be very different for different members of the population. There would be no need for the population to produce differentiated patterns of activity. This is not a team problem as defined here.",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "A second requirement for collective learning in a team problem is that there has to be variability in the actions of the team members in order for the team to explore the space of collective actions. The simplest way for a team of reinforcement learning agents to do this is for each member to independently explore its own action space through persistent variability in its output. This will cause the team as a whole to vary its collective actions. For example, a team of the actor units described in Section 15.8 explores the space of collective actions because the output of each unit, being a Bernoulli- logistic unit, probabilistically depends on the weighted sum of its input vector's components. The weighted sum biases firing probability up or down, but there is always variability. Because each unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts its weights with the goal of maximizing the average reward rate it experiences while stochastically exploring its own action space. One can show, as Williams (1992) did, that a team of Bernoulli- logistic REINFORCE units implements a policy gradient algorithm as a whole with respect to average rate of the team's common reward signal, where the actions are the collective actions of the team.",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Further, Williams (1992) showed that a team of Bernoulli- logistic units using REINFORCE ascends the average reward gradient when the units in the team are interconnected to form a multilayer ANN. In this case, the reward signal is broadcast to all the units in",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "the network, though reward may depend only on the collective actions of the network's output units. This means that a multilayer team of Bernoulli- logistic REINFORCE units learns like a multilayer network trained by the widely- used error backpropagation method, but in this case the backpropagation process is replaced by the broadcasted reward signal. In practice, the error backpropagation method is considerably faster, but the reinforcement learning team method is more plausible as a neural mechanism, especially in light of what is being learned about reward- modulated STDP as discussed in Section 15.8.",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Exploration through independent exploration by team members is only the simplest way for a team to explore; more sophisticated methods are possible if the team members coordinate their actions to focus on particular parts of the collective action space, either by communicating with one another or by responding to common inputs. There are also mechanisms more sophisticated than contingent eligibility traces for addressing structural credit assignment, which is easier in a team problem when the set of possible collective actions is restricted in some way. An extreme case is a winner- take- all arrangement (for example, the result of lateral inhibition in the brain) that restricts collective actions to those to which only one, or a few, team members contribute. In this case the winners get the credit or blame for resulting reward or punishment.",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Details of learning in cooperative games (or team problems) and non- cooperative game problems are beyond the scope of this book. The Bibliographical and Historical Remarks section at the end of this chapter cites a selection of the relevant publications, including extensive references to research on implications for neuroscience of collective reinforcement learning.",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "15.11 Model-based Methods in the Brain",
        "text_level": 1,
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Reinforcement learning's distinction between model- free and model- based algorithms is proving to be useful for thinking about animal learning and decision processes. Section 14.6 discusses how this distinction aligns with that between habitual and goal- directed animal behavior. The hypothesis discussed above about how the brain might implement an actor- critic algorithm is relevant only to an animal's habitual mode of behavior because the basic actor- critic method is model- free. What neural mechanisms are responsible for producing goal- directed behavior, and how do they interact with those underlying habitual behavior?",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "One way to investigate questions about the brain structures involved in these modes of behavior is to inactivate an area of a rat's brain and then observe what the rat does in an outcome- devaluation experiment (Section 14.6). Results from experiments like these indicate that the actor- critic hypothesis described above is too simple in placing the actor in the dorsal striatum. Inactivating one part of the dorsal striatum, the dorsolateral striatum (DLS), impairs habit learning, causing the animal to rely more on goal- directed processes. On the other hand, inactivating the dorsomedial striatum (DMS) impairs goal- directed processes, requiring the animal to rely more on habit learning. Results like these support the view that the DLS in rodents is more involved in model- free processes, whereas their DMS is more involved in model- based processes. Results of",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "studies with human subjects in similar experiments using functional neuroimaging, and with non- human primates, support the view that the analogous structures in the primate brain are differentially involved in habitual and goal- directed modes of behavior.",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Other studies identify activity associated with model- based processes in the prefrontal cortex of the human brain, the front- most part of the frontal cortex implicated in executive function, including planning and decision making. Specifically implicated is the orbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes. Functional neuroimaging in humans, and also recordings of the activities of single neurons in monkeys, reveals strong activity in the OFC related to the subjective reward value of biologically significant stimuli, as well as activity related to the reward expected as a consequence of actions. Although not free of controversy, these results suggest significant involvement of the OFC in goal- directed choice. It may be critical for the reward part of an animal's environment model.",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Another structure involved in model- based behavior is the hippocampus, a structure critical for memory and spatial navigation. A rat's hippocampus plays a critical role in the rat's ability to navigate a maze in the goal- directed manner that led Tolman to the idea that animals use models, or cognitive maps, in selecting actions (Section 14.5). The hippocampus may also be a critical component of our human ability to imagine new experiences (Hassabis and Maguire, 2007; Olafsdóttir, Barry, Saleem, Hassabis, and Spiers, 2015).",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The findings that most directly implicate the hippocampus in planning—the process needed to enlist an environment model in making decisions—come from experiments that decode the activity of neurons in the hippocampus to determine what part of space hippocampal activity is representing on a moment- to- moment basis. When a rat pauses at a choice point in a maze, the representation of space in the hippocampus sweeps forward (and not backwards) along the possible paths the animal can take from that point (Johnson and Redish, 2007). Furthermore, the spatial trajectories represented by these sweeps closely correspond to the rat's subsequent navigational behavior (Pfeiffer and Foster, 2013). These results suggest that the hippocampus is critical for the state- transition part of an animal's environment model, and that it is part of a system that uses the model to simulate possible future state sequences to assess the consequences of possible courses of action: a form of planning.",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The results described above add to a voluminous literature on neural mechanisms underlying goal- directed, or model- based, learning and decision making, but many questions remain unanswered. For example, how can areas as structurally similar as the DLS and DMS be essential components of modes of learning and behavior that are as different as model- free and model- based algorithms? Are separate structures responsible for (what we call) the transition and reward components of an environment model? Is all planning conducted at decision time via simulations of possible future courses of action as the forward sweeping activity in the hippocampus suggests? In other words, is all planning something like a rollout algorithm (Section 8.10)? Or are models sometimes engaged in the background to refine or recompute value information as illustrated by the Dyna architecture (Section 8.2)? How does the brain arbitrate between the use of the habit and goal- directed systems? Is there, in fact, a clear separation between the neural substrates of these systems?",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The evidence is not pointing to a positive answer to this last question. Summarizing the situation, Doll, Simon, and Daw (2012) wrote that \"model- based influences appear ubiquitous more or less wherever the brain processes reward information,\" and this is true even in the regions thought to be critical for model- free learning. This includes the dopamine signals themselves, which can exhibit the influence of model- based information in addition to the reward prediction errors thought to be the basis of model- free processes.",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Continuing neuroscience research informed by reinforcement learning's model- free and model- based distinction has the potential to sharpen our understanding of habitual and goal- directed processes in the brain. A better grasp of these neural mechanisms may lead to algorithms combining model- free and model- based methods in ways that have not yet been explored in computational reinforcement learning.",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "15.12 Addiction",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Understanding the neural basis of drug abuse is a high- priority goal of neuroscience with the potential to produce new treatments for this serious public health problem. One view is that drug craving is the result of the same motivation and learning processes that lead us to seek natural rewarding experiences that serve our biological needs. Addictive substances, by being intensely reinforcing, effectively co- opt our natural mechanisms of learning and decision making. This is plausible given that many- - though not all- - drugs of abuse increase levels of dopamine either directly or indirectly in regions around terminals of dopamine neuron axons in the striatum, a brain structure firmly implicated in normal reward- based learning (Section 15.7). But the self- destructive behavior associated with drug addiction is not characteristic of normal learning. What is different about dopamine- mediated learning when the reward is the result of an addictive drug? Is addiction the result of normal learning in response to substances that were largely unavailable throughout our evolutionary history, so that evolution could not select against their damaging effects? Or do addictive substances somehow interfere with normal dopamine- mediated learning?",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The reward prediction error hypothesis of dopamine neuron activity and its connection to TD learning are the basis of a model due to Redish (2004) of some- - but certainly not all- - features of addiction. The model is based on the observation that administration of cocaine and some other addictive drugs produces a transient increase in dopamine. In the model, this dopamine surge is assumed to increase the TD error,  $\\delta$ , in a way that cannot be cancelled out by changes in the value function. In other words, whereas  $\\delta$  is reduced to the degree that a normal reward is predicted by antecedent events (Section 15.6), the contribution to  $\\delta$  due to an addictive stimulus does not decrease as the reward signal becomes predicted: drug rewards cannot be \"predicted away.\" The model does this by preventing  $\\delta$  from ever becoming negative when the reward signal is due to an addictive drug, thus eliminating the error- correcting feature of TD learning for states associated with administration of the drug. The result is that the values of these states increase without bound, making actions leading to these states preferred above all others.",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Addictive behavior is much more complicated than this result from Redish's model, but the model's main idea may be a piece of the puzzle. Or the model might be misleading. Dopamine appears not to play a critical role in all forms of addiction, and not everyone is equally susceptible to developing addictive behavior. Moreover, the model does not include the changes in many circuits and brain regions that accompany chronic drug taking, for example, changes that lead to a drug's diminishing effect with repeated use. It is also likely that addiction involves model- based processes. Still, Redish's model illustrates how reinforcement learning theory can be enlisted in the effort to understand a major health problem. In a similar manner, reinforcement learning theory has been influential in the development of the new field of computational psychiatry, which aims to improve understanding of mental disorders through mathematical and computational methods.",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "15.13 Summary",
        "text_level": 1,
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The neural pathways involved in the brain's reward system are complex and incompletely understood, but neuroscience research directed toward understanding these pathways and their roles in behavior is progressing rapidly. This research is revealing striking correspondences between the brain's reward system and the theory of reinforcement learning as presented in this book.",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The reward prediction error hypothesis of dopamine neuron activity was proposed by scientists who recognized striking parallels between the behavior of TD errors and the activity of neurons that produce dopamine, a neurotransmitter essential in mammals for reward- related learning and behavior. Experiments conducted in the late 1980s and 1990s in the laboratory of neuroscientist Wolfram Schultz showed that dopamine neurons respond to rewarding events with substantial bursts of activity, called phasic responses, only if the animal does not expect those events, suggesting that dopamine neurons are signaling reward prediction errors instead of reward itself. Further, these experiments showed that as an animal learns to predict a rewarding event on the basis of preceding sensory cues, the phasic activity of dopamine neurons shifts to earlier predictive cues while decreasing to later predictive cues. This parallels the backing- up effect of the TD error as a reinforcement learning agent learns to predict reward.",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Other experimental results firmly establish that the phasic activity of dopamine neurons is a reinforcement signal for learning that reaches multiple areas of the brain by means of profusely branching axons of dopamine producing neurons. These results are consistent with the distinction we make between a reward signal,  $R_{t}$ , and a reinforcement signal, which is the TD error  $\\delta_{t}$  in most of the algorithms we present. Phasic responses of dopamine neurons are reinforcement signals, not reward signals.",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "A prominent hypothesis is that the brain implements something like an actor- critic algorithm. Two structures in the brain (the dorsal and ventral subdivisions of the striatum), both of which play critical roles in reward- based learning, may function respectively like an actor and a critic. That the TD error is the reinforcement signal for both the actor and the critic fits well with the facts that dopamine neuron axons target both the dorsal and ventral subdivisions of the striatum; that dopamine appears to be",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "critical for modulating synaptic plasticity in both structures; and that the effect on a target structure of a neuromodulator such as dopamine depends on properties of the target structure and not just on properties of the neuromodulator.",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "The actor and the critic can be implemented by ANNs consisting of neuron- like units having learning rules based on the policy- gradient actor- critic method described in Section 13.5. Each connection in these networks is like a synapse between neurons in the brain, and the learning rules correspond to rules governing how synaptic efficacies change as functions of the activities of the presynaptic and the postsynaptic neurons, together with neuromodulatory input corresponding to input from dopamine neurons. In this setting, each synapse has its own eligibility trace that records past activity involving that synapse. The only difference between the actor and critic learning rules is that they use different kinds of eligibility traces: the critic unit's traces are non- contingent because they do not involve the critic unit's output, whereas the actor unit's traces are contingent because in addition to the actor unit's input, they depend on the actor unit's output. In the hypothetical implementation of an actor- critic system in the brain, these learning rules respectively correspond to rules governing plasticity of corticostriatal synapses that convey signals from the cortex to the principal neurons in the dorsal and ventral striatal subdivisions, synapses that also receive inputs from dopamine neurons.",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "The learning rule of an actor unit in the actor- critic network closely corresponds to reward- modulated spike- timing- dependent plasticity. In spike- timing- dependent plasticity (STDP), the relative timing of pre- and postsynaptic activity determines the direction of synaptic change. In reward- modulated STDP, changes in synapses in addition depend on a neuromodulator, such as dopamine, arriving within a time window that can last up to 10 seconds after the conditions for STDP are met. Evidence is accumulating that reward- modulated STDP occurs at corticostriatal synapses, where the actor's learning takes place in the hypothetical neural implementation of an actor- critic system, adds to the plausibility of the hypothesis that something like an actor- critic system exists in the brains of some animals.",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "The idea of synaptic eligibility and basic features of the actor learning rule derive from Klopf's hypothesis of the \"hedonistic neuron\" (Klopf, 1972, 1981). He conjectured that individual neurons seek to obtain reward and to avoid punishment by adjusting the efficacies of their synapses on the basis of rewarding or punishing consequences of their action potentials. A neuron's activity can affect its later input, because the neuron is embedded in many feedback loops, some within the animal's nervous system and body and others passing through the animal's external environment. Klopf's idea of eligibility is that synapses are temporarily marked as eligible for modification if they participated in the neuron's firing (making this the contingent form of eligibility trace). A synapse's efficacy is modified if a reinforcing signal arrives while the synapse is eligible. We alluded to the chemotactic behavior of a bacterium as an example of a single cell that directs its movements in order to seek some molecules and to avoid others.",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "A conspicuous feature of the dopamine system is that fibers releasing dopamine project widely to multiple parts of the brain. Although it is likely that only some populations of dopamine neurons broadcast the same reinforcement signal, if this signal reaches the synapses of many neurons involved in actor- type learning, then the situation can",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "be modeled as a team problem. In this type of problem, each agent in a collection of reinforcement learning agents receives the same reinforcement signal, where that signal depends on the activities of all members of the collection, or team. If each team member uses a sufficiently capable learning algorithm, the team can learn collectively to improve performance of the entire team as evaluated by the globally- broadcast reinforcement signal, even if the team members do not directly communicate with one another. This is consistent with the wide dispersion of dopamine signals in the brain and provides a neurally plausible alternative to the widely- used error- backpropagation method for training multilayer networks.",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "The distinction between model- free and model- based reinforcement learning is helping neuroscientists investigate the neural bases of habitual and goal- directed learning and decision making. Research so far points to their being some brain regions more involved in one type of process than the other, but the picture remains unclear because model- free and model- based processes do not appear to be neatly separated in the brain. Many questions remain unanswered. Perhaps most intriguing is evidence that the hippocampus, a structure traditionally associated with spatial navigation and memory, appears to be involved in simulating possible future courses of action as part of an animal's decision- making process. This suggests that it is part of a system that uses an environment model for planning.",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Reinforcement learning theory is also influencing thinking about neural processes underlying drug abuse. A model of some features of drug addiction is based on the reward prediction error hypothesis. It proposes that an addicting stimulant, such as cocaine, destabilizes TD learning to produce unbounded growth in the values of actions associated with drug intake. This is far from a complete model of addiction, but it illustrates how a computational perspective suggests theories that can be tested with further research. The new field of computational psychiatry similarly focuses on the use of computational models, some derived from reinforcement learning, to better understand mental disorders.",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "This chapter only touched the surface of how the neuroscience of reinforcement learning and the development of reinforcement learning in computer science and engineering have influenced one another. Most features of reinforcement learning algorithms owe their design to purely computational considerations, but some have been influenced by hypotheses about neural learning mechanisms. Remarkably, as experimental data has accumulated about the brain's reward processes, many of the purely computationally- motivated features of reinforcement learning algorithms are turning out to be consistent with neuroscience data. Other features of computational reinforcement learning, such as eligibility traces and the ability of teams of reinforcement learning agents to learn to act collectively under the influence of a globally- broadcast reinforcement signal, may also turn out to parallel experimental data as neuroscientists continue to unravel the neural basis of reward- based animal learning and behavior.",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Bibliographical and Historical Remarks",
        "text_level": 1,
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "The number of publications treating parallels between the neuroscience of learning and decision making and the approach to reinforcement learning presented in this book is enormous. We can cite only a small selection. Niv (2009), Dayan and Niv (2008), Gimcher (2011), Ludvig, Belemare, and Pearson (2011), and Shah (2012) are good places to start.",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Together with economics, evolutionary biology, and mathematical psychology, reinforcement learning theory is helping to formulate quantitative models of the neural mechanisms of choice in humans and non- human primates. With its focus on learning, this chapter only lightly touches upon the neuroscience of decision making. Glimcher (2003) introduced the field of \"neuroeconomics,\" in which reinforcement learning contributes to the study of the neural basis of decision making from an economics perspective. See also Glimcher and Fehr (2013). The text on computational and mathematical modeling in neuroscience by Dayan and Abbott (2001) includes reinforcement learning's role in these approaches. Sterling and Laughlin (2015) examined the neural basis of learning in terms of general design principles that enable efficient adaptive behavior.",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "15.1 There are many good expositions of basic neuroscience. Kandel, Schwartz, Jessell, Siegelbaum, and Hudspeth (2013) is an authoritative and very comprehensive source.",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "15.2 Berridge and Kringelbach (2008) reviewed the neural basis of reward and pleasure, pointing out that reward processing has many dimensions and involves many neural systems. Space prevents discussion of the influential research of Berridge and Robinson (1998), who distinguish between the hedonic impact of a stimulus, which they call \"liking,\" and the motivational effect, which they call \"wanting.\" Hare, O'Doherty, Camerer, Schultz, and Rangel (2008) examined the neural basis of value- related signals from an economic perspective, distinguishing between goal values, decision values, and prediction errors. Decision value is goal value minus action cost. See also Rangel, Camerer, and Montague (2008), Rangel and Hare (2010), and Peters and Büchel (2010).",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "15.3 The reward prediction error hypothesis of dopamine neuron activity is most prominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis was first explicitly put forward by Montague, Dayan, and Sejnowski (1996). As they stated the hypothesis, it referred to reward prediction errors (RPEs) but not specifically to TD errors; however, their development of the hypothesis made it clear that they were referring to TD errors. The earliest recognition of the TD- error/dopamine connection of which we are aware is that of Montague, Dayan, Nowlan, Pouget, and Sejnowski (1993), who proposed a TD- error- modulated Hebbian learning rule motivated by results on dopamine signaling from Schultz's group. The connection was also pointed out in an abstract by Quartz, Dayan, Montague, and Sejnowski (1992). Montague and Sejnowski (1994) emphasized the importance of prediction in the brain and outlined how predictive Hebbian learning modulated by TD errors could be implemented via a diffuse neuromodulatory system, such as the dopamine system. Friston, Tononi, Reeke, Sporns,",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "and Edelman (1994) presented a model of value- dependent learning in the brain in which synaptic changes are mediated by a TD- like error provided by a global neuromodulatory signal (although they did not single out dopamine). Montague, Dayan, Person, and Sejnowski (1995) presented a model of honeybee foraging using the TD error. The model is based on research by Hammer, Menzel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that the neuromodulator octopamine acts as a reinforcement signal in the honeybee. Montague et al. (1995) pointed out that dopamine likely plays a similar role in the vertebrate brain. Barto (1995a) related the actor- critic architecture to basal- ganglionic circuits and discussed the relationship between TD learning and the main results from Schultz's group. Houk, Adams, and Barto (1995) suggested how TD learning and the actor- critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD- like error identified with dopamine to reinforce the selection of auditory input to be memorized. O'Reilly and Frank (2006) and O'Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine signals are RPEs but not TD errors. In support of their theory they cited results with variable interstimulus intervals that do not match predictions of a simple TD model, as well as the observation that higher- order conditioning beyond second- order conditioning is rarely observed, while TD learning is not so limited. Dayan and Niv (2008) discussed \"the good, the bad, and the ugly\" of how reinforcement learning theory and the reward prediction error hypothesis align with experimental data. Glimcher (2011) reviewed the empirical findings that support the reward prediction error hypothesis and emphasized the significance of the hypothesis for contemporary neuroscience.",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "15.4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg, Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge- Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenbock (2009). Fiorillo, Yun, and Song (2013), Lammel, Lim, and Malenka (2014), and Saddoris, Cacciapaglia, Wightman, and Carvelli (2015) are among studies showing that the signaling properties of dopamine neurons are specialized for different target regions. RPE- signaling neurons may belong to one among multiple populations of dopamine neurons having different targets and subserving different functions. Eshel, Tian, Bukwich, and Uchida (2016) found homogeneity of reward prediction error responses of dopamine neurons in the lateral VTA during classical conditioning in mice, though their results do not rule out response diversity across wider areas. Gershman, Pesaran, and Daw (2009) studied reinforcement learning tasks that can be decomposed into independent components with separate reward signals, finding evidence in human neuroimaging data suggesting that the brain exploits this kind of structure.",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "15.5 Schultz's 1998 survey article is a good entree into the very extensive literature on reward predicting signaling of dopamine neurons. Berns, McClure, Pagnoni, and Montague (2001), Breiter, Aharon, Kahneman, Dale, and Shizgal (2001), Pagnoni, Zink, Montague, and Berns (2002), and O'Doherty, Dayan, Friston, Critchley, and Dolan (2003) described functional brain imaging studies supporting the existence of signals like TD errors in the human brain.",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "15.6 This section roughly follows Barto (1995a) in explaining how TD errors mimic the main results from Schultz's group on the phasic responses of dopamine neurons.",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "15.7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv (2009). To the best of our knowledge, Barto (1995a) and Houk, Adams, and Barto (1995) first speculated about possible implementations of actor- critic algorithms in the basal ganglia. On the basis of functional magnetic resonance imaging of human subjects while engaged in instrumental conditioning, O'Doherty, Dayan, Schultz, Deichmann, Friston, and Dolan (2004) suggested that the actor and the critic are most likely located respectively in the dorsal and ventral striatum. Gershman, Mustafa, and Ludvig (2014) focused on how time is represented in reinforcement learning models of the basal ganglia, discussing evidence for, and implications of, various computational approaches to time representation.",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "The hypothetical neural implementation of the actor- critic architecture described in this section includes very little detail about known basal ganglia anatomy and physiology. In addition to the more detailed hypothesis of Houk, Adams, and Barto (1995), a number of other hypotheses include more specific connections to anatomy and physiology and are claimed to explain additional data. These include hypotheses proposed by Suri and Schultz (1998, 1999), Brown, Bullock, and Grossberg (1999), Contreras- Vidal and Schultz (1999), Suri, Bargas, and Arbib (2001), O'Reilly and Frank (2006), and O'Reilly, Frank, Hazy, and Watz (2007). Joel, Niv, and Ruppin (2002) critically evaluated the anatomical plausibility of several of these models and present an alternative intended to accommodate some neglected features of basal ganglionic circuitry.",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "15.8 The actor learning rule discussed here is more complicated than the one in the early actor- critic network of Barto et al. (1983). Actor- unit eligibility traces in that network were traces of just  $A_{t} \\times \\mathbf{x}(S_{t})$  instead of the full  $(A_{t} - \\pi (1|S_{t}, \\pmb {\\theta})) \\mathbf{x}(S_{t})$ . That work did not benefit from the policy- gradient theory presented in Chapter 13 or the contributions of Williams (1986, 1992), who showed how an ANN of Bernoulli- logistic units could implement a policy- gradient method.",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Reynolds and Wickens (2002) proposed a three- factor rule for synaptic plasticity in the corticostriatal pathway in which dopamine modulates changes in corticostriatal synaptic efficacy. They discussed the experimental support for this kind of learning rule and its possible molecular basis. The definitive demonstration of spike- timing- dependent plasticity (STDP) is attributed to Markram, Lübke, Frotscher, and Sakmann (1997), with evidence from earlier experiments",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "by Levy and Steward (1983) and others that the relative timing of pre- and postsynaptic spikes is critical for inducing changes in synaptic efficacy. Rao and Sejnowski (2001) suggested how STDP could be the result of a TD- like mechanism at synapses with non- contingent eligibility traces lasting about 10 milliseconds. Dayan (2002) commented that this would require an error as in Sutton and Barto's (1981a) early model of classical conditioning and not a true TD error. Representative publications from the extensive literature on reward- modulated STDP are Wickens (1990), Reynolds and Wickens (2002), and Calabresi, Picconi, Tozzi and Di Filippo (2007). Pawlak and Kerr (2008) showed that dopamine is necessary to induce STDP at the corticostriatal synapses of medium spiny neurons. See also Pawlak, Wickens, Kirkwood, and Kerr (2010). Yagishita, Hayashi- Takagi, Ellis- Davies, Urakubo, Ishii, and Kasai (2014) found that dopamine promotes spine enlargement of the medium spiny neurons of mice only during a time window of from 0.3 to 2 seconds after STDP stimulation. Izhikevich (2007) proposed and explored the idea of using STDP timing conditions to trigger contingent eligibility traces. Frémaux, Sprekeler, and Gerstner (2010) proposed theoretical conditions for successful learning by rules based on reward- modulated STDP.",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "15.9 Klopf's hedonistic neuron hypothesis (Klopf 1972, 1982) inspired our actor- critic algorithm implemented as an ANN with a single neuron- like unit, called the actor unit, implementing a Law- of- Effect- like learning rule (Barto, Sutton, and Anderson, 1983). Ideas related to Klopf's synaptically- local eligibility have been proposed by others. Crow (1968) proposed that changes in the synapses of cortical neurons are sensitive to the consequences of neural activity. Emphasizing the need to address the time delay between neural activity and its consequences in a reward- modulated form of synaptic plasticity, he proposed a contingent form of eligibility, but associated with entire neurons instead of individual synapses. According to his hypothesis, a wave of neuronal activity",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "leads to a short- term change in the cells involved in the wave such that they are picked out from a background of cells not so activated. ... such cells are rendered sensitive by the short- term change to a reward signal ... in such a way that if such a signal occurs before the end of the decay time of the change the synaptic connexions between the cells are made more effective. (Crow, 1968)",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Crow argued against previous proposals that reverberating neural circuits play this role by pointing out that the effect of a reward signal on such a circuit would \"...establish the synaptic connexions leading to the reverberation (that is to say, those involved in activity at the time of the reward signal) and not those on the path which led to the adaptive motor output.\" Crow further postulated that reward signals are delivered via a \"distinct neural fiber system,\" presumably the one into which Olds and Milner (1954) tapped, that would transform synaptic connections \"from a short into a long- term form.\"",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "In another farsighted hypothesis, Miller proposed a Law- of- Effect- like learning rule that includes synaptically- local contingent eligibility traces:",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "... it is envisaged that in a particular sensory situation neurone B, by chance, fires a 'meaningful burst' of activity, which is then translated into motor acts, which then change the situation. It must be supposed that the meaningful burst has an influence, at the neuronal level, on all of its own synapses which are active at the time ... thereby making a preliminary selection of the synapses to be strengthened, though not yet actually strengthening them. ...The strengthening signal ... makes the final selection ... and accomplishes the definitive change in the appropriate synapses. (Miller, 1981, p. 81)",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "Miller's hypothesis also included a critic- like mechanism, which he called a \"sensory analyzer unit,\" that worked according to classical conditioning principles to provide reinforcement signals to neurons so that they would learn to move from lower- to higher- valued states, thus anticipating the use of the TD error as a reinforcement signal in the actor- critic architecture. Miller's idea not only parallels Klopf's (with the exception of its explicit invocation of a distinct \"strengthening signal\"), it also anticipated the general features of reward- modulated STDP.",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "A related though different idea, which Seung (2003) called the \"hedonistic synapse,\" is that synapses individually adjust the probability that they release neurotransmitter in the manner of the Law of Effect: if reward follows release, the release probability increases, and decreases if reward follows failure to release. This is essentially the same as the learning scheme Minsky used in his 1954 Princeton PhD dissertation, where he called the synapse- like learning element a SNARC (Stochastic Neural- Analog Reinforcement Calculator). Contingent eligibility is involved in these ideas too, although it is contingent on the activity of an individual synapse instead of the postsynaptic neuron. Also related is the proposal of Unnikrishnan and Venugopal (1994) that uses the correlation- based method of Harth and Tzanakou (1974) to adjust ANN weights.",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "Frey and Morris (1997) proposed the idea of a \"synaptic tag\" for the induction of long- lasting strengthening of synaptic efficacy. Though not unlike Klopf's eligibility, their tag was hypothesized to consist of a temporary strengthening of a synapse that could be transformed into a long- lasting strengthening by subsequent neuron activation. The model of O'Reilly and Frank (2006) and O'Reilly, Frank, Hazy, and Watz (2007) uses working memory to bridge temporal intervals instead of eligibility traces. Wickens and Kotter (1995) discuss possible mechanisms for synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood (2015) provide evidence supporting the existence of contingent eligibility traces in synapses of cortical neurons with time courses like those of the eligibility traces Klopf postulated.",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "The metaphor of a neuron using a learning rule related to bacterial chemotaxis was discussed by Barto (1989). Koshland's extensive study of bacterial chemotaxis was in part motivated by similarities between features of bacteria and features of neurons (Koshland, 1980). See also Berg (1975). Shimansky (2009) proposed a",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "synaptic learning rule somewhat similar to Seung's mentioned above in which each synapse individually acts like a chemotactic bacterium. In this case a collection of synapses \"swims\" toward attractants in the high- dimensional space of synaptic weight values. Montague, Dayan, Person, and Sejnowski (1995) proposed a chemotactic- like model of the bee's foraging behavior involving the neuromodulator octopamine.",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "15.10 Research on the behavior of reinforcement learning agents in team and game problems has a long history roughly occurring in three phases. To the best of our knowledge, the first phase began with investigations by the Russian mathematician and physicist M. L. Tsetlin. A collection of his work was published as Tsetlin (1973) after his death in 1966. Our Sections 1.7 and 4.8 refer to his study of learning automata in connection to bandit problems. The Tsetlin collection also includes studies of learning automata in team and game problems, which led to later work in this area using stochastic learning automata as described by Narendra and Thathachar (1974, 1989), Viswanathan and Narendra (1974), Lakshmivarshan and Narendra (1982), Narendra and Wheeler (1983), and Thathachar and Sastry (2002). Thathachar and Sastry (2011) is a more recent comprehensive account. These studies were mostly restricted to non- associative learning automata, meaning that they did not address associative, or contextual, bandit problems (Section 2.9).",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "The second phase began with the extension of learning automata to the associative, or contextual, case. Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981b) experimented with associative stochastic learning automata in single- layer ANNs to which a global reinforcement signal was broadcast. The learning algorithm was an associative extension of the Alopex algorithm of Harth and Tzanakou (1974). Barto et al. called neuron- like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced an associative reinforcement learning algorithm called the associative reward- penalty (AR  $\\mathbb{P}$  ) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR  $\\mathbb{P}$  units connected into multi- layer ANNs, showing that they could learn nonlinear functions, such as XOR and others, with a globally- broadcast reinforcement signal. Barto (1985) extensively discussed this approach to ANNs and how this type of learning rule is related to others in the literature at that time. Williams (1992) mathematically analyzed and broadened this class of learning rules and related their use to the error backpropagation method for training multilayer ANNs. Williams (1988) described several ways that backpropagation and reinforcement learning can be combined for training ANNs. Williams (1992) showed that a special case of the AR  $\\mathbb{P}$  algorithm is a REINFORCE algorithm, although better results were obtained with the general AR  $\\mathbb{P}$  algorithm (Barto, 1985).",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "The third phase of interest in teams of reinforcement learning agents was influenced by increased understanding of the role of dopamine as a widely broadcast neuromodulator and speculation about the existence of reward- modulated STDP. Much more so than earlier research, this research considers details of synaptic plasticity and other constraints from neuroscience. Publications include the following (chronologically and alphabetically): Bartlett and Baxter (1999, 2000), Xie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian (2007), Izbikevich (2007), Pecevski, Maass, and Legenstein (2008), Legenstein, Pecevski, and Maass (2008), Kolodziejski, Porr, and Wörgötter (2009), Urbanczik and Senn (2009), and Vasilaki, Frémaux, Urbanczik, Senn, and Gerstner (2009). Nowé, Vrancx, and De Hauwere (2012) reviewed more recent developments in the wider field of multi- agent reinforcement learning",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "15.11 Yin and Knowlton (2006) reviewed findings from outcome- devaluation experiments with rodents supporting the view that habitual and goal- directed behavior (as psychologists use the phrase) are respectively most associated with processing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS). Results of functional imaging experiments with human subjects in the outcome- devaluation setting by Valentin, Dickinson, and O'Doherty (2007) suggest that the orbitofrontal cortex (OFC) is an important component of goal- directed choice. Single unit recordings in monkeys by Padoa- Schioppa and Assad (2006) support the role of the OFC in encoding values guiding choice behavior. Rangel, Camerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from the perspective of neuroeconomics about how the brain makes goal- directed decisions. Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience of internally generated sequences and presented a model of how these mechanisms might be components of model- based planning. Daw and Shohamy (2008) proposed that while dopamine signaling connects well to habitual, or model- free, behavior, other processes are involved in goal- directed, or model- based, behavior. Data from experiments by Bromberg- Martin, Matsumoto, Hong, and Hikosaka (2010) indicate that dopamine signals contain information pertinent to both habitual and goal- directed behavior. Doll, Simon, and Daw (2012) argued that there may not a clear separation in the brain between mechanisms that subserve habitual and goal- directed learning and choice.",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "15.12 Keiflin and Janak (2015) reviewed connections between TD errors and addiction. Nutt, Lingford- Hughes, Erritzoe, and Stokes (2015) critically evaluated the hypothesis that addiction is due to a disorder of the dopamine system. Montague, Dolan, Friston, and Dayan (2012) outlined the goals and early efforts in the field of computational psychiatry, and Adams, Huys, and Roiser (2015) reviewed more recent progress.",
        "page_idx": 42
    }
]