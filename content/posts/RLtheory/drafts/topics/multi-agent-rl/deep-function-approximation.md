---
_build:
  render: never
  list: never

date: "2025-07-19"
title: "E. Deep Function Approximation for MARL"
summary: "E. Deep Function Approximation for MARL"
lastmod: "2025-07-19"
category: "Notes"
series: ["RL Topics", "MARL"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---

## 5. Deep Learning and Function Approximation

**Function Approximation Fundamentals**
- What does deep learning offer over other techniques used to learn value functions, policies, and models in RL?
- Can I still afford a tabular solution?
- How do we make a value or policy function generalise to states the agent has never seen?
- Why isn't a simple linear model good enough?
- What really happens inside a 'universal function approximator'?

**Neural Network Architecture and Training**
- If gradient descent is so old, why does it still work at billion‑parameter scale?
- How large should my batch size be?
- Why bother with specialised architectures like CNNs and RNNs instead of sticking to MLPs?
- How can an agent remember what it saw earlier?
- Is back‑propagation just the chain rule?


# 1 Deep Learning and Function Approximation

## 1.1 What does deep learning offer over other techniques used to learn value functions, policies, and models in RL?

## 1.2 Can I still afford a tabular solution?

## 1.3 How do we make a value or policy function generalise to states the agent has never seen?

## 1.4 Why isn't a simple linear model good enough?

## 1.5 What really happens inside a 'universal function approximator'?

# 2 Neural Network Architecture and Training

## 2.1 If gradient descent is so old, why does it still work at billion‑parameter scale?

## 2.2 How large should my batch size be?

## 2.3 Why bother with specialised architectures like CNNs and RNNs instead of sticking to MLPs?

## 2.4 How can an agent remember what it saw earlier?

## 2.5 Is back‑propagation just the chain rule?
















