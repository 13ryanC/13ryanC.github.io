---
_build:
  render: never
  list: never

date: "2025-07-12"
title: "(5) Briefly on Distributional RL"
summary: "(5) Briefly on Distributional RL"
lastmod: "2025-07-12"
category: "Notes"
series: ["RL Topics"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---


### 1â€¯â€¯Core concepts established in the chapter

| Concept                                             | Formal definition / role                                                                                                            | Why it matters                                                                                                  |                                                                                                 |
| --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **Returnâ€‘distribution functionâ€¯Î·<sup>Ï€</sup>**      | Maps each state *x* to the *distribution* of the random return G<sub>Ï€</sub>(x) under policyâ€¯Ï€.                                     | Puts the full uncertainty about longâ€‘term rewards, not just the mean, at centre stage.                          |                                                                                                 |
| **Distributional Bellman operatorâ€¯*T*<sup>Ï€</sup>** | (T<sup>Ï€</sup>â€¯Î·)(x)Â =â€¯ğ”¼<sub>Ï€</sub>\[(rÂ +â€¯Î³â€¯Â·â€¯Î·(Xâ€²))â€¯\mid Xâ€¯=â€¯x] â€“ a pushâ€‘forward that shiftsÂ & scales distributions then mixes across actions, rewards and next states. | Fundamental fixedâ€‘point equation for Î·<sup>Ï€</sup>; contractive in Wasserstein/CramÃ©r metrics.  |
| **Probabilityâ€‘distribution representations**        | Finiteâ€‘parameter families F âŠ‚ P(â„) used to *store* return distributions in memory (e.g., empirical, normal, categorical, quantile). | Choice of F controls **expressivenessâ€¯â†”â€¯tractability**. No single best representation.                          |                                                                                                 |
| **Projection operatorâ€¯Î <sub>F</sub>**               | Maps an arbitrary distribution to its â€œbestâ€ element in representationâ€¯F (best in `â„“â‚‚` for categorical, in Wâ‚ for quantile).        | Allows iterative algorithms even when F is *not* closed underÂ *T*<sup>Ï€</sup>.                                  |                                                                                                 |
| **Distributional Dynamic Programming (DDP)**        | Generic loop Î·<sub>k+1</sub>Â =Â Î <sub>F</sub>Â T<sup>Ï€</sup>â€¯Î·<sub>k</sub> (Alg.â€¯5.2).                                                | Unifies all exact/approximate returnâ€‘distribution evaluators.                                                   |                                                                                                 |
| **Contraction & Lipschitz analysis**                | kT<sup>Ï€</sup>k<sub>Wâ‚š</sub>Â =â€¯Î³; kÎ <sub>C</sub>T<sup>Ï€</sup>k<sub>â„“â‚‚</sub>Â â‰¤â€¯âˆšÎ³/2; etc.                                            | Guarantees convergence & provides nonâ€‘asymptotic error bounds.                                                  |                                                                                                 |

---

### 2â€¯â€¯Key arguments developed by the authors

| Claim                                                                       | Supporting reasoning / evidence                                                                                                                                                                                               |
| --------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **(i) Exact computation with full empirical support is *intractable*.**     | Number of particles after *k* iterations can grow as O(N<sub>X</sub>â½áµâ¾Â N<sub>R</sub>â½áµâ¾); computing Î· exactly is NPâ€‘hard via reduction from Hamiltonian cycle (Remarkâ€¯5.2).                                                  |
| **(ii) Different representations give orthogonal tradeâ€‘offs.**              | *Empirical* is expressive but explodes; *Normal* is closedâ€‘form but often a bad fit (Fig.â€¯5.1 shows bimodality loss); *Fixedâ€‘size empirical* (categorical, quantile, mâ€‘particle) hit a â€œGoldilocksâ€ zone (Sectionâ€¯5.5).       |
| **(iii) Projection choice dictates stability.**                             | Categorical projection is an â„“â‚‚â€‘orthogonal projection â‡’ nonâ€‘expansion, yields a âˆšÎ³/2â€‘contraction; quantile projection is only a Î³â€‘contraction in Wâ‚, but still converges; arbitrary projections can diverge (Exerciseâ€¯5.19).  |
| **(iv) â€˜Diffusionâ€™ is a real risk unless Î <sub>F</sub> is diffusionâ€‘free.** | Example 5.19 (Fig.â€¯5.6) shows categorical DP spreading probability mass far beyond the optimal support along a deterministic chain.                                                                                           |
| **(v) Approximation error is bounded and tunable.**                         | For mâ€‘categorical: â„“Ì„â‚‚(Î·Ì‚,â€¯Î·)â€¯â‰¤â€¯(V<sub>max</sub>âˆ’V<sub>min</sub>)/(mâ€‘1)(1âˆ’âˆšÎ³) (Cor.â€¯5.29). For mâ€‘quantile: Wâ‚(Î·Ì‚,â€¯Î·)â€¯â‰¤â€¯3(V<sub>max</sub>âˆ’V<sub>min</sub>)/(2m(1âˆ’Î³)) (Lemmaâ€¯5.30).                                             |

---

### 3â€¯â€¯Practical frameworks & algorithms you can apply

| Framework                                   | Inputs & outputs                                                              | Complexity                                                                              | When to use                                                                                  |
| ------------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **Algorithmâ€¯5.1 â€“ Empirical T<sup>Ï€</sup>** | Takes empirical Î·, returns exact T<sup>Ï€</sup>â€¯Î· (still empirical).           | O(N<sub>X</sub>N<sub>R</sub>N<sub>A</sub>) per state.                                   | Diagnostic baseline; impractical for large *k*.                                              |
| **Algorithmâ€¯5.2 â€“ DDP template**            | Parameters: representationâ€¯F, projection Î <sub>F</sub>, iterationsâ€¯K.         | Dominated by cost of T<sup>Ï€</sup> + projection.                                        | Skeleton for any distributional evaluator.                                                   |
| **Algorithmâ€¯5.3 â€“ *Categorical* DP (CDP)**  | Probabilities p<sub>i</sub>(x) on fixed grid Î¸<sub>i</sub>.                   | O(Kâ€¯mâ€¯N<sub>R</sub>N<sub>X</sub>Â²).                                                     | Finite supports; meanâ€‘preserving (if grid covers returns); easy to implement; basis of C51.  |
| **Algorithmâ€¯5.4 â€“ *Quantile* DP (QDP)**     | Locations Î¸<sub>i</sub>(x) with equal weights 1/m.                            | O(Kâ€¯mâ€¯N<sub>R</sub>N<sub>X</sub>Â²â€¯logâ€¯m) (sorting) or faster with incremental variants. | When tail accuracy matters; basis of QRâ€‘DQN, IQN.                                            |
| **Normalâ€‘DP (Sectionâ€¯5.4)**                 | Stores meanâ€¯Âµ and varianceâ€¯ÏƒÂ² per state. Closedâ€‘form updates (Eqsâ€¯5.21â€‘5.22). | O(Kâ€¯N<sub>X</sub>Â²) â€“ same as classical value iteration.                                | Rapid rough estimates; risk metrics derived from ÏƒÂ².                                         |

Implementation checklist for CDP/QDP in your own codebase

1. **Choose grid or quantile countâ€¯m.** Trade memory vs bound above.
2. **Preâ€‘compute r<sub>Ï€</sub>(x) and P<sub>Ï€</sub>(xâ†’xâ€²)** if modelâ€‘based; else sampleâ€‘based update (next chapter).
3. **Loop** `for k in range(K):`

   * **Update**: build empirical mixture via Eq.â€¯5.8 (Algorithmâ€¯5.1).
   * **Project**:

     * *Categorical*: distribute mass with triangular kernels (Fig.â€¯5.3).
     * *Quantile*: compute midâ€‘quantiles Ï„<sub>i</sub>â€¯=â€¯(2iâˆ’1)/2m (Fig.â€¯5.4).
4. **Stop** when â€–Î·<sub>k+1</sub>âˆ’Î·<sub>k</sub>â€–<Îµ or after Kâ‰¥âŒˆlog(Îµâ»Â¹)/log(1/Î³)âŒ‰.

---

### 4â€¯â€¯Notable insights & illustrative examples

* **Visual intuition.**

  * *Figureâ€¯5.1* (p.â€¯126) shows that mixing two Gaussians produces a bimodal curve; a single normal fit (solid) misses multimodality â€“ a caution against overâ€‘simplified models.
  * *Figureâ€¯5.2* (p.â€¯129) contrasts mâ€‘categorical, mâ€‘quantile, and mâ€‘particle approximations of the same distribution, making particleâ€‘location vs particleâ€‘weight tradeâ€‘offs tangible.
  * *Figureâ€¯5.6* (p.â€¯140) demonstrates *diffusion*: CDP spreads mass along a 10â€‘state chain, whereas the optimal categorical projection would remain sharply peaked.
  * *Figureâ€¯5.7* (p.â€¯146) plots how increasing m (3â†’33) progressively sharpens the estimated returns in the Cliffs domain.&#x20;

* **NPâ€‘hardness (Remarkâ€¯5.2).** Computing *exact* distribution support can encode Hamiltonian cycle; underscores need for approximation.&#x20;

* **Meanâ€‘preservation vs variance inflation.** Categorical DP keeps the expected value intact (if Î¸â‚..Î¸\_m span returns) but tends to overâ€‘estimate variance, which can be desirable for optimistic exploration or detrimental for riskâ€‘sensitive control.&#x20;

* **Projection metrics matter.** Switching from â„“â‚‚ to Wâ‚ changes both theory (contraction constant) and empirical behaviour (tail coverage). Offers a design knob when adapting to domain requirements (e.g., financial tails vs robotics safety).&#x20;

---

### 5â€¯â€¯Synthesis & value for broader study

* **Bridges expectationâ€‘based RL and fullâ€‘distribution RL.** The chapter generalises classical dynamic programming (Bellman Eq.Â 5.2) to *distributional* form and shows how established convergence tools (contractions, fixedâ€‘point theorems) extend when the state space is upgraded from â„ to P(â„).

* **Provides a **design catalogue**.** By dissecting representations and projections, it equips practitioners to choose or invent the right tool for a given accuracyâ€‘budgetâ€‘risk profile â€“ an essential step before moving to sampleâ€‘based, functionâ€‘approximation, or control settings in later chapters.

* **Sets theoretical guarantees behind popular algorithms.** C51, QRâ€‘DQN, IQN and their continuousâ€‘control variants all rely on CDP/QDP style updates. Understanding their fixedâ€‘point properties clarifies *why* they work and when they might fail (e.g., diffusion, insufficient support).

* **Connects to risk and robustness.** Variance Bellman equations (Normalâ€‘DP) and Wasserstein analysis pave the way for riskâ€‘aware objectives, distributionallyâ€‘robust RL, and uncertainty quantification â€“ themes prevalent in modern safe RL research.

* **Highlights open problems.** Efficient projection for mâ€‘particle mixtures, reducing diffusion, and tailoring metrics to taskâ€‘specific costs are all identified as fertile ground for innovation.

---

#### Practical takeâ€‘aways for your own work

1. **Start with CDP if you need a dropâ€‘in replacement for value iteration.** Pick Î¸<sub>1</sub>,â€¯Î¸<sub>m</sub> to bracket theoretical return extremes; set mâ‰ˆâ€¯20â€“51 for Atariâ€‘scale tasks.
2. **Move to QDP when tail risk matters or when continuous control actions create highly skewed returns.**
3. **Use Normalâ€‘DP only for coarse safety checks or as a baseline; verify multimodality visually (cf. Fig.â€¯5.1).**
4. **Monitor diffusion** by plotting support width over iterations; tighten projection or enlarge m if you see uncontrolled spread.
5. **Leverage bounds** (Cor.â€¯5.29; Lemmaâ€¯5.30) to budget *m* given desired Wasserstein / CramÃ©r error Îµ: mâ€¯â‰ˆâ€¯O((V<sub>max</sub>âˆ’V<sub>min</sub>)/(Îµ(1âˆ’Î³))).

By internalising these concepts and frameworks, youâ€™ll be able to reason formally about uncertainty in returns, prototype distributional evaluators, and lay dependable foundations for distributional RL algorithms in both research and applied settings.


---



Below is a â€œdictionaryâ€ that translates the two canonical control routines of *classical* dynamic programming (DP) into their natural counterparts in **distributional** dynamic programming (DDP).
Throughout, letâ€¯Ï€ be a stationary policy, letâ€¯F be the probabilityâ€‘distribution representation you have chosen (categorical, quantile, mâ€‘particle, â€¦), and letâ€¯Î \_F denote the projection that maps an arbitrary returnâ€‘distribution to F (Algorithmsâ€¯5.2â€‘5.4 give concrete realisations for the popular categorical and quantile cases).&#x20;

# Classical vs Distributional Reinforcement Learning Algorithms

| Classical Concept | Recursion (scalar value functions) | Distributional Analogue | Recursion (return-distribution functions) | Notes |
|-------------------|-------------------------------------|-------------------------|-------------------------------------------|--------|
| **Iterative policy evaluation** (core of *policy iteration*, also used as a stand-alone algorithm) | \(V_{k+1} = T^{\pi} V_k\), where \(T^{\pi}\) is the Bellman *expectation* operator | **Distributional policy evaluation** (Algorithm 5.2 â€” "Distributional Dynamic Programming") | \(\eta_{k+1} = \Pi_F T^{\pi} \eta_k\) | This is exactly the loop described in Algorithm 5.2; it keeps the policy fixed and successively sharpens the *full* return distribution at every state. Converges when \(\Pi_F T^{\pi}\) is a contraction (proved for categorical/\(\ell_2\) and quantile/\(W_1\) cases). |
| **Value iteration** | \(V_{k+1} = T^* V_k\), \(T^* v(x) = \max_a \mathbb{E}[R + \gamma v(X') \mid x,a]\) | **Distributional value iteration (DVI)** | \(\eta_{k+1} = \Pi_F T^* \eta_k\), \(T^* \eta(x) = \max_a \mathbb{E}[(R,\gamma) \eta(X') \mid x,a]\) | Replace the policy-specific operator by the *optimal* distributional Bellman operator, then project. DVI collapses to classical VI when you keep only the mean of each \(\eta\). Converges for \(\gamma<1\) under the same metric-projection conditions as evaluation. |
| **Policy iteration (Howard)** | **(1)** Evaluate \(\pi_k\): solve \(V^{\pi_k}\) (exactly or approximately). **(2)** Improve: \(\pi_{k+1}(x) = \arg\max_a \mathbb{E}[R + \gamma V^{\pi_k}(X') \mid x,a]\) | **Distributional policy iteration (DPI)** | **(1)** Evaluate \(\pi_k\) with DDP: \(\eta^{\pi_k} = \lim_K \Pi_F (T^{\pi_k})^K \eta_0\). **(2)** Improve: \(\pi_{k+1}(x) = \arg\max_a \mathbb{E}_{Z \sim \eta^{\pi_k}(x,a)}[Z]\) (or another risk-criterion of interest). | Step (1) is categorical/quantile DP; step (2) uses the expectation of the evaluated distribution unless you purposely adopt a risk-sensitive rule (e.g., maximise VaR, minimise CVaR, etc.). DPI inherits the monotonic policy-improvement guarantee when the improvement rule is expectation-based. |

### Where the algorithms live in Chapterâ€¯5

* **Algorithmâ€¯5.2** (â€œDistributional Dynamic Programmingâ€) realises the evaluation half of DPI and, when you swap *T^Ï€* for *T^â˜…*, gives a concrete template for DVI.&#x20;
* **Algorithmsâ€¯5.3â€¯&â€¯5.4** plug categorical or quantile projections into that template, yielding **Categorical DP (CDP)** and **Quantile DP (QDP)** â€” readyâ€‘made implementations of both evaluation and control once you decide whether you are in the *Ï€â€‘fixed* (evaluation) or *greedy/improvement* (control) regime.&#x20;

### Practical guidance for choosing among them

| Goal                                         | Recommended distributional routine | Typical representation / projection | Why                                                                                                 |
| -------------------------------------------- | ---------------------------------- | ----------------------------------- | --------------------------------------------------------------------------------------------------- |
| Fast approximate control with limited memory | **Categorical DVI** (CDP + T^\*)   | mâ€‘categorical, â„“â‚‚ projection        | O(Kâ€¯mâ€¯NÂ²) like VI; support fixed â‡’ simple greedy step; mean preserved â‡’ same policy as VI when mâ†’âˆ. |
| Tailâ€‘risk aware control                      | **Quantile DVI / DPI**             | mâ€‘quantile, Wâ‚ projection           | Gives direct access to quantiles and CVaR; contraction in Wâ‚ guarantees stability.                  |
| Research on new risk criteria                | **Generic DPI**                    | Any F + appropriate Î \_F            | Keep evaluation generic; plug preferred risk measure into the improvement step.                     |

> **Key takeâ€‘away** â€“Â **Replace the scalar Bellman operators in VI/PI with their distributional counterparts, sandwich every update inside a projection Î \_F, and you obtain the oneâ€‘toâ€‘one distributional versions of value iteration and policy iteration.**
> The rest is an implementation choice (categorical vs. quantile vs. something richer) governed by the tradeâ€‘offs in Tableâ€¯5â€‘1 of the chapter (expressiveness, tractability, diffusion, meanâ€‘preservation).&#x20;
