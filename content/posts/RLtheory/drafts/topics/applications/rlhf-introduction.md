---
_build:
  render: never
  list: never

date: "2025-07-17"
title: "Reinforcement Learning from Human Feedback"
summary: "Reinforcement Learning from Human Feedback"
lastmod: "2025-07-17"
category: "Notes"
series: ["RL Topics"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---

## Motivation of optimising for human preferences

The core of RLHF is designed to optimise machine learning models where the design of reward functions is hard, where simple things are hard and hard things are easy by Moravecâ€™s paradox.
















































































