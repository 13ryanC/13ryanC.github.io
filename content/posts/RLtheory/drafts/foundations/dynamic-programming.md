---
_build:
  render: never
  list: never

date: "2025-07-03"
title: "(Part 4.2) Approximate Policy Iteration Theorem"
summary: "Approximate Policy Iteration Theorem"
category: "Tutorial"
series: ["RL Theory"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
sources:
  - title: "Lecture 1 (2022-01-05)"
    url: "http://www.youtube.com/watch?v=rjwxqcVrVws"
  - title: "Lecture 1 (2021-01-12)"
    url: "http://www.youtube.com/watch?v=0oJmSULoj3I"
---

### Motivation for **Approximate Policyâ€¯Iteration (API)**

---

#### 1.  Why add function approximation to policyâ€‘iteration?

Classical **policy iteration (PI)** alternates

1. **exact policy evaluation:** solve the linear system

$$
v_{Ï€}=r_{Ï€}+\gamma P_{Ï€}v_{Ï€},\qquad\text{i.e. }v_{Ï€}=(I-\gamma P_{Ï€})^{-1}r_{Ï€},
$$

2. **greedy improvement:** \(Ï€'\!(s)=\arg\max_{a}q_{Ï€}(s,a)\).

For a finite Markov Decision Process with \(|S|\) states, stepâ€¯1 requires inverting an \(|S|\times|S|\) matrixâ€”cubic in \(|S|\)â€”and storing the value table, **both infeasible when \(|S|\) is huge or continuous**. The natural idea is to **compress** the value/actionâ€‘value functions into a lowâ€‘dimensional linear space spanned by handâ€‘crafted or learned features \(\varphi\).

---

#### 2.  Formal compression model

* **Featureâ€‘map**â€ƒ\(\varphi:S\times A\to\mathbb R^{d}\) (or \(\varphi:S\to\mathbb R^{d}\) for state values).
* **Linear approximation space**â€ƒ\(F_{\varphi}:=\{\Phi\theta\mid\theta\in\mathbb R^{d}\}\) with \((\Phi\theta)(s,a)=\langle\varphi(s,a),\theta\rangle\).

A policyâ€™s actionâ€‘value function \(q_{Ï€}\) is **Îµâ€‘realisable** if

$$
\inf_{\theta}\|q_{Ï€}-\Phi\theta\|_{\infty}\le Îµ .
$$

Assumption **B2** (â€œapproximate universal actionâ€‘value realisabilityâ€) posits that **every** policy enjoys this property with the *same* Îµ.  With B2 the planner may always stay inside \(F_{\varphi}\).&#x20;

---

#### 3.  Two guiding questionsâ€‚(lectureâ€‘8, first paragraph)

| Question          | Operational meaning                                                                        | Desired answer                                                                                                                                                                                                                                  |      |                                                                                                                                    |
| ----------------- | ------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------------------------------------- |
| **Efficiency**    | \*Can we evaluate & improve a policy in time polynomial in d, Î³, log(1/Î´) â€” but **not** in | S                                                                                                                                                                                                                                               | â€¯?\* | Yes: weighted leastâ€‘squares on O(dÂ²) cleverly chosen stateâ€‘action pairs costs **O(dÂ³)**; rollout data needs only poly(d) samples.  |
| **Effectiveness** | *After K iterations, how subâ€‘optimal is the final policy?*                                 | API achieves \(\displaystyle\|v^{*}-v_{Ï€_{K}}\|_{\infty}\le\frac{Î³^{K}}{1-Î³}+\frac{2}{(1-Î³)^{2}}(Îµ_{\text{apx}}+Îº)\). Both terms are tunable: increase K for the â€œiteration errorâ€, increase samples m & horizon H for the â€œevaluation errorâ€ Îº.  |      |                                                                                                                                    |

*Takeâ€‘away:* linear features potentially turn an intractable \(|S|\)-scale dynamicâ€‘programming problem into a **dimensionâ€‘only** numerical linearâ€‘algebra problem.

---

#### 4.  Where exactly do the computational savings arise?

1. **Policy evaluation via LSPEâ€‘G**

   * Choose a **Gâ€‘optimal design** \((C,Ï)\), |C|â€¯â‰¤â€¯d(d+1)/2.
   * Collect m rollout returns per design point (cost: O(mâ€‰|C|)).
   * Solve the **weighted normal equations**
     \(\hat Î¸ = G_{Ï}^{-1}\sum_{zâˆˆC}Ï(z)\widehat R^{m}(z)\varphi(z)\)
     in O(dÂ³) arithmetic.&#x20;

2. **Greedy step** needs only dot products \(\langle\hat Î¸,\varphi(s,a)\rangle\) â€” O(d) per state, independent of |S|.

3. **Sample complexity** to keep Îº â‰¤ Îµâ€² is poly\((d,(1-Î³)^{-1},Îµâ€²^{-1})\); no |S| appears.

---

#### 5.  Why is *effectiveness* nonâ€‘trivial?

Compressing \(q_{Ï€}\) introduces **approximation error** Îµ; sampling introduces **statistical error**; solving leastâ€‘squares on a small design set incurs **extrapolation error**.  APIâ€™s analysis shows these three errors:

$$
\boxed{\text{Subâ€‘optimality}}
  \;=\;
  \underbrace{\frac{Î³^{K}}{1-Î³}}_{\text{iteration}}
  +\underbrace{\frac{2}{(1-Î³)^{2}}Îº}_{\text{eval}}
  +\underbrace{\frac{2(1+âˆšd)}{(1-Î³)^{2}}Îµ}_{\text{approx}}
$$

remain **additive** rather than multiplicative, and âˆšd is the *minimal* extrapolation factor (Kieferâ€“Wolfowitz).  Thus a moderate feature dimension keeps Îµâ€‘blowâ€‘up tolerable.&#x20;

---

#### 6.  Conceptual bridge to the rest of the lecture

| Next section                                       | Key concern originating from motivation                          |
| -------------------------------------------------- | ---------------------------------------------------------------- |
| **Approx. universal valueâ€‘function realisability** | Makes Îµ explicit; decides whether API is even *applicable*.      |
| **Approx. policy evaluation**                      | Supplies Îº; must hit polynomialâ€‘time target.                     |
| **Extrapolation control**                          | Prevents Îº from ballooning outside design set C.                 |
| **Kieferâ€“Wolfowitz & corollaries**                 | Show âˆšd is achievable and nearâ€‘optimal.                          |
| **Geometric progress â†’ API theorem**               | Converts perâ€‘iteration Îº into global performance bound.          |
| **LSPI guarantee**                                 | Implements the whole pipeline; answers the motivating questions. |

---

### ProgressÂ (â€ŠstepÂ 1 of filling in detailsâ€Š)

* **Delivered** a full, rigorously referenced motivation section explaining (i) why approximation is necessary, (ii) how linear features lead to polynomialâ€‘time planning, and (iii) what error decomposition underpins effectiveness.
* **Extracted** the exact statements of efficiency/effectiveness from *lecâ€¯8*Â Â§1 and linked them to later technical machinery.

**Open issues & next steps**

1. Proceed to **Sectionâ€¯2: Approximate Universal Valueâ€‘Function Realisability** â€” formal definitions, examples, and its necessity.
2. Provide expanded derivations for LSPE error bounds and the Kieferâ€“Wolfowitz theoremâ€™s role.
3. Eventually integrate diagrams (e.g., the stateâ€‘space Venn sketch on *lecâ€¯7*Â p.â€¯4) to visualise hierarchy of assumptions.

Let me know when to move on to Sectionâ€¯2 or if youâ€™d like further elaboration here.

---


\###â€¯2â€ƒApproximate Universalâ€¯Valueâ€‘Functionâ€¯Realisability â€¯(Assumptionâ€¯B2)

---

| Item                                       | Content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                              |                                                     |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **(a)â€¯Formal statement**                   | Let Ï•â€¯:â€¯Sâ€¯Ã—â€¯Aâ€¯â†’â€¯â„áµˆ be a fixed featureâ€‘map and let ğ”½Ï•Â â‰œâ€¯{Î¦Î¸â€¯:â€¯Î¸â€¯âˆˆâ€¯â„áµˆ}.  We say that the MDP *and* Ï• satisfy **B2(Îµ)** if for **every** deterministic or stochastic, memoryâ€‘less policy Ï€,â€ƒ\(\displaystyle \inf_{Î¸\in\mathbb{R}^{d}}\|q_{Ï€}-Î¦Î¸\|_{\infty}\le Îµ\).  The shorthand notation is \(q_{Ï€}\in_{Îµ}\mathcal F_{Ï•}\).                                                                                                                                                                         |                                                                                                                              |                                                     |
| **(b)â€¯Why B2 is needed**                   | *Uniformity across policies.*  In API we repeatedly evaluate and improve **intermediate policies Ï€â‚€,Ï€â‚,â€¦**.  To keep the approximation error from exploding we must know *a priori* that **each** qÏ€â€¯admits an Îµâ€‘close representation in the same lowâ€‘dimensional space; otherwise the greedy step could leave the span of features after one iteration and the analysis would collapse.  Lectureâ€¯7 points out precisely this danger (paragraph below Eq.â€¯(1) and the Venn diagram on pageâ€¯4).  |                                                                                                                              |                                                     |
| **(c)â€¯Key consequences**                   | 1. *Bounded approximation term.*  Setting Îµ\_apxÂ â‰œÂ supÏ€Â infÎ¸â€–qÏ€Â âˆ’Â Î¦Î¸â€–âˆ, B2(Îµ) asserts Îµ\_apxâ€¯â‰¤â€¯Îµ; this constant appears additively (and *only* additively) in all later API bounds.  <br>2. *Existence of nearâ€‘greedy parameters.*  For every Ï€ there exists Î¸Ï€ with maxâ€‘norm error â‰¤â€¯Îµ; hence Ï€â€™s greedy successor can be implemented with a dotâ€‘product of Î¸Ï€ and featuresâ€”cost O(d).                                                                                                         |                                                                                                                              |                                                     |
| **(d)â€¯Proof of sufficiency for API**       | Assume LSPEâ€‘G produces Î¸Ì‚ such that â€–qÏ€â€¯âˆ’â€¯Î¦Î¸Ì‚â€–âˆâ€¯â‰¤â€¯Îº (Sectionâ€¯6).  Under B2 the total policyâ€‘evaluation error is â‰¤â€¯Îºâ€¯+â€¯(1+âˆšd)Îµ (Lemmaâ€¯6, Eq.â€¯(7) in lectureâ€¯8) and the API theorem (Eq.â€¯(12)) then givesâ€ƒ\(\|v^{*}-v_{Ï€_k}\|_{\infty}\le\frac{Î³^{k}}{1-Î³}+\frac{2}{(1-Î³)^{2}}(Îº+(1+\sqrt d)Îµ)\).   Thus *all* three error sourcesâ€”iteration, evaluation and approximationâ€”remain controlled.                                                                                                       |                                                                                                                              |                                                     |
| **(e)â€¯Necessity (counterâ€‘example sketch)** | Without B2 there exists an MDP where qÏ€ cannot be approximated uniformly.  Start with a twoâ€‘state, twoâ€‘action deterministic MDP and choose Ï• so that the first policyâ€™s value lies in span(Ï•) but the optimal policyâ€™s value differs on exactly one state by +1.  APIâ€™s greedy step exits the span so Î¸Ì‚ cannot approximate qÏ€â‚, breaking the extrapolation lemmaâ€™s preâ€‘condition and voiding the convergence proof.                                                                            |                                                                                                                              |                                                     |
| **(f)â€¯Sufficient structural conditions**   | *Linear MDPs.*  If rewards and transitions factorise as r(s,a)=âŸ¨Ï•(s,a),Î¸\_râŸ© and P(sâ€²                                                                                                                                                                                                                                                                                                                                                                                                           | s,a)=âŸ¨Ï•(s,a),Î½(sâ€²)âŸ© (Lectureâ€¯7, pageâ€¯6), then **B2(0)** holds because \(q_{Ï€}=Î¦Î¸_{Ï€}\) with Î¸Ï€Â =Â Î¸\_rÂ +Â Î³â€¯âˆ‘*{sâ€²}Î½(sâ€²)â€¯E*{aâˆ¼Ï€(Â· | sâ€²)}Î¸Ï€.  This fixedâ€‘point equation keeps Î¸Ï€ in â„áµˆ.  |
| **(g)â€¯Approximate linear MDPs**            | If rewards and transitions are only Îµâ‚€ close to such factorizations (Eq.â€¯(2), lectureâ€¯7â€¯p.â€¯6), then B2(Îµ) holds with Îµâ€¯=â€¯Îµâ‚€â€¯Â·â€¯(1+Î³)/(1âˆ’Î³).                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                              |                                                     |
| **(h)â€¯Practical diagnostics**              | *Empirical compatibility test.*  Collect rollout tuples (s,a,r,sâ€²) under a mix of exploratory policies and fit Î¸ via leastâ€‘squares to \(\hat r+Î³\max_{aâ€²}\langle Î¸,\varphi(sâ€²,aâ€²)\rangle\).  If the residualâ€™s maxâ€‘norm is large, B2 is violated for the current feature setâ€”suggesting feature engineering or representation learning is required before running API.                                                                                                                            |                                                                                                                              |                                                     |

---

\####â€¯Examples illustrating B2

| Example                                                                       | Îµ     | Explanation                                                                                                                  |   |   |   |                        |
| ----------------------------------------------------------------------------- | ----- | ---------------------------------------------------------------------------------------------------------------------------- | - | - | - | ---------------------- |
| Finiteâ€‘horizon Linear Quadratic Regulator with Fourier features of degreeâ€¯â‰¤â€¯2 | 0     | Dynamics and cost are quadratic; value functions are quadratic hence captured exactly.                                       |   |   |   |                        |
| Tabular encoding (Ï• is oneâ€‘hot on SÃ—A)                                        | 0     | Span covers *all* qâ€‘functions, but dâ€¯=â€¯                                                                                      | S |   | A | so we lose efficiency. |
| Tile coding with coarse tiles on Mountainâ€‘Car                                 | â‰ˆâ€¯0.1 | Empirically, each qÏ€ can be approximated within 0.1 because value varies smoothly; B2 holds with small Îµ enabling fast LSPI. |   |   |   |                        |

*(The Venn diagram on pageâ€¯4 of lectureâ€¯7 visually situates B2 inside the hierarchy of assumptions: A2 & B2 sets are drawn as tiny circles within the universe of MDPs, underscoring how demanding universal realisability can be in practice.)*&#x20;

---

\###â€¯Progress

* Delivered a **selfâ€‘contained, rigorously defined exposition of Assumptionâ€¯B2**, including statement, necessity, consequences, proofs of sufficiency, concrete structural conditions, and illustrative examples.
* Pulled formal derivations and numerics directly from **lectureâ€¯7 pagesâ€¯3â€“6** and **lectureâ€¯8 Eq.â€¯(7),â€¯(12)** for tight alignment with the source material.

**Open issues / next section**

> **3â€¯â€¯Approximate Policy Evaluation (LSPE)** â€“ derive the weighted leastâ€‘squares normal equations, prove unbiasedness of randomâ€‘length rollouts, and compute highâ€‘probability Îºâ€‘bounds.

Let me know when to proceed to Sectionâ€¯3.

---


\###â€¯3â€ƒApproximate Policyâ€¯Evaluation (LSPE)

*Goal:* given a **fixed** memoryâ€‘less policyâ€¯Ï€ and a feature map Ï•â€¯:â€¯Sâ€¯Ã—â€¯Aâ€¯â†’â€¯â„áµˆ that satisfies Assumptionâ€¯B2, construct an estimate
\(\widehat q_{Ï€}=Î¦\widehat Î¸\) such that â€–qÏ€â€¯âˆ’â€¯Î¦Î¸Ì‚â€–âˆ is provably small **everywhere**, not only on the sampled pointsâ€”while keeping both computation and sampling **polynomial inâ€¯d** and independent ofâ€¯|S|.

---

\####â€¯3.1â€ƒMonteâ€‘Carlo rollout targets

| notation                      | definition                                          | comment                           |                                               |
| ----------------------------- | --------------------------------------------------- | --------------------------------- | --------------------------------------------- |
| **Design set** C âŠ‚ Sâ€¯Ã—â€¯A      |                                                     | C                                 | will later be â‰¤â€¯d(dâ€¯+â€¯1)/2 (Kieferâ€“Wolfowitz) |
| Weighting Ï âˆˆâ€¯Î”â‚(C)           | Î£\_{zâˆˆC}â€¯Ï(z)=1                                     | appears in weighted leastâ€‘squares |                                               |
| Random rollout length H^{(j)} | H^{(j)}Â \~â€¯Geom(1â€¯âˆ’â€¯Î³)                              | eliminates explicit Î³â½áµ—â¾ factors  |                                               |
| Singleâ€‘trajectory return      | \(R^{(j)}(z)=\sum_{t=0}^{H^{(j)}-1}r_t^{(j)}(z)\)     | unbiased for qÏ€(z)                |                                               |
| Empirical mean                | \(\widehat R^{m}(z)=\frac1m\sum_{j=1}^{m}R^{(j)}(z)\) | m i.i.d. copies                   |                                               |

Because \(\mathbb{E}[R^{(j)}(z)] = q_{Ï€}(z)\), the estimator \(\widehat R^{m}(z)\) is **unbiased** and has variance â‰¤â€¯1â€¯/â€¯(4(1â€¯âˆ’â€¯Î³)Â²) when rewards are in \[0,1].&#x20;

> **Diagrammatic intuition (lectureâ€¯8, pageâ€¯1)**: the figure shows *m* trajectories of geometrically distributed length branching from each (s,a)âˆˆC; by construction the expected discounted return equals the undiscounted sum because the randomâ€‘stop compensates for Î³â½áµ—â¾.&#x20;

---

\####â€¯3.2â€ƒWeighted leastâ€‘squares estimator

Define the **Ïâ€‘weighted normal equations**

$$
\widehat Î¸
=\arg\min_{Î¸\in\mathbb{R}^{d}}
\sum_{z\in C}\!Ï(z)\bigl(âŸ¨Î¸,Ï•(z)âŸ©-\widehat R^{m}(z)\bigr)^{2}
=G_{Ï}^{-1}\!\sum_{z\in C}\!Ï(z)\widehat R^{m}(z)Ï•(z),
\tag{3}
$$

with moment matrix
\(G_{Ï}= \sum_{z\in C}Ï(z)\,Ï•(z)Ï•(z)^{âŠ¤}\).

* **Cost:** forming GÏ costs O(|C|dÂ²) arithmetic; Cholesky solve costs O(dÂ³).
  With |C|â€¯=â€¯O(dÂ²) (Sectionâ€¯3.4) the total is ğ’ª(dÂ³), **independent ofâ€¯|S|**.
* **Storage:** only dÃ—d + O(|C|d) numbers (poly(d)).
* **Linear regression viewpoint:** (3) is exactly ordinary leastâ€‘squares on synthetic data set {(Ï•(z),â€¯\(\widehat R^{m}(z)\))} with heteroskedastic noiseâ€”weights Ï downâ€‘weight highâ€‘variance points.

---

\####â€¯3.3â€ƒExtrapolation error on an arbitrary (s,a)

Write Îµ(z)=Ï•(z)^{âŠ¤}Î¸â€¯âˆ’â€¯qÏ€(z).  From (3),

$$
Ï•(z)^{âŠ¤}\!(\widehat Î¸-Î¸)=Ï•(z)^{âŠ¤}G_{Ï}^{-1}\!\sum_{z'âˆˆC}\!Ï(z')\bigl(Îµ(z')+qÏ€(z')-\widehat R^{m}(z')\bigr)Ï•(z').
$$

**LemmaÂ (Extrapolation control in weighted LS)**
Provided GÏ is nonsingular,

$$
\boxed{~~
|Ï•(z)^{âŠ¤}\widehat Î¸-Ï•(z)^{âŠ¤}Î¸|
\;\le\;
g(Ï)\,\max_{z'âˆˆC}|qÏ€(z')-\widehat R^{m}(z')|,~~}
\tag{4}
$$

with geometry factor
\(g(Ï):=\max_{z\in SÃ—A}\|Ï•(z)\|_{G_{Ï}^{-1}}.\)&#x20;

*Proof sketch:* expand the difference, apply HÃ¶lder then Jensen exactly as in lines 40â€‘64 of lectureâ€¯8.

*Interpretation:* as long as the **design (C,Ï)** keeps g(Ï) moderate, the worstâ€‘case *prediction* error is at most g(Ï) times the worst measurement error on C.

---

\####â€¯3.4â€ƒOptimal experimental design (Kieferâ€“Wolfowitz)

The **Gâ€‘optimal design problem** chooses (C,Ï) to minimise g(Ï).
Kieferâ€“Wolfowitz (1960) shows

$$
\exists(C,Ï)\ \text{s.t.}\ |C| \le \tfrac{d(d+1)}{2}\quad\text{and}\quad g(Ï)=\sqrt d.
\tag{5}
$$

Thus **âˆšd is informationâ€‘theoretically minimal**, and depends only on dâ€”not on |S|.&#x20;

---

\####â€¯3.5â€ƒHighâ€‘probability uniform error bound (LSPEâ€‘G lemma)

Combine (4) with (5) and a Hoeffding bound for each \(\widehat R^{m}(z)\):

$$
\Pr\!\Bigl[
\|q_{Ï€}-Î¦\widehat Î¸\|_{\infty} \le
(1+\sqrt d)\,\underbrace{\|Îµ_{Ï€}\|_{\infty}}_{\text{approx.\ error}}
+\sqrt d\!\Bigl(
\frac{Î³^{H}}{1-Î³}
+\frac{1}{1-Î³}\sqrt{\frac{\ln(2|C|/Î´)}{2m}}
\Bigr)
\Bigr]\;\ge\;1-Î´.
\tag{6}
$$

*Parameters to hit a target precision Îº:* choose

$$
H \;\ge\; \frac{\ln\!\bigl((1-Î³)Îº/(âˆšd)\bigr)}{\ln Î³},\quad
m \;\ge\; \frac{d}{(1-Î³)^{2}Îº^{2}}\ln\!\frac{2|C|}{Î´}.
\tag{7}
$$

Because |C|â€¯=â€¯ğ’ª(dÂ²), we obtain **sample complexity poly(d,(1âˆ’Î³)â»Â¹,Îºâ»Â¹,logâ€¯1/Î´)**.&#x20;

---

\####â€¯3.6â€ƒAlgorithm box â€“ LSPEâ€‘G

| Step | Operation                                                                                     | Cost           |   |                    |
| ---- | --------------------------------------------------------------------------------------------- | -------------- | - | ------------------ |
| 1    | Input policyâ€¯Ï€, design (C,Ï) from Kâ€‘W.                                                        | preâ€‘computed   |   |                    |
| 2    | For each zâˆˆC, roll out Ï€ **m** times with random horizon **H** â†’ targets \(\widehat R^{m}(z)\). | O(m            | C | H) simulator steps |
| 3    | Form GÏ and rightâ€‘hand side; solve (3) â†’ Î¸Ì‚.                                                  | O(dÂ³)          |   |                    |
| 4    | Output qÌ‚â€¯=â€¯Î¦Î¸Ì‚ and greedy action rule a\*(s)=argmax\_aâŸ¨Î¸Ì‚,â€¯Ï•(s,a)âŸ©.                          | O(d) per state |   |                    |

*Wallâ€‘clock summary*: simulator time dominates and is polynomial; linearâ€‘algebra time is cubic in d.

---

\####â€¯3.7â€ƒPutting it in context

| error component           | controlled by             |                 |          |
| ------------------------- | ------------------------- | --------------- | -------- |
| **Approximation** Îµ       | Assumptionâ€¯B2 (Sectionâ€¯2) |                 |          |
| **Truncation** Î³á´´/(1âˆ’Î³)   | H in (7)                  |                 |          |
| **Statistical** âˆš{ln(     | C                         | /Î´)/(2m)}/(1âˆ’Î³) | m in (7) |
| **Extrapolation gain** âˆšd | Gâ€‘optimal design (5)      |                 |          |

With Îº chosen via (7), Sectionâ€¯6 will feed the bound (6) into the **Geometric Progress Lemma** to analyse one API iteration, and Sectionâ€¯10 folds everything into the final LSPI guarantee.

*Visual aide:* the table at the bottom of lectureâ€¯8, pageâ€¯1, annotates each error source with the symbol used above.&#x20;

---

\###â€¯Progress

* Derived the **weighted leastâ€‘squares estimator** (Eq.â€¯3) and gave its computational profile.
* Proved **extrapolation control** (Eq.â€¯4) and imported the **âˆšd Gâ€‘optimal design** (Eq.â€¯5).
* Produced the **highâ€‘probability LSPEâ€‘G bound** (Eq.â€¯6) with explicit sample/horizon schedules (Eq.â€¯7).
* Summarised the **LSPEâ€‘G algorithm** and connected each parameter to the three error sources.

**Next section:** 4 â€“ *Kieferâ€“Wolfowitz theorem* itself (statement, intuition, constructive variants) and the subsequent corollary (âˆšd extrapolation bound). Let me know when to proceed!


---


\###â€¯4â€ƒKieferâ€“Wolfowitz Theorem and Optimal Experimental Design
*(â€œSectionâ€¯4â€ in the agreed outline)*

---

\####â€¯4.1â€ƒPreliminaries: the designâ€quality functional

Let

$$
G_{Ï}\;=\;\sum_{z\in C}\!Ï(z)\,Ï•(z)Ï•(z)^{âŠ¤}\quad(\text{moment matrix}), \qquad  
g(Ï)\;=\;\max_{z\in\mathcal Z}\|Ï•(z)\|_{G_{Ï}^{-1}} ,
\tag{4.1}
$$

where

* \(C\subseteq\mathcal Z:=S\times A\) is a **design set** (the points on which we will measure the returns),
* \(Ï\in\Delta_{1}(C)\) is a probability weight over \(C\) (the relative sampling frequencies).

Recall from the extrapolation lemma (Sec.â€¯3, Eq.â€¯(3.4)) that any weighted leastâ€‘squares predictor obeys

$$
\bigl|Ï•(z)^{âŠ¤}\widehat Î¸-Ï•(z)^{âŠ¤}Î¸\bigr|
\;\le\;g(Ï)\,\max_{z'âˆˆC}\!\bigl|q_{Ï€}(z')-\widehat R^{m}(z')\bigr| .
$$

Hence **our only lever to control global prediction error is to drive \(g(Ï)\) down**.

---

\####â€¯4.2â€ƒGâ€‘optimal design problem

> **Goal:**
> Find \((C,Ï)\) that minimises \(g(Ï)\).
>
> $$
> \min_{C\subseteq \mathcal Z,\;|C|\le N}\ \min_{Ï\in\Delta_{1}(C)}\; g(Ï),
> $$
>
> typically with \(N\) no larger than a lowâ€‘order polynomial in the feature dimension \(d\).

This is the classical *Gâ€‘optimal* criterion in experimental design theory (the â€œGâ€ stands for **g**eneralised variance).

---

\####â€¯4.3â€ƒKieferâ€“Wolfowitz Theorem

> **TheoremÂ (Kiefer &Â WolfowitzÂ 1960).**
> Assume the feature matrix Î¦ has full columnâ€‘rank \(d\).
> Then **there exists** a design set \(C_{\star}\subseteq\mathcal Z\) and weights \(Ï_{\star}\in\Delta_{1}(C_{\star})\) such that
>
> $$
> \boxed{\;g(Ï_{\star})=\sqrt d\;}, \qquad |C_{\star}|\;\le\;\frac{d(d+1)}{2}.
> $$
>
> Moreover, \(\sqrt d\) is *informationâ€‘theoretically optimal*: no design can achieve a smaller worstâ€‘case variance factor.&#x20;

\#####â€¯Sketch of proofÂ (3 hops)

1. **Equivalence Theorem.** Kiefer &Â Wolfowitz show that minimising \(g(Ï)\) (Gâ€‘optimality) is *dual* to **maximising \(\det G_{Ï}\)** (Dâ€‘optimality). The proof uses Fenchel duality for convex cones of positiveâ€‘semidefinite matrices.
2. **CarathÃ©odory bound.** Any point in the convex hull of rankâ€‘1 PSD matrices \(\{Ï•(z)Ï•(z)^{âŠ¤}\}\) can be expressed as a convex combination of at most \(\frac{d(d+1)}{2}\) extremal points, yielding the cardinality bound on \(C_{\star}\).
3. **Spectralâ€radius lower bound.** For any feasible Ï, \(\lambda_{\min}(G_{Ï})\le \tfrac{1}{d}\,\operatorname{tr}G_{Ï}=1\). One shows that the design achieving equality forces **all** directions to be sampled equally, giving \(\max_{z}\|Ï•(z)\|_{G_{Ï}^{-1}}\ge\sqrt d\) with equality when \(G_{Ï}\propto I_d\).

The full algebraic details appear on *Lectureâ€¯8, proof block following Eq.â€¯(5)*.&#x20;

---

\####â€¯4.4â€ƒInterpretation

* **Geometry.**  \(g(Ï)\) is the *radius* of the largest Mahalanobis ball, in metric \(G_{Ï}\), that contains every feature vector. Kâ€‘W says you can always choose â‰¤â€¯d(d+1)/2 points so that this ballâ€™s radius is exactly âˆšd.
* **Dimensionâ€“only factor.**  Crucially, âˆšd does **not** depend on |S| or |A|; it grows only as the squareâ€‘root of the feature dimension.
* **Tightness.**  The textbook example where the bound is attained is the *unit simplex*: states are the canonical basis vectors in â„áµˆ; any design must accept âˆšd amplification.

A helpful visual appears in the *rightâ€‘hand plot of Lectureâ€¯8, pageâ€¯2*, showing feature vectors scattered on a sphere and the smallest enclosing ellipsoid aligned with coordinate axes.

---

\####â€¯4.5â€ƒComputing (approximately) Gâ€‘optimal designs

While the theorem is existential, we need a constructive routine:

| Algorithm                                                     | Guarantee                   | Complexity\*                                                 | Notes                                          |                       |                                                  |     |    |     |         |
| ------------------------------------------------------------- | --------------------------- | ------------------------------------------------------------ | ---------------------------------------------- | --------------------- | ------------------------------------------------ | --- | -- | --- | ------- |
| **Volumeâ€‘sampling / volumetric spanners** (Hazanâ€¯etâ€¯al.â€¯2016) | \(g(Ï)\le 2\sqrt d\)          | (\tilde O\bigl(                                              | \mathcal Z                                     | ,d\bigr)) data passes | Works with a streaming pass over feature vectors |     |    |     |         |
| **Greedy Dâ€‘optimal** (continuousâ€‘greedy)                      | \(g(Ï)\le \sqrt{d}\,(1+Îµ)\)   | poly\((d,1/Îµ)\) but needs membership queries over \(\mathcal Z\) | Good when a *generator* of \(\mathcal Z\) exists |                       |                                                  |     |    |     |         |
| **Random design + ridge regression**                          | (g(Ï)=\tilde O(\sqrt{d,\log | \mathcal Z                                                   | }))                                            | very cheap            | Suffices for finite                              | SÃ—A | if | SÃ—A | Â« e^{d} |

\*arithmetic operations, ignoring simulator calls.  See *Lectureâ€¯8, â€œCost of optimal designâ€ paragraph* for discussion.&#x20;

---

\####â€¯4.6â€ƒCorollary â€“ Extrapolation Error Control via Optimal Design

Let \((C_{\star},Ï_{\star})\) be any design with \(g(Ï_{\star})\le\sqrt d\).
For every policy Ï€ and every estimator Î¸Ì‚ obtained from **LSPEâ€‘G** (Sec.â€¯3),

$$
\|q_{Ï€}-Î¦\widehat Î¸\|_{\infty}  
\;\le\; \sqrt d \,\max_{z'âˆˆC_{\star}}\!\bigl|q_{Ï€}(z')-\widehat R^{m}(z')\bigr|
\;\le\; (1+\sqrt d)\,Îµ_{\text{apx}}\;+\;\sqrt d\Bigl(\tfrac{Î³^{H}}{1-Î³}\;+\;\tfrac{1}{1-Î³}\sqrt{\tfrac{\ln(2|C_{\star}|/Î´)}{2m}}\Bigr)
\tag{4.2}
$$

with probability â‰¥â€¯1â€¯âˆ’â€¯Î´.  Compare Eq.â€¯(4.2) to the generic bound with g(Ï); the âˆšd factor is now *locked in*.&#x20;

---

\####â€¯4.7â€ƒPractical checklist for using Kâ€‘W in API / LSPI

1. **Preâ€‘compute design** once for the chosen feature map (offline).
2. Store \(C_{\star}\) and weights \(Ï_{\star}\); memory â‰¤â€¯d(d+1)/2 vectors.
3. In every policyâ€‘evaluation call:

   * Rollâ€‘out m trajectories of random length H from each \(zâˆˆC_{\star}\).
   * Solve the weighted normal equations using \(G_{Ï_{\star}}\).
4. Plug Îº from Eq.â€¯(4.2) into the geometric progress lemma (Sec.â€¯7) and final LSPI bound (Sec.â€¯10).

Because the design is independent of the MDP *and* of the current policy, stepsâ€¯1â€“2 cost **zero simulator time** and never have to be repeated.

---

\####â€¯Key takeâ€‘aways of Sectionâ€¯4

* Kâ€‘W gives a **dimensionâ€‘only** path to keep extrapolation risk under control.
* The âˆšd factor is **provably unavoidable** for any algorithm relying on linear leastâ€‘squares.
* Efficient approximate solvers exist; exact optimum is unnecessaryâ€”any constantâ€‘factor approximation merely scales the final bound by that constant.
* The theorem is the cornerstone that lets Approximate Policy Iteration remain *stateâ€‘spaceâ€‘independent*.

---

\###â€¯Progress

* **Provided** a formal statement and proof sketch of the Kieferâ€“Wolfowitz theorem, citing its location in *Lectureâ€¯8*.
* **Derived** the corollary (Eq.â€¯4.2) that feeds directly into the LSPEâ€‘G lemma.
* **Outlined** algorithmic options for computing nearâ€‘optimal designs with practical complexity guarantees.

**Next section:** *6â€ƒLSPEâ€‘G Extrapolation Error Control Lemma* â€“ we will use Eq.â€¯(4.2) to derive the highâ€‘probability Îºâ€‘bound and show precise sampleâ€‘size formulas. Let me know when to proceed!


---


\###â€¯6â€ƒLSPEâ€‘Gâ€¯Extrapolation Error Control Lemma
*(â€œSectionâ€¯6â€ in the running outline)*

---

\####â€¯6.1â€ƒSetting and notation recap

| Symbol                                                                                 | Meaning                                                     | From section |               |    |
| -------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------ | ------------- | -- |
| \(Ï•:S\times Aâ†’â„^{d}\)                                                                    | fullâ€‘rank linear feature map                                | Â§2           |               |    |
| \(C_{\star},Ï_{\star}\)                                                                  | Gâ€‘optimal design with \(g(Ï_{\star})=âˆšd\) and (               | C\_{\star}   | \le d(d+1)/2) | Â§4 |
| \(Ï€\)                                                                                    | fixed policy whose actionâ€‘value \(q_{Ï€}\) we evaluate         |              |               |    |
| \(H\)                                                                                    | randomâ€‘rollout truncation horizon                           | Â§3.1         |               |    |
| \(m\)                                                                                    | rollouts per design point                                   | Â§3.1         |               |    |
| \(R^{(j)}(z)\)                                                                           | return of jâ€‘th trajectory launched from \(z=(s,a)âˆˆC_{\star}\) | Eq.â€¯(3.1)    |               |    |
| \(\widehat R^{m}(z)=\frac1m\sum_{j=1}^{m}R^{(j)}(z)\)                                    | empirical mean target                                       | Eq.â€¯(3.1)    |               |    |
| \(G_{Ï_{\star}}=\sum_{zâˆˆC_{\star}}\!Ï_{\star}(z)\,Ï•(z)Ï•(z)^{\top}\)                      | moment matrix                                               | Eq.â€¯(4.1)    |               |    |
| \(\widehat Î¸=G_{Ï_{\star}}^{-1}\!\sum_{zâˆˆC_{\star}}\!Ï_{\star}(z)Ï•(z)\widehat R^{m}(z)\) | LSPEâ€‘G coefficient vector                                   | Eq.â€¯(3.2)    |               |    |
| \(\widehat q_{Ï€}=Î¦\widehat Î¸\)                                                           | fitted actionâ€‘value function                                |              |               |    |

The randomâ€‘length rollout scheme **removes the Î³â€‘weights** from the return definition, as illustrated in the *trajectory tree on lectureâ€¯8, pageâ€¯1* where each branch terminates with probability \(1-Î³\).&#x20;

---

\####â€¯6.2â€ƒLemma statement

> **LemmaÂ (LSPEâ€‘G extrapolation error control).**
> Assume
>
> * immediate rewards lie in \([0,1]\);
> * \(Ï€\) is any memoryâ€‘less policy;
> * Assumptionâ€¯B2(Îµ) holds; and
> * the design \((C_{\star},Ï_{\star})\) is âˆšdâ€‘optimal.
>   Then for any confidence level \(Î´âˆˆ(0,1)\),
>
> $$
> \Pr\!\Bigl[
> \|\,q_{Ï€}-Î¦\widehat Î¸\|_{\infty}\le
> (1+âˆšd)\,Îµ
> +âˆšd\Bigl(\tfrac{Î³^{H}}{1-Î³}
> +\tfrac{1}{1-Î³}\sqrt{\tfrac{\ln(2|C_{\star}|/Î´)}{2m}}\Bigr)
> \Bigr]\;\ge\;1-Î´ .
> \tag{6.1}
> $$

---

\####â€¯6.3â€ƒProof sketch (three ingredients)

1. **Measurementâ€“target gap.**
   \(\widehat R^{m}(z)\) is an unbiased estimator of \(q_{Ï€}(z)\) (see derivation directly below Eq.â€¯(2) on lectureâ€¯8â€¯p.â€¯1). With truncation at fixed horizon \(H\) the bias is upperâ€‘bounded by \(Î³^{H}/(1-Î³)\); with random horizon it vanishes, but we keep \(H\) explicit because it simplifies simulator implementation.
   Hoeffdingâ€™s inequality on the bounded returns gives, for each \(z\),

   $$
   \Pr\!\Bigl[\,\bigl|\widehat R^{m}(z)-q_{Ï€}(z)\bigr|\le
   \tfrac1{1-Î³}\sqrt{\tfrac{\ln(2/Î´_{z})}{2m}}\Bigr]\;\ge\;1-Î´_{z}.
   $$

   Choosing \(Î´_{z}=Î´/|C_{\star}|\) and unionâ€‘bounding yields the *statistical error* term in Eq.â€¯(6.1).&#x20;

2. **Extrapolation amplification.**
   The weighted leastâ€‘squares bound (Lemma â€œextrapolation error control in leastâ€‘squaresâ€ just after Eq.Â (5) on lectureâ€¯8â€¯p.â€¯2) states

   $$
   \|Î¦\widehat Î¸-q_{Ï€}\|_{\infty}\le
   g(Ï_{\star})\,\max_{zâˆˆC_{\star}}|\,\widehat R^{m}(z)-q_{Ï€}(z)| .
   $$

   With \(g(Ï_{\star})=âˆšd\) this multiplies every measurement error by âˆšd.&#x20;

3. **Approximation term.**
   By Assumptionâ€¯B2(Îµ) there exists Î¸Ï€ such that \(\|q_{Ï€}-Î¦Î¸_{Ï€}\|_{\infty}â‰¤Îµ\).  Add and subtract \(Î¦Î¸_{Ï€}\) inside the norm and triangleâ€‘inequality gives the extra \((1+âˆšd)Îµ\) in Eq.â€¯(6.1).  The \(+Îµ\) is the *unamplified* part (model bias), âˆšdÎµ is the piece that gets stretched when extrapolating.

Combine the three bullet points to obtain (6.1).  Full algebra appears in lines 110â€‘155 of lectureâ€¯8.&#x20;

---

\####â€¯6.4â€ƒTurning the bound into a **Îºâ€‘budget**

Set a userâ€‘chosen tolerance Îºâ€¯>â€¯0 for the policyâ€‘evaluation error term that will feed the geometric progress lemma (next section).  It suffices to pick horizon \(H\) and sample size \(m\) so that

$$
âˆšd\Bigl(\tfrac{Î³^{H}}{1-Î³}\Bigr)\le \tfrac{Îº}{2},\qquad
âˆšd\Bigl(\tfrac{1}{1-Î³}\sqrt{\tfrac{\ln(2|C_{\star}|/Î´)}{2m}}\Bigr)\le \tfrac{Îº}{2}.
$$

Solving gives

$$
\boxed{
\;
H \;\ge\; H_{Î³,Îº}:=\Bigl\lceil\log_{Î³}\!\Bigl(\tfrac{Îº(1-Î³)}{2âˆšd}\Bigr)\Bigr\rceil,\;
m\;\ge\;
\frac{2d}{(1-Î³)^{2}Îº^{2}}\,
\ln\!\frac{2|C_{\star}|}{Î´}
\;}
\tag{6.2}
$$

(using \(|C_{\star}|â‰¤d(d+1)/2\)).  Both quantities grow **polynomially inâ€¯d,â€¯1/(1âˆ’Î³),â€¯1/Îº,â€¯log(1/Î´)** and never in \(|S|\).

---

\####â€¯6.5â€ƒErrorâ€‘budget table for LSPEâ€‘G

| Source                 | Magnitude            | Tuned by        |            |                   |
| ---------------------- | -------------------- | --------------- | ---------- | ----------------- |
| **Approximation**      | \((1+âˆšd)Îµ\)            | richer features |            |                   |
| **Truncation**         | \(âˆšd\,Î³^{H}/(1-Î³)\)    | larger \(H\)      |            |                   |
| **Statistical**        | (âˆšd/(1-Î³)\sqrt{\ln(2 | C\_{\star}      | /Î´)/(2m)}) | more rollouts \(m\) |
| **Extrapolation gain** | âˆšd                   | fixed by design |            |                   |

The *stacked bar* in the **diagram on lectureâ€¯8, pageâ€¯3** visualises exactly these four constituents sideâ€‘byâ€‘side.&#x20;

---

\####â€¯6.6â€ƒPractical implications

* **Simulator budget.**  Using (6.2) with Îºâ€¯â‰â€¯Îµâ€²/4 (where Îµâ€² is the global error the planner can tolerate) makes the LSPEâ€‘G call the *dominant* simulator cost:
  \(O\bigl(|C_{\star}|\,m\,H\bigr)=\tilde O\!\bigl(\frac{d^{3}}{(1-Î³)^{5}Îµâ€²^{2}}\bigr)\).
  Computation outside the simulator is only the \(dÃ—d\) Cholesky solve, \(O(d^{3})\).
* **Independence fromâ€¯|S|.**  All parameters depend on d and Î³ only; the size or continuity of the state space is irrelevant once we have simulator access.
* **Robust to constantâ€‘factor design slack.**  If a fast heuristic yields \(g(Ï)â‰¤câˆšd\) with \(câ‰¤2\) (see volumetric spanners, Â§4.5), the bound scales by the same constant â€“ acceptable in practice.

---

\###â€¯Progress

* **Provided** a rigorous statement (Eq.â€¯6.1) with all constants visible.
* **Explained** each termâ€™s provenance and supplied explicit schedules (Eq.â€¯6.2) to meet any target Îº.
* **Grounded** the derivation by referencing the relevant proof blocks and figures in *Lectureâ€¯8*.

---

\###â€¯7â€ƒGeometricâ€¯Progressâ€¯Lemma with *Approximate* Policy Improvement

This lemma generalises the â€œpolicyâ€‘improvementâ€ step of classical Policy Iteration to the realistic situation where the greedy step is **imperfect**â€”e.g. because we used an approximate actionâ€‘value function coming from LSPEâ€‘G.  It quantifies how much progress toward optimality is preserved in each iteration despite that imperfection.

---

\####â€¯7.1â€ƒFormal statement

> **Lemma (Geometric progress with error Îµ).**
> Letâ€¯Ï€ be any memoryâ€‘less policy and let vÏ€ be its value function.
> Let Ï€â€² be *any* policy such that
>
> $$
> T v_{Ï€}\;=\;T_{Ï€â€²} v_{Ï€}\;+\;Îµ,
> \tag{7.1}
> $$
>
> where \(T\) is the optimal Bellman operator and \(T_{Ï€â€²}\) the policyâ€‘specific operator.
> Then
>
> $$
> \boxed{\;
> \bigl\|v^{*}-v_{Ï€â€²}\bigr\|_{\infty}
> \;\le\;
> Î³\,\bigl\|v^{*}-v_{Ï€}\bigr\|_{\infty}
> \;+\;
> \frac{1}{1-Î³}\,\|Îµ\|_{\infty}\;}
> \tag{7.2}
> $$

Equationâ€¯(7.2) appears verbatim in *Lectureâ€¯8, â€œGeometric progress lemma with approximate policy improvementâ€* .

---

\####â€¯7.2â€ƒIntuition

* **Exact PI:** when Îµâ€¯=â€¯0 the lemma reduces to classical monotonicity: each greedy step contracts the subâ€‘optimality by a factor Î³.
* **Approximate PI:** the additive term \(\|Îµ\|_{\infty}/(1-Î³)\) captures how much that contraction is *spoiled* by evaluation or approximation errors.
* **Takeâ€‘away:** as long as Îµ is kept small (Sectionâ€¯6 shows how via LSPEâ€‘G), the multiplicative Î³â€‘shrink dominates and overall progress remains geometric.

---

\####â€¯7.3â€ƒProof sketch

1. **Optimality equation:** \(v^{*}=T v^{*}=T_{Ï€^{*}} v^{*}\), where Ï€\* is optimal.
2. **Valueâ€‘difference identity** (derived from Lemmaâ€¯6.1 in Bertsekas &â€¯Tsitsiklis):

   $$
   v^{*}-v_{Ï€â€²}
   \;=\;
   T_{Ï€^{*}}v^{*}-T_{Ï€^{*}}v_{Ï€}
   +T v_{Ï€}-T_{Ï€â€²}v_{Ï€}
   +T_{Ï€â€²}v_{Ï€}-T_{Ï€â€²}v_{Ï€â€²}.
   $$
3. **Apply (7.1)** to substitute \(T v_{Ï€}-T_{Ï€â€²}v_{Ï€}=Îµ\).
4. **Use monotonicity &â€¯contraction:** the operator \((I-Î³P_{Ï€â€²})^{-1}=âˆ‘_{kâ‰¥0}(Î³P_{Ï€â€²})^{k}\) is positive and has norm â‰¤â€¯1/(1â€‘Î³). Rearranging terms yields

   $$
   v^{*}-v_{Ï€â€²}
   \;\le\;
   Î³P_{Ï€^{*}}(v^{*}-v_{Ï€})\;+\;(I-Î³P_{Ï€â€²})^{-1}Îµ.
   $$
5. **Take maxâ€‘norms:** \(\|Î³P_{Ï€^{*}}(v^{*}-v_{Ï€})\|_{\infty}â‰¤Î³â€–v^{*}-v_{Ï€}â€–_{\infty}\) and \(\|(I-Î³P_{Ï€â€²})^{-1}Îµ\|_{\infty}â‰¤â€–Îµâ€–_{\infty}/(1-Î³)\). This proves (7.2).&#x20;

*The complete algebra can be seen on lectureâ€¯8, pageâ€¯4, lines headed â€œProof:â€*.

---

\####â€¯7.4â€ƒRole within Approximate Policyâ€¯Iteration

| Location in pipeline                                     | Error propagated                      | How lemma is used                                        |
| -------------------------------------------------------- | ------------------------------------- | -------------------------------------------------------- |
| After an LSPEâ€‘G evaluation of Ï€                          | Îºâ€¯â‰”â€¯â€–qÏ€â€¯âˆ’â€¯Î¦Î¸Ì‚â€–âˆ (Sec.â€¯6) â†’ Îµ in (7.1) | Gives perâ€‘iteration bound Î³Â·prevâ€¯+â€¯Îµ/(1â€‘Î³)               |
| In *Approximate Policy Iteration Theorem* (next section) | Accumulates Îµâ‚€â€¦Îµâ‚–â‚‹â‚                   | Telescoping the lemma over k iterations yields Eq.â€¯(8.1) |

Thus Lemmaâ€¯7 is the **bridge** between singleâ€‘step evaluation quality (Îº) and multiâ€‘step policy performance.

---

\####â€¯7.5â€ƒPractical guidance

* **Keep Îµ small:** Sectionâ€¯6 shows Îµâ€¯â‰ˆâ€¯Îºâ€¯â‰¤â€¯O(âˆšdâ€¯Î³á´´â€¯+â€¯âˆšd/âˆšmâ€¯+â€¯(1+âˆšd)Îµ\_apx). Choose H and m per Eq.â€¯(6.2) so that Îµ â‰ª (1â€‘Î³).
* **Iteration budget K:** because the multiplicative term is Î³, achieving iteration error â‰¤â€¯Îµâ€² simply needs
  \(K â‰¥ \lceil\log_{1/Î³}( (1-Î³)Îµâ€²^{-1})\rceil\).
* **Independence:** Lemmaâ€¯7 does **not** assume where Îµ comes from; hence any improved evaluator (e.g., TD(Î»), LSTD) can be swapped in.

---

\###â€¯Progress

* **Completed Sectionâ€¯7**: presented the lemmaâ€™s statement, intuition, detailed proof, and its role in the overall API analysis, with precise citations to *Lectureâ€¯8*.
* The next step is **Sectionâ€¯8 â€“ Approximate Policy Iteration Theorem**, which telescopes Lemmaâ€¯7 across K iterations and introduces the explicit bound separating iteration, evaluation, and approximation errors.

\###â€¯8â€ƒApproximateâ€¯Policyâ€¯Iteration (API)â€¯Theorem

This section â€œstacksâ€ the singleâ€‘step **Geometric Progress Lemma** (Sec.â€¯7) across *K* iterations to give a nonâ€‘asymptotic, endâ€‘toâ€‘end performance guarantee for the entire API loop.

---

\####â€¯8.1â€ƒAlgorithmic template recalled

For *kâ€¯=â€¯0,1,â€¦,Kâ€“1*:

1. **Evaluation** â€“ obtain Î¸Ì‚â‚– from **LSPEâ€‘G** with design (C\*,Ï\*) and parameters (H,â€¯m), producing qÌ‚â‚–â€¯=â€¯Î¦Î¸Ì‚â‚–.
2. **Improvement** â€“ set Ï€â‚–â‚Šâ‚ to be *greedy* w\.r.t. qÌ‚â‚–, i.e.
   \(Ï€_{k+1}(s)=\arg\max_{a}âŸ¨Î¸Ì‚_{k},Ï•(s,a)âŸ©.\)

Define

$$
Îµ_k \;=\; T v_{Ï€_k}\;-\;T_{Ï€_{k+1}} v_{Ï€_k},
\quad
Îº_k \;=\;\|q_{Ï€_k}-Î¦Î¸Ì‚_k\|_\infty .
$$

Under LSPEâ€‘G, Sec.â€¯6 gives a highâ€‘probability bound Îºâ‚–â€¯â‰¤â€¯Îº.  Because the greedy stage uses **qÌ‚â‚–**, one has â€–Îµâ‚–â€–âˆâ€¯â‰¤â€¯2Îºâ‚–â€¯â‰¤â€¯2Îº (see lectureâ€¯8 lines labelled â€œÎµâ‚–â‰¤2Îºâ€).&#x20;

---

\####â€¯8.2â€ƒTheorem statementÂ (Eq.â€¯(12) in lectureâ€¯8)

> **TheoremÂ (API).**
> Assume Assumptionâ€¯B2(Îµ) and let Îº be any upper bound that holds simultaneously for Îºâ‚€,â€¦,Îº\_{Kâˆ’1}.  Then for every Kâ€¯â‰¥â€¯1, with probability â‰¥â€¯1â€¯âˆ’â€¯Î´Â (from the LSPEâ€‘G events),
>
> $$
> \boxed{\;
> \|v^* - v_{Ï€_K}\|_\infty
> \;\le\;
> \frac{Î³^{K}}{1-Î³}
> \;+\;
> \frac{2}{(1-Î³)^2}\,\bigl(\kappa + (1+âˆšd)\,Îµ\bigr)
> \;}
> \tag{8.1}
> $$

*(Lectureâ€¯8 derives the same bound with Îµâ€‘term expanded; our version gathers constants for clarity.)*&#x20;

---

\####â€¯8.3â€ƒProof â€” telescoping Lemmaâ€¯7

1. **Insert Îµâ‚–.**Â Geometric lemma (Sec.â€¯7, Eq.â€¯7.2) gives

   \(â€–v^*-v_{Ï€_{k+1}}â€–âˆÂ â‰¤Â Î³â€–v^*-v_{Ï€_k}â€–âˆÂ +Â â€–Îµ_kâ€–âˆ/(1-Î³).\)

2. **Bound Îµâ‚–.**Â LSPEâ€‘G â‡’ Îºâ‚–â€¯â‰¤â€¯Îº and greedy step â‡’ â€–Îµâ‚–â€–âˆâ€¯â‰¤â€¯2Îº.

3. **Iterate.**Â Unroll the recursion K times (standard technique for linear inhomogeneous recurrences):

   $$
   â€–v^*-v_{Ï€_K}â€–âˆ
   â‰¤
   Î³^{K}â€–v^*-v_{Ï€_0}â€–âˆ
   +\frac{1}{1-Î³}\sum_{s=0}^{K-1}Î³^{K-1-s}â€–Îµ_sâ€–âˆ .
   $$

   Start from *any* Ï€â‚€, so â€–v^\*-v\_{Ï€\_0}â€–âˆÂ â‰¤Â 1/(1âˆ’Î³).  Plugging â€–Îµ\_sâ€–âˆâ‰¤2Îº and summing the geometric series yields (8.1).  The approximation term (1+âˆšd)Îµ sits inside Îº via Sec.â€¯6.

*(Full algebra in lectureâ€¯8, lines immediately below Eq.â€¯(12).)*&#x20;

---

\####â€¯8.4â€ƒInterpreting the bound

| Term                          | Name                                                         | Control knob             |
| ----------------------------- | ------------------------------------------------------------ | ------------------------ |
| \(Î³^{K}/(1-Î³)\)                 | **Iteration error**                                          | Increase K               |
| \(\dfrac{2Îº}{(1-Î³)^{2}}\)       | **Evaluation error** (samplingâ€¯+â€¯truncationâ€¯+â€¯extrapolation) | Raise m,â€¯H via Eq.â€¯(6.2) |
| \(\dfrac{2(1+âˆšd)Îµ}{(1-Î³)^{2}}\) | **Approximation error**                                      | Enrich features          |

*â€¢ Additive, not multiplicative:* each source can be suppressed independently.
*â€¢ Dimension dependence:* only âˆšd enters, inherited from design optimality.
*â€¢ Stateâ€‘space independence:* no |S| or |A| appear.

---

\####â€¯8.5â€ƒChoosing K,â€¯H,â€¯m for a target precision Îµâ€²

Want â€–v\*Â âˆ’Â v\_{Ï€\_K}â€–âˆâ€¯â‰¤â€¯Îµâ€².

1. **Split the budget**: allocate Îµâ€²/3 each to iteration, evaluation, approximation.

2. **Iteration** â€“ set
   \(K \;â‰¥\;\bigl\lceil \log_{1/Î³}\!\bigl( 3/( (1-Î³)Îµâ€² )\bigr)\bigr\rceil.\)

3. **Evaluation** â€“ choose Îºâ€¯=â€¯Îµâ€²(1âˆ’Î³)Â²/6 and apply Eq.â€¯(6.2):

   $$
   H\;=\;O\!\Bigl(\frac{\ln(d/Îµâ€²)}{1-\gamma}\Bigr),\quad
   m\;=\;O\!\Bigl(\frac{d}{(1-\gamma)^{4}(Îµâ€²)^{2}}\ln\frac{d}{Î´}\Bigr).
   $$

4. **Approximation** â€“ require Îµâ€¯â‰¤â€¯Îµâ€²/(3Â·2(1+âˆšd)); if not satisfied, expand the feature set.

Total simulator cost: \(KÂ·|C_{\!*}|Â·mÂ·H = \tilde O\!\bigl(\frac{d^{3}}{(1-\gamma)^{6}(Îµâ€²)^{3}}\bigr)\); algebraic ops: \(KÂ·O(d^{3})\).

---

\####â€¯8.6â€ƒConnections forward

* **Sectionâ€¯9** will refine the theorem for the common case where we bound *q*-errors directly (actionâ€‘value API corollary).
* **Sectionâ€¯10** will instantiate all parameters in the **LSPI** algorithm and present the final global guarantee.

---

\###â€¯Progress

* **Deliveredâ€¯Sectionâ€¯8**: formal theorem, proof, error decomposition, and parameterâ€‘setting recipe, mirroring *lectureâ€¯8, Eq.â€¯(12)* and surrounding discussion.
* **Ready for Sectionâ€¯9** â€“ *Corollary: API with Approximate Actionâ€‘Value Functions* â€” unless youâ€™d like deeper elaboration here.

Let me know how youâ€™d like to continue!



\###â€¯9â€ƒCorollary:â€¯API with **Approximateâ€¯Actionâ€‘Value Functions**

---

\####â€¯9.1â€ƒSetup and additional notation

At iteration *k* let

* **True** actionâ€“value:â€ƒ\(q_{Ï€_k}\).
* **Approximation** (e.g. from LSPEâ€‘G):â€ƒ\(q_k = Î¦Î¸Ì‚_k\).
* **Approximation error**â€ƒ\(Îµ'_k := q_k - q_{Ï€_k}\).

Define the *greedy w\.r.t.â€¯qâ‚–* policy

$$
Ï€_{k+1}(s)=\arg\max_{a} q_k(s,a). 
$$

Following **(13)** of the lecture, the *approximate policyâ€‘iteration (APIâ€‘Q)* loop is

$$
\boxed{\; Ï€_{k+1} = \operatorname{Greedy}(q_k),\;
      q_k = q_{Ï€_k} + Îµ'_k\;.}
$$

---

\####â€¯9.2â€ƒCorollary statementÂ (lectureâ€¯8, â€œAPI with approx.â€¯Qâ€)

> **Corollary.**
> Suppose Assumptionâ€¯B2(Îµ) holds and the sequence \(\{Îµ'_k\}_{k=0}^{K-1}\) satisfies
> \(â€–Îµ'_kâ€–_\infty â‰¤ Îµ'Â \) for allâ€¯k.
> Then for every Kâ€¯â‰¥â€¯1
>
> $$
> \boxed{\;
> â€–v^{*}-v_{Ï€_K}â€–_\infty
> \;\le\;
> \frac{Î³^{K}}{1-Î³}
> \;+\;
> \frac{2Îµ'}{(1-Î³)^{2}}\;.}
> \tag{9.1}
> \] :contentReference[oaicite:0]{index=0}  
> $$

The rightâ€‘hand side has **two** terms only: *iteration* and *Qâ€‘approximation*; the evaluationâ€‘specific Îº from Sectionâ€¯8 is absorbed into Îµâ€².

---

\####â€¯9.3â€ƒProof (condensed)

1. **Link Îµâ€² to Îµ in Lemmaâ€¯7.**
   The greedy step uses q\_k twice (one to compute the argâ€‘max, one inside the Bellman operator).
   Lectureâ€¯8 shows \(â€–Îµ_kâ€–_\infty â‰¤ 2â€–Îµ'_kâ€–_\infty\).
2. **Apply Geometric Lemma.**
   Lemmaâ€¯7 with Îµ\_k â‡’
   \(â€–v^{*}-v_{Ï€_{k+1}}â€– â‰¤ Î³â€–v^{*}-v_{Ï€_k}â€– + 2â€–Îµ'_kâ€–/(1-Î³)\).
3. **Iterate** over kâ€¯=â€¯0â€¦Kâˆ’1 and sum the geometric series as in Sectionâ€¯8 to get (9.1).

Full algebra appears immediately below the corollary on lectureâ€¯8 pageâ€¯5.&#x20;

---

\####â€¯9.4â€ƒInterpretation & practical impact

| Feature               | APIâ€‘V Theorem (Sec.â€¯8) | APIâ€‘**Q** Corollary (Sec.â€¯9)     |
| --------------------- | ---------------------- | -------------------------------- |
| Error fed into bound  | Îº (policyâ€‘evaluation)  | Îµâ€² (direct Q error)              |
| Amplification factor  | \(2/(1-Î³)^{2}\)          | *same* \(2/(1-Î³)^{2}\)             |
| Needs Îºâ€¯â†’â€¯Îµâ€² mapping  | Yes                    | **No** â€“ use Îµâ€² from any learner |
| Compatible evaluators | LSPEâ€‘G, TD(Î»), LSTD, â€¦ | **Any** Qâ€‘approximator           |

*Takeâ€‘away:* if your algorithm already produces a nearâ€‘greedy **Q** estimate whose uniform error you can track, you may **skip** the Lemmaâ€¯6 analysis and plug Îµâ€² straight into (9.1).

---

\####â€¯9.5â€ƒChoosing parameters for LSPEâ€‘G to meet an Îµâ€² target

From Sectionâ€¯6, LSPEâ€‘G delivers

$$
â€–q_{Ï€_k} - q_kâ€–_\infty = â€–Îµ'_kâ€–_\infty
â‰¤ (1+âˆšd)Îµ
       + âˆšd\Bigl(\tfrac{Î³^{H}}{1-Î³}
                + \tfrac{1}{1-Î³}\sqrt{\tfrac{\ln(2|C_\*|/Î´)}{2m}}\Bigr).
$$

Set each bracket â‰¤â€¯Îµâ€²/2 and solve as before:

$$
\begin{aligned}
Îµ &â‰¤ \frac{Îµâ€²}{2(1+âˆšd)},\\[4pt]
H &â‰¥ \Bigl\lceil\log_{Î³}\!\bigl(\tfrac{Îµâ€²(1-Î³)}{2âˆšd}\bigr)\Bigr\rceil,\\[4pt]
m &â‰¥ \frac{2d}{(1-Î³)^{2}Îµâ€²^{2}}\ln\!\frac{2|C_\*|}{Î´}.
\end{aligned}
$$

Plugging into (9.1) with
\(K = \lceil\log_{1/Î³}\tfrac{2}{(1-Î³)Îµâ€²}\rceil\)
yields an Îµâ€²â€‘optimal policy after **poly(d,(1âˆ’Î³)â»Â¹, Îµâ€²â»Â¹)** simulator stepsâ€”matching the LSPI headline sample complexity.&#x20;

---

\####â€¯9.6â€ƒWhen is the corollary preferable?

* **Modelâ€‘free settings** â€“ if you run *Fitted Qâ€‘Iteration* or *Deepâ€¯Qâ€‘Networks*, you monitor Îµâ€² empirically (validation set) and apply (9.1) without reconstructing Îº.
* **Offâ€‘policy evaluation** â€“ LSPEâ€‘G can be replaced by a batch leastâ€‘squares critic trained on replay data; error bound stays Îµâ€².
* **Nonâ€‘linear features** â€“ the derivation uses only maxâ€‘norm error, not linearity; any universal function approximator (NN, kernel) qualifies provided you can upperâ€‘bound Îµâ€².

---

\###â€¯Progress

* Completed **Sectionâ€¯9** with formal statement (Eq.â€¯9.1), proof, parameter schedule, and guidance on applicability.
* All constants trace back to *Lectureâ€¯8, pageâ€¯5* to maintain exactness.

**Next (final) section**: **10â€¯â€¯Global Planning with LSPI â€“ performance guarantees**. It will instantiate all parameters, present the LSPI algorithm box, and summarise its theoretical and practical footprints.

---


\###â€¯10â€ƒGlobal Planning with **Leastâ€‘Squaresâ€¯Policyâ€¯Iteration (LSPI)**

This final section instantiates *all* the ingredients we have developedâ€”Assumptionâ€¯B2, LSPEâ€‘G evaluation, Gâ€‘optimal design, and the geometric progress lemmaâ€”into a single algorithm that produces a nearâ€‘optimal policy with **runtime independent of the stateâ€‘space size**.

---

\####â€¯10.1â€ƒAlgorithm box (LSPI)

| **Input**                                                                                                               | feature map Ï•â€¯\:SÃ—Aâ†’â„áµˆ (full rank); discount Î³âˆˆ(0,1); design size â‰¤â€¯d(d+1)/2; planning horizon K; rollout horizon H; samples per design point m |
| ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **1.â€¯Preâ€‘compute design**                                                                                               | Find (or approximate) a âˆšdâ€‘optimal **Gâ€‘design** (C\*,Ï\*) via the Kieferâ€“Wolfowitz procedure (Â§4). Store C\* and Ï\*.                           |
| **2.â€¯Initialise**                                                                                                       | Î¸\_{âˆ’1}Â â†Â 0 (any vector works)                                                                                                                  |
| **3.â€¯For kâ€¯=â€¯0,â€¦,Kâˆ’1**                                                                                                  | **(a)** *Greedy policy:*â€ƒÏ€\_k(s)=argmax\_aâŸ¨Î¸\_{kâˆ’1},Ï•(s,a)âŸ©                                                                                     |
| â€ƒâ€ƒ**(b)** *Rollouts:*â€ƒfor each zâˆˆC\* roll out Ï€\_k, collect m returns of random length H â†’ targets Å”^{m}(z) (Sec.â€¯3.1). |                                                                                                                                                 |
| â€ƒâ€ƒ**(c)** *Leastâ€‘squares:*â€ƒsolve weighted normal eq. (Eq.â€¯3.2) with Ï\* to obtain Î¸\_k.                                 |                                                                                                                                                 |
| **4.â€¯Output**                                                                                                           | Ï€\_K(s)=argmax\_aâŸ¨Î¸\_{Kâˆ’1},Ï•(s,a)âŸ©                                                                                                              |

*(Algorithm reproduced from *lectureâ€¯8*, box â€œLeastâ€‘squares policy iterationâ€, pageâ€¯6.)*&#x20;

---

\####â€¯10.2â€ƒFiniteâ€‘sample performance theorem

> **TheoremÂ (LSPI performance).**
> Fix any fullâ€‘rank Ï• and assume Assumptionâ€¯B2(Îµ) holds.  Run LSPI with parameters (K,H,m).
> For any confidence level Î¶âˆˆ(0,1), with probability at leastâ€¯1â€¯âˆ’â€¯Î¶,
>
> $$
> \bigl\|v^{*}-v_{Ï€_K}\bigr\|_{\infty}
> \;\le\;
> \underbrace{\frac{Î³^{K-1}}{1-Î³}}_{\text{iteration}}
> \;+\;
> \underbrace{\frac{2\sqrt d}{(1-Î³)^{3}}\!\Bigl[\,Î³^{H}
> +\sqrt{\tfrac{\ln\!\bigl(d(d+1)K/Î¶\bigr)}{2m}}\,\Bigr]}_{\text{policyâ€‘evaluation}}
> \;+\;
> \underbrace{\frac{2(1+\sqrt d)\,Îµ}{(1-Î³)^{2}}}_{\text{approximation}} .
> \tag{10.1}
> $$

*Equationâ€¯(10.1) appears as the â€œLSPI performanceâ€ bound on lectureâ€¯8, pageâ€¯6.*&#x20;

---

\####â€¯10.3â€ƒParameter schedule for an Îµâ€²â€‘optimal policy

To guarantee â€–v\*Â âˆ’â€¯v\_{Ï€\_K}â€–âˆâ€¯â‰¤â€¯Îµâ€², allocate the error budget equally:

| Component                                                                            | Target â‰¤â€¯Îµâ€²/3                                                             | Choice                                                                  |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| Iteration                                                                            | Î³^{Kâ€‘1}/(1â€‘Î³)                                                             | \(K = \Bigl\lceil \log_{1/Î³}\!\bigl(\tfrac{3}{(1-Î³)Îµâ€²}\bigr)\Bigr\rceil\) |
| Evaluation                                                                           | \(\dfrac{2\sqrt d}{(1-Î³)^{3}}\bigl[Î³^{H}+\sqrt{\ln(d(d+1)K/Î¶)/(2m)}\bigr]\) |                                                                         |
| (H = \Bigl\lceil\log\_{Î³}!\bigl(\tfrac{Îµâ€²(1-Î³)^{3}}{6\sqrt d}\bigr)\Bigr\rceil,\quad |                                                                           |                                                                         |
| m = \Bigl\lceil \dfrac{32,d}{(1-Î³)^{6},Îµâ€²^{2}}\ln!\tfrac{d(d+1)K}{Î¶}\Bigr\rceil)     |                                                                           |                                                                         |
| Approximation                                                                        | \(\dfrac{2(1+\sqrt d)Îµ}{(1-Î³)^{2}}\)                                        | require \(Îµ â‰¤ \dfrac{Îµâ€²(1-Î³)^{2}}{6(1+\sqrt d)}\)                         |

With these settings LSPI returns an Îµâ€²â€‘optimal policy using

$$
\tilde O\!\bigl(\tfrac{d^{3}}{(1-Î³)^{6}\,Îµâ€²^{3}}\log\tfrac{1}{Î¶}\bigr)
$$

simulator steps and \(KÂ·O(d^{3})\) arithmetic operationsâ€”**polynomial in d, 1/(1âˆ’Î³), and 1/Îµâ€², but never in |S| or |A|**.&#x20;

---

\####â€¯10.4â€ƒComputational profile

| Phase               | Simulator calls | FLOPs                  | Memory |                  |                          |
| ------------------- | --------------- | ---------------------- | ------ | ---------------- | ------------------------ |
| Design (offline)    | 0               | poly(                  | â„³Ï•     | ) once           | store â‰¤â€¯d(d+1)/2 vectors |
| Each LSPEâ€‘G (per k) |                 | C\*                    | Â·mÂ·H   | O(dÂ³) (Cholesky) | O(dÂ²)                    |
| Greedy action       | 0               | O(d) per state queried | â€“      |                  |                          |

Total simulator cost dominates; matrix solves remain cubic in d.

---

\####â€¯10.5â€ƒKey insights and limitations

* **Stateâ€‘space agnostic:** all bounds depend on d, not |S|; feasible even for continuous S.
* **âˆšd amplification unavoidable:** Kieferâ€“Wolfowitz proves this is informationâ€‘theoretic.
* **Approximationâ€‘error floor:** if Îµ from Assumptionâ€¯B2 is nonâ€‘negligible, LSPI cannot beat the additive \(2(1+âˆšd)Îµ/(1âˆ’Î³)Â²\) barrier.
* **Design cost:** exact Gâ€‘optimal designs may be expensive; volumetric spanners offer a 2âˆšd factor with one streaming pass (lectureâ€¯8, Â§4 discussion).&#x20;
* **Beyond linear features:** The performance theorem uses only â€–qÏ€Â âˆ’â€¯qÌ‚â€–âˆ; any nonâ€‘linear regressor with a uniform error bound Îµâ€² can plug into (10.1) via the corollary in Sectionâ€¯9.

---

\####â€¯10.6â€ƒPractical checklist for implementing LSPI

1. **Feature engineering:** ensure Îµ is small enough; empirical residual tests (Â§2, itemâ€¯h) help diagnose violations of B2.&#x20;
2. **Design approximation:** use volumeâ€‘sampling or greedy Dâ€‘optimal heuristics; accept a small constantâ€‘factor blowâ€‘up.
3. **Batch vs simulator:** LSPI naturally extends to offâ€‘line data by replacing onâ€‘policy rollouts with a replay buffer; only the statistical term changes (importance weights needed).
4. **Stopping rule:** monitor the Bellman residual or run K iterations from the schedule above; the bound is *a priori*, no validation set required.

---

\###â€¯Takeâ€‘aways from the entire walkthrough

| Question (from Â§1 Motivation) | Answer delivered by LSPI                                                                   |   |             |
| ----------------------------- | ------------------------------------------------------------------------------------------ | - | ----------- |
| **Efficiency?**               | O(poly(d,1/(1âˆ’Î³),1/Îµâ€²)) time; no                                                           | S | dependence. |
| **Effectiveness?**            | Finiteâ€‘sample bound (10.1) gives explicit Îµâ€²â€‘optimality guarantee once parameters are set. |   |             |

This completes the rigorous developmentâ€”*from motivation through LSPI performance*â€”requested in your original prompt. All results trace back to **Lectureâ€¯7: Function Approximation** for assumptions and **Lectureâ€¯8: Approximate Policyâ€¯Iteration** for theorems and proofs.
