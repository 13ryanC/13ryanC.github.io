---
date: "2025-07-11"
title: "(4.2) From Simulator Access to Batch RL: the H³ Frontier"
summary: "From Simulator Access to Batch RL: the H³ Frontier"
lastmod: "2025-07-11"
category: "Notes"
series: ["RL Theory"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
---

lec17, 18

> **Previously** we showed that **online planning with a generative simulator** eliminates the curse of $\lvert S\rvert$ yet still pays an exponential $\Theta(A^{H})$ price in the horizon $H=1/(1-\gamma)$. :contentReference[oaicite:0]{index=0}  
> **Today** we flip the information pattern: *fixed batch data, no simulator*.  Under the right experimental **Z‑design** the horizon tax collapses to **cubic**.  Lose control over the design and the exponential curse resurfaces.

---

## 1 Why Batch Reinforcement Learning?

Batch RL formalises situations where exploration is impossible (medical logs, industrial control, historic gameplay):

* **Input.** A dataset  
  $\tau^{(i)}=(S_0^{(i)},A_0^{(i)},R_0^{(i)},\dots,S_{T_i}^{(i)})$, $i=1{:}n$, produced by some unknown behaviour mechanism. :contentReference[oaicite:1]{index=1}  
* **Goal.** Either  
  1. *Value prediction* for a fixed policy $\pi$; or  
  2. *Policy optimisation*: output $\hat\pi$ s.t. $V^{\hat\pi}_\mu\geV^\star_\mu-\delta$.

### Two canonical data designs

| Design | How data are sampled | Minimax sample complexity* |
|--------|---------------------|-----------------------------|
| **Z‑design** | Choose $n(s,a)$ i.i.d. transitions for every $(s,a)$ | $\Theta\bigl(SAH^{3}/\delta^{2}\bigr)$ |
| **Policy‑induced** | $n$ full trajectories from a fixed logging policy $\pi_{\text{log}}$ | $\Theta\bigl(A^{H}/\delta^{2}\bigr)$ |

\*Tabular discounted MDPs; constants suppressed. :contentReference[oaicite:2]{index=2}

---

## 2 Plug‑in (Certainty‑Equivalence) Still Works

1 **Estimate a model.**  
  $\displaystyle\hat r_a(s)=\tfrac1{n(s,a)}\sum_{i=1}^{n(s,a)}R_i,
    \hat P_a(s,\cdot)=\frac1{n(s,a)}\sum_{i=1}^{n(s,a)}\delta_{S_i'}$.

2 **Solve** the empirical MDP $\hat M=(S,A,\hat P,\hat r,\gamma)$ and return an optimal policy $\hat\pi$.

The residual bound for any $\gamma$‑contraction $F$ with fixed point $y$,
$$\|x-y\|\le\frac{\|F x-x\|}{1-\gamma},$$
implies that controlling the Bellman **residual** under $\hat M$ controls policy error in the real $M$. :contentReference[oaicite:3]{index=3}

### Sensitivity

$$
\|v^\pi-\hat v^\pi\|_\infty
\leH\Bigl(\|r-\hat r\|_\infty+\gamma H\|P-\hat P\|_\infty\Bigr).
$$
Reward noise scales with $H$; transition noise with $H^{2}$.

---

## 3 Tight Horizon Dependence: from $H^{6}$ to $H^{3}$

### 3.1 Upper bound (Agarwal–Kakade–Yang 2020)

* **Bernstein > Hoeffding.** Variance‑aware bounds shave an $H^{3}$ factor.  
* **Total discounted variance lemma.**  
  $\displaystyle\bigl\|(I-\gamma P_\pi)^{-1}M_\pi\sigma_\pi\bigr\|_\infty
  \le\frac{\sqrt2}{(1-\gamma)^{3/2}}=O(H^{3/2}).$ :contentReference[oaicite:4]{index=4}

With
$$
n(s,a)\gec\frac{H^{3}}{\delta^{2}}\log\frac{SAH}{\delta},
$$
plug‑in returns a $\delta$‑optimal $\hat\pi$ w.p. $\ge1-\zeta$.

### 3.2 Lower bound

Two‑state “sticky” MDP: state 1 self‑loops w.p. $p$, else jumps to state 2.  Reward $\pm2\delta$ only on the jump.  Estimating $p$ within $O(\delta/H)$ needs $\Omega(H^{3}/\delta^{2})$ samples. :contentReference[oaicite:5]{index=5}

Hence **$\Theta(SAH^{3}/\delta^{2})$ is minimax‑optimal** under Z‑designs.

---

## 4 Policy‑Induced Data Brings Back Exponential Pain

Logging policy $\pi_{\mathrm{log}}$ picks each action with prob $\ge 1/A$.  The rare path probability $(1/A)^{H}$ yields
$$n\ge\Omega\bigl(A^{H}/\delta^{2}\bigr)$$
for any scheme that always delivers a $\delta$‑optimal policy. :contentReference[oaicite:6]{index=6}

---

## 5 Connecting Back to Online Planning

| Setting | State‑space factor | Horizon factor | Tight? |
|---------|-------------------|----------------|--------|
| Simulator planning | none | $A^{H}$ | ✔ |
| Batch RL, Z‑design | none | $H^{3}$ | ✔ |
| Batch RL, $\pi_{\text{log}}$‑design | none | $A^{H}$ | ✔ |

Hardness migrates with the information pattern; it never disappears.

---

## 6 Practitioner’s Checklist

* **Have a simulator?** Use look‑ahead / MCTS; expect $A^{H}$ queries worst‑case.  
* **Offline logs only?**  
  * Control data → aim for uniform Z‑design with $\gtrsim SAH^{3}/\delta^{2}$ samples.  
  * No control → audit coverage; exponential blow‑ups likely.  
* **Plug‑in is enough.** In tabular domains it meets the minimax rate.

---

## 7 Open Problems

1. Does the $H^{3}$ term survive under function approximation? Partial results exist for linear $Q$‑features. :contentReference[oaicite:7]{index=7}  
2. Adaptive designs between Z‑designs and behaviour data.  
3. Simulators without random‑access resets: sample‑complexity characterisation?

---

### TL;DR

Certainty‑equivalence + uniform Z‑design $\Rightarrow$ **minimax‑optimal** batch RL with $O(SAH^{3}/\delta^{2})$ samples.  Lose design control and the exponential horizon curse returns.

---

**References**

* Agarwal, A.; Kakade, S.; Yang, L. F. (2020) *Model‑based RL with a generative model is minimax optimal*.  
* Xiao, C. et al. (2021) *Sample complexity of batch RL with policy‑induced data*.  
* Azar, M. G.; Munos, R.; Kappen, H. J. (2013) *Minimax PAC bounds with a generative model*.

