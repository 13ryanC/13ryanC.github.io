---
date: "2025-07-03"
title: "(Part 3) Online Planning and Theoretical Foundations"
summary: "Online Planning and Theoretical Foundations"
category: "Tutorial"
series: ["RL Theory"]
author: "Bryan Chan"
hero: /assets/images/hero3.png
image: /assets/images/card3.png
sources:
  - title: "Lecture 1 (2022-01-05)"
    url: "http://www.youtube.com/watch?v=rjwxqcVrVws"
  - title: "Lecture 1 (2021-01-12)"
    url: "http://www.youtube.com/watch?v=0oJmSULoj3I"
---


## 1â€¯â€¯Motivation for Online Planning

### 1.1â€ƒBellmanâ€™s Curseâ€¯ofâ€¯Dimensionality and the â€œPipeâ€‘Dreamâ€ Runtime

In a finite discounted Markov Decision Process (MDP)

$$
\mathcal M=(S,A,P,r,\gamma),\qquad |S|<\infty,\;|A|=A,\;0<\gamma<1,
$$

a classical dynamicâ€‘programming (DP) planner is expected to **read** almost every transitionâ€‘probability vector $P_a(s)$ and **write** an action for every state to output an $\varepsilon$-optimal *full policy*.
The tableâ€‘lookup representation therefore incurs a worstâ€‘case arithmetic cost of

$$
\Omega\!\bigl(|S|^2\,|A|\bigr),
$$

because the planner must (i) compute a value for each $(s,a)$ pair and (ii) enumerate an action for every $s$.â€‚This squareâ€‘inâ€‘$|S|$ dependence is one facet of **Bellmanâ€™s curse of dimensionality**.

> **Lowerâ€‘bound insight.**  When $\delta<\gamma/(1-\gamma)$ (highâ€‘confidence setting), any planner that emits a complete nearâ€‘optimal policy needs $\Omega(|S|^2A)$ operations; there is no way around enumerating the huge table.

Because realistic problemsâ€”robotic manipulation, molecular design, dialogue systemsâ€”often have *astronomical* state spaces, a runtime that scales linearly (let alone quadratically) in $|S|$ is hopeless.  The dream is to achieve **perâ€‘decision runtimes independent of $|S|$**.

### 1.2â€ƒWhy a Full Policy Is Overkill in an Embedded Agent

Consider how an embodied agent actually operates:

1. At time $t$ it observes a *single* environment state $s_t$.
2. It must choose one action $a_t$.
3. The environment transitions to $s_{t+1}$ and the process repeats.

Hence the agent never needs to know what it *would* do in states it will never visit; it merely needs the next action in *the current state*.  Requiring the planner to output an entire mapping $S\to A$ is therefore unnecessary for online control.  This observation removes the first $|S|$ factor from the cost profile.

### 1.3â€ƒReplacing Table Access with a Simulator Oracle

Most modern RL systems come with or learn a **simulator**â€”a blackâ€‘box function

$$
\texttt{Sim}:(s,a)\;\longmapsto\;(r,s'),\qquad r\sim r_a(s),\;s'\sim P_a(s).
$$

A planner can query `Sim` in arbitrary states without reading explicit probability tables.  When the environment is:

* **Deterministic**, one sample exactly matches the expectation.
* **Stochastic**, Monteâ€‘Carlo samples provide unbiased estimates whose error does **not** depend on $|S|$.

Consequently, if we restrict planning to online calls of `Sim`, the second $|S|$ factor (reading big probability vectors) disappears.

> **Key intuition.**  In a deterministic MDP the planner can compute an $\varepsilon$-optimal action in $O(A^{H})$ time, where
> $H = H_{\gamma,\varepsilon}\approx\log_{1/\gamma}(1/\varepsilon)$, *irrespective of $|S|$*.  This was proved explicitly in Lectureâ€¯5.

### 1.4â€ƒFrom Global to Local to Online Access

| Access Mode | What the planner can query                                | Stateâ€‘space cost                                   |   |       |
| ----------- | --------------------------------------------------------- | -------------------------------------------------- | - | ----- |
| **Global**  | Any $(s,a)$ at any time                                   | Reads entire tables â‡’ ( \Omega(                    | S | ^2A)) |
| **Local**   | States it has visited so far                              | Cost proportional to size of explored subtree      |   |       |
| **Online**  | Only the *current* state (plus optional simulator resets) | $O(A^{H})$ deterministic; $O((mA)^{H})$ stochastic |   |       |

Local and online modes capture the information flow in real agents while preserving feasibility even when $|S|$ is infinite.

### 1.5â€ƒFormal Problem Statement (Preview)

*Input to planner per call*
$(A,s_t,\texttt{Sim},\varepsilon,\delta,Q)$

*Goal*
Return action $a_t$ such that the induced perâ€‘step policy is $\varepsilon$-optimal with probability $1-\delta$.

*Performance metric*
Query complexity $m\le Q$ and arithmetic complexity per callâ€”**both ideally independent of $|S|$**.

These formalities will be detailed in Sectionâ€¯4; they are sketched here only to clarify the objective of *online planning*.

### 1.6â€ƒTakeâ€‘Away

Online planning reframes DPâ€™s â€œglobalâ€‘policyâ€ requirement into a *currentâ€‘state* action decision, leveraging simulator queries to evade Bellmanâ€™s curse.  This shift opens the door to algorithms whose runtime depends exponentially on effective horizon $H$ but **not** on the cardinality of the state spaceâ€”a crucial breakthrough for largeâ€‘scale reinforcementâ€‘learning problems.

---

### Progressâ€¯ğŸ”„

* Reâ€‘established the computational lower bound in tableâ€‘representation MDPs and articulated why producing a full policy is unnecessary online.
* Introduced simulator access modes and previewed the formal onlineâ€‘planning definition.
* All claims have been grounded in Lectureâ€¯5 (pagesâ€¯1â€“4).
* **Next section to expand:** â€œ2â€¯â€¯From explicit global to implicit local planningâ€.

---

## 2â€¯â€¯From Explicit Global to Implicit Localâ€¯/â€¯Online Planning

### 2.1â€ƒThree Simulatorâ€‘Access Modes

| Mode              | Plannerâ€™s legal queries                                                                                                                                                                                                                                                             | World assumptions                                                                                                        | Pros                                                                                                 | Cons                                                                                        |   |                        |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | - | ---------------------- |
| **Global access** | Any stateâ€“action pair $(s,a)\in S\times A$ at any time.                                                                                                                                                                                                                             | Planner sees identifiers for *all* states; simulator can â€œteleportâ€ to arbitrary states and roll stochastic nextâ€‘states. | Close to the classical tabular modelâ€”easy to analyse.                                                | Requires enumerating or sampling from the *entire* state set; impossible when (             | S | ) is huge or infinite. |
| **Local access**  | Only to states that have already been generated in the current planning call (including the root state). Allows manual *reset* to any of those past states.                                                                                                                         | Simulator supports **checkpoint/restore**; planner maintains a growing search tree anchored at the current root.         | Removes need for full state enumeration; still lets the planner branch offline from earlier samples. | Extra bookkeeping; still unrealistic inside a robotic control loop if resets are not cheap. |   |                        |
| **Online access** | At time $t$ the simulatorâ€™s *internal* state is the *real* environment state $s_t$; planner may: 1) query a **step** $a\mapsto(r,s')$ (which advances the internal state), or 2) **reset** that internal state **only to the original root $s_t$**. No access to earlier rollâ€‘outs. | Matches an embedded agent interacting with the live world.                                                               | Most general and most realistic; avoids huge tables.                                                 | Hardest theoretical settingâ€”branching away from the real trajectory is expensive.           |   |                        |

*Source pages*: formal definitions and discussion of the three interfaces appear in **Lectureâ€¯5, pp.â€¯3â€“4** and **Lectureâ€¯6, pp.â€¯7â€“8**

---

### 2.2â€ƒFormal Simulator Definition

A **finiteâ€‘MDP simulator** with access mode tag $\chi\in\{\text{global},\text{local},\text{online}\}$ is a blackâ€‘box oracle

$$
\texttt{Sim}_\chi:\;
\begin{cases}
\texttt{reset()}                   &\!\!\to s_0\in S,\\[2pt]
\texttt{step}(a)                   &\!\!\to (r,s')\text{ and updates internal state to }s',\\[2pt]
\texttt{teleport}(s,a)\text{ (if }\chi=\text{global})\!\!\!\! &\!\!\to (r,s'),\\[2pt]
\texttt{restore}(s)\text{ (if }s\text{ was generated and }\chi\in\{\text{local},\text{global}\})
\end{cases}
$$

subject to

$$
r\sim r_a(s),\quad s'\sim P_a(s),\qquad (S,A,P,r,\gamma)\text{ a fixed discounted MDP}.
$$

The planner $\Pi$ must respect the admissible call set determined by $\chi$. Violating the protocol is undefined behaviour.

---

### 2.3â€ƒPlanner State & Query Complexity

* **Search tree**  â€ƒ$T_h=(V_h,E_h)$ after $h$ expansion levels contains exactly the states reachable under the admissible access mode.
* **Local branching factor**â€ƒ$b\_h^\chi \le mA$ (stochastic case, $m$ samples per $(s,a)$).
* **Query complexity** per planning call:

$$
Q_\chi(H,m)=
\begin{cases}
A^{H} & \text{deterministic, global/local/online},\\[4pt]
(mA)^{H} & \text{stochastic, global/local},\\[4pt]
(mA)^{H}\!\!\!\! & \text{stochastic, online *with reset*},\\[4pt]
\text{open}      & \text{stochastic, online *without reset* (open problem).}
\end{cases}
$$

Lowerâ€‘bound needleâ€‘inâ€‘aâ€‘haystack arguments (depthâ€‘$H$ $A$-ary tree with a single rewarding leaf) show that any $\delta$-sound planner needs $\Omega(A^{H})$ queriesâ€”even under global accessâ€”when $\delta<\gamma/(1-\gamma)$.&#x20;

---

### 2.4â€ƒWhy Localâ€¯â‰ˆâ€¯Global in Worstâ€‘Case Runtime

Lectureâ€¯6 proves that sparseâ€‘sampling under **local** access matches globalâ€‘access upper bounds:

$$
Q_{\text{local}}(H,m)=O\bigl((mA)^{H}\bigr),\quad
m\;\text{independent of }|S|.
$$

The key technical insight (p.â€¯6) is that the planner ever consults at most $(mA)^{h}$ distinct simulator states at depth $h$; a union bound over this *subtree*â€”not over the whole $|S|$â€”keeps the Hoeffding error term logarithmic in $(mA)^{H}$ .

---

### 2.5â€ƒAlgorithmic Skeletons per Access Mode

1. **Global** â€“ Dynamic Programming (table or explicit sampled model).
2. **Local** â€“ *Sparse Samplingâ€¯/â€¯Monteâ€‘Carlo Tree Search*:

   ```text
   def Value(s, k):
       if k == 0: return 0
       for a in A:
           Q[a] = r(s,a) + (Î³/m) * Î£_{s'âˆˆC(s,a)} Value(s', k-1)
       return max_a Q[a]
   ```

   here `C(s,a)` is the list of `m` children produced by `Sim.restore(s); Sim.step(a)`; guaranteed to run in $O((mA)^k)$.
3. **Online** â€“ Same code but `restore` is allowed only to the **root** $s_t$. Practical variants: UCT, progressive widening; theory still immature.

Pseudoâ€‘code adapted from Lectureâ€¯5, p.â€¯5 and Lectureâ€¯6, pp.â€¯2â€“3 .

---

### 2.6â€ƒInformation Hierarchy & Implications

$$
\text{online} \;\subset\; \text{local} \;\subset\; \text{global}.
$$

*Any* planner that works under online access automatically works under the other two modes (just ignore extra capabilities), but not viceâ€‘versa. Hence:

* **Soundness monotone:** if $\Pi$ is $\delta$-sound under online access, it remains $\delta$-sound under local/global access for the *same* query budget.
* **Complexity gaps:** no known separation between global and local; potential logarithmic or polynomial gap between local and pure online without reset (open).&#x20;

---

### 2.7â€ƒSingleâ€‘Sentence Takeaway

Moving from **explicit global** tables to **implicit local/online** simulators eliminates the $|S|$-factor in planning cost while preserving worstâ€‘case optimal query complexity $Î˜(A^{H})$; the only price is tighter control of howâ€”and whereâ€”the planner may query the environment.

---

#### Progressâ€¯ğŸ”„

* Defined the three access modes formally, with simulator API and complexity consequences.
* Proved why local access inherits global upper/lower bounds via subtree union bounds.
* Highlighted open problem: tight bounds for pure online access *without* resets.
* **Next to expand:** Sectionâ€¯3â€¯â€¯â€œAssumed environment model (finite discounted MDP + simulator)â€.


---

## 3â€¯â€¯Assumed Environment Model

### 3.1â€ƒFinite Discounted MDP

We model the environment as a **finite discounted Markov Decision Process (MDP)**

$$
\mathcal M=(S,\;A,\;P,\;r,\;\gamma),\qquad  
|S|<\infty,\;|A|=A,\;0<\gamma<1,
$$

where

| symbol      | meaning                               | type / range                    |
| ----------- | ------------------------------------- | ------------------------------- |
| $S$         | state space                           | finite set                      |
| $A$         | action set (identical in every state) | finite set $\{1,\dots ,A\}$     |
| $P_a(s,s')$ | oneâ€‘step transition kernel            | $\sum_{s'}P_a(s,s')=1$          |
| $r_a(s)$    | expected immediate reward             | $[0,1]$ (w\.l.o.g. via scaling) |
| $\gamma$    | discount factor                       | real, $0<\gamma<1$              |

> *Lectureâ€¯5 introduces the same tuple on pageâ€¯1 and fixes the reward range to $[0,1]$ for simplicity.*&#x20;

---

### 3.2â€ƒValue Functions

For any (possibly stochastic) policy $\pi:A\!\leftarrow\!S$

$$
v^\pi(s)\;=\; \mathbb E\!\Bigl[\sum_{t=0}^{\infty}\gamma^{t}r_{a_t}(s_t)\mid s_0=s\Bigr],\qquad  
q^\pi(s,a)\;=\; r_a(s)+\gamma\sum_{s'}P_a(s,s')v^\pi(s').
$$

The optimal value functions $v^\*,q^\*$ satisfy the Bellman optimality equations

$$
v^\*(s)=\max_{a}q^\*(s,a),\qquad  
q^\*(s,a)=r_a(s)+\gamma\sum_{s'}P_a(s,s')v^\*(s').  
$$

---

### 3.3â€ƒSimulator (Generative Model)

Instead of explicit tables $P,r$, the planner interacts with a **blackâ€‘box simulator**

$$
\texttt{Sim}:(s,a)\;\longmapsto\;(r,s')\quad\text{with}\quad  
r\sim r_a(s),\;s'\sim P_a(s,\cdot).
$$

* **IID property:** repeated calls with the same $(s,a)$ return independent samples from the same distribution.
* **Reset capability:** the caller may supply any $s\in S$ as the *root* for a new query tree (local access) or roll the simulator back to the real environment state (online access).
* **Deterministic special case:** if $P_a(s,\cdot)$ has a single support point, one sample exactly equals the expectation, eliminating statistical error.

> *The definition above follows the â€œMDP simulatorâ€ block on pageâ€¯2 of Lectureâ€¯5.*&#x20;

---

### 3.4â€ƒQuery Budget and Horizon Notation

We measure planning cost per call via

| symbol | definition                                                                                                                |
| ------ | ------------------------------------------------------------------------------------------------------------------------- |
| $m$    | simulator queries per inner node (stochastic case)                                                                        |
| $H$    | lookâ€‘ahead depth such that the residual tail $\gamma^{H}/(1-\gamma)\le \varepsilon/2$ (formally $H_{\gamma,\varepsilon}$) |
| $Q$    | overall query budget $Q=(mA)^{H}$ (stochastic) or $A^{H}$ (deterministic)                                                 |

These quantities will drive the upperâ€‘ and lowerâ€‘bound proofs in later sections.

---

### 3.5â€ƒWhy This Abstraction?

1. **Decouples modelling and planning:** algorithms need only *sample* transitionsâ€”crucial when $P$ is generated by a physics engine, game server, or learned model.
2. **Enables $|S|$-independent runtimes:** simulators let us compute expectations via Monteâ€‘Carlo rollâ€‘outs, avoiding table scans that scale with $|S|$.
3. **Matches embedded control:** most real agents cannot query hypothetical states; they can, however, reset or clone a simulator of their environment.

> *These motivations are discussed in Lectureâ€¯5, pp.â€¯2â€“3 and Lectureâ€¯6, p.â€¯1 when introducing â€œgenerative modelsâ€.*&#x20;

---

### 3.6â€ƒTakeâ€‘Away

All forthcoming algorithms assume nothing more than **finite rewards, discounting, and a generative simulator** providing IID samples $(r,s')$ for any queried $(s,a)$; this minimal interface underpins the $|S|$-free complexity results developed in Sectionsâ€¯4â€“16.

---

#### Progressâ€¯ğŸ”„

* Formalised the discounted MDP, value functions, and simulator oracle.
* Clarified query budgets $m,H,Q$ that parameterise later bounds.
* Cited Lectureâ€¯5 (pagesâ€¯1â€“3) and Lectureâ€¯6 (pageâ€¯1) for primary definitions.

**Next up:** Sectionâ€¯4â€¯â€¯â€œFormal statement of the onlineâ€‘planning problemâ€.

---

## 4â€¯â€¯Formal Statement of the Onlineâ€‘Planning Problem

### 4.1â€ƒInputs, Interface & Call Contract

At *each* decision epoch $t=0,1,\dots$ the **online planner** $\Pi$ is invoked with

$$
\bigl(A,\;s_{t},\;\texttt{Sim},\;\varepsilon,\;\delta,\;Q\bigr),
$$

where

| Symbol                | Type / meaning                                                                                                | Scope                 |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | --------------------- |
| $A$                   | finite **action set** (cardinality $A\ge 2$)                                                                  | fixed for the episode |
| $s_{t}$               | **root state** (current real environment state)                                                               | varies per call       |
| `Sim`                 | blackâ€‘box **simulator** implementing the MDP; admissible API depends on access mode (globalâ€¯/â€¯localâ€¯/â€¯online) | fixed                 |
| $\varepsilon\in(0,1)$ | desired **value subâ€‘optimality** per call                                                                     | userâ€‘set              |
| $\delta\in(0,1)$      | allowable **failure probability** per call                                                                    | userâ€‘set              |
| $Q\in\mathbb N$       | **query budget** on simulator calls for this invocation                                                       | userâ€‘set or adaptive  |

`Sim` satisfies the contract (Lectureâ€¯5,â€¯pp.â€¯2â€“3)

$$
\texttt{Sim}(s,a)\;\longmapsto\;(r,s'),\qquad r\sim r_a(s),\;s'\sim P_a(s,\cdot),
$$

and returns IID draws on repeated identical queries.

### 4.2â€ƒOnline Planner Definition

> **DefinitionÂ 1 (Wellâ€‘formed planner).**
> A (possibly randomised) algorithm $\Pi$ is *wellâ€‘formed* under access mode $\chi\in\{\text{global},\text{local},\text{online}\}$ if for **every** finite discounted MDP $\mathcal M$ and every input tuple $(A,s_t,\texttt{Sim}_\chi,\varepsilon,\delta,Q)$:
> 1.Â $\Pi$ respects the interface restrictions of $\chi$ (Sectionâ€¯2).
> 2.Â $\Pi$ halts after at most $Q$ simulator queries.
> 3.Â $\Pi$ outputs an action $a_t\in A$.

### 4.3â€ƒSoundness Guarantee

For a fixed call, let the **plannerâ€‘induced policy** be

$$
\pi^{\Pi}(s)\;=\;\begin{cases}
\Pi(A,s,\texttt{Sim},\varepsilon,\delta,Q) & \text{if }s=s_{t}\text{ for some call}\\
\text{any tieâ€‘breaking rule} & \text{otherwise}.
\end{cases}
$$

Let $v^\pi$ (resp. $v^\*$) denote the value function of $\pi$ (resp. the optimal policy) in $\mathcal M$.

> **DefinitionÂ 2 ($(\varepsilon,\delta)$-soundness).**
> A wellâ€‘formed planner $\Pi$ is **$(\varepsilon,\delta)$-sound** if, for *every* MDP $\mathcal M$ and every call root $s_t$,

$$
\Pr\!\Bigl[v^{\pi^{\Pi}}(s_t)\;\ge\;v^\*(s_t)-\varepsilon\Bigr]\;\ge\;1-\delta.
$$

The probability is over both the plannerâ€™s internal randomness **and** the simulatorâ€™s stochasticity.
(Lectureâ€¯5, Definition â€œ$\delta$-sound Plannerâ€, pp.â€¯3â€“4)&#x20;

### 4.4â€ƒComplexity Measures

Two orthogonal cost metrics are recorded **per call**:

1. **Query complexity** $m\;=\;\#\{\texttt{Sim}Â \text{invocations}\}\;\le Q$.
2. **Arithmetic complexity** $W\;=$ basic arithmetic/logical ops executed by $\Pi$.

When analysing families of planners we express these as functions of

$$
(m,W) \;=\; f\bigl(A,\;H_{\gamma,\varepsilon},\;\varepsilon,\;\delta,\;\text{accessÂ mode}\bigr),
$$

where $H_{\gamma,\varepsilon}=\bigl\lceil\log\!\bigl(\varepsilon(1-\gamma)/2\bigr)/\log\gamma\bigr\rceil$ is the **effective horizon** ensuring tailâ€‘value truncation â‰¤â€¯$\varepsilon/2$.

### 4.5â€ƒDecision Map vs. Policy Map

Classical (offline) planning seeks a **decision map** $\pi:S\to A$.
Online planning replaces this by a *transient* mapping

$$
\Pi_s: s \;\mapsto\; a\quad\text{for the current }s=s_{t}.
$$

Because only one state is queried, both $m$ and $W$ are allowed to depend **exponentially in $H$** yet must be *independent of $|S|$*â€”the crux of online efficiency (Lectureâ€¯5, pp.â€¯1â€“2).

### 4.6â€ƒProblem Statement (concise form)

> **Onlineâ€‘Planning Decision Problem**
> *Given* a finite discounted MDP simulator, accuracy pair $(\varepsilon,\delta)$, and perâ€‘call budget $Q$, design a $(\varepsilon,\delta)$-sound planner $\Pi$ minimising query complexity $m$ (and secondarily arithmetic cost $W$) subject to the admissible access mode.

Lowerâ€bound results (Â§11) show that $m=\Omega(A^{H_{\gamma,\varepsilon}})$ even under global access when $\delta<\gamma/(1-\gamma)$ , while Sectionsâ€¯9â€“10 construct matching upper bounds using sparse sampling.

### 4.7â€ƒSingleâ€‘Sentence Takeaway

An **online planner** is a subroutine that, given only the *current* state and a simulator oracle, must output an $\varepsilon$-optimal action with probability $1-\delta$; its performance is measured by simulator queries $m$ and arithmetic work $W$, both ideally independent of the (potentially massive) state space.

---

#### Progressâ€¯ğŸ”„

* **Completed Sectionâ€¯4**: formalised inputs, planner definition, $(\varepsilon,\delta)$-soundness, and complexity measures, with citations to Lectureâ€¯5.
* Remaining open issues:

  1. Prove the equivalence between $\varepsilon$-greedy action selection and $\varepsilon/(1-\gamma)$ value loss (will appear in Sectionâ€¯7).
  2. Instantiate concrete algorithms (Sectionsâ€¯9â€“10) and derive explicit $m,W$ as functions of $H$.

Next up: **Sectionâ€¯5â€¯â€¯Optimisation language & oracle types**.


---

## 5â€¯â€¯Optimisation Language & Oracle Types

### 5.1â€ƒWhy Borrow Notions from Blackâ€‘Box Optimisation?

The onlineâ€‘planning problem can be viewed as a **stochastic optimisation task** over the finite action set $A$:

$$
\max_{a\in A} Q^{\pi_{t-1}}(s_t,a),
\qquad 
Q^{\pi_{t-1}}(s_t,a)=r_a(s_t)+\gamma\; \mathbb E_{s'\sim P_a(s_t)}\bigl[V^{\pi_{t-1}}(s')\bigr].
$$

Because the planner **cannot** read $P_a(s_t)$ or $r_a(s_t)$ directly, it must optimise this objective through *queries* to a blackâ€‘box simulator.  This is precisely the scenario studied in optimisation theory under various **oracle models**.  Lectureâ€¯5 introduces the same viewpoint, explicitly describing MDP simulators â€œin a language similar to that used to describe optimisation oracles (zerothâ€‘order, firstâ€‘order, noisy, etc.)â€ .

> **Takeaway.**  Thinking in â€œoracleâ€ terms lets us port lowerâ€‘ and upperâ€‘bound techniques from convex/nonâ€‘convex optimisation to online planning.

---

### 5.2â€ƒCanonical Oracle Classes

| Oracle type                    | Optimisation analogue         | Call signature $O(\cdot)$ | Guarantees returned        | MDP instantiation                                    |
| ------------------------------ | ----------------------------- | ------------------------- | -------------------------- | ---------------------------------------------------- |
| **Deterministic zerothâ€‘order** | Evaluate $f(x)$               | $O(x)\to f(x)$            | Exact value                | Deterministic simulator; single sample = expectation |
| **Stochastic zerothâ€‘order**    | Evaluate noisy $f(x)+\xi$     | $O(x)\to \tilde f(x)$     | Unbiased, bounded variance | Standard MDP simulator call $\texttt{Sim}(s,a)$      |
| **Firstâ€‘order**                | Evaluate $(f(x),\nabla f(x))$ | $O(x)\to (f,\nabla f)$    | Exact gradient             | Requires full $P_a(s)$ vector â€” *unavailable* online |
| **Higherâ€‘order / Hessian**     | $\nabla^2 f(x)$ access        | $O(x)\to \nabla^2 f$      | Curvature info             | Impossible without model tables                      |
| **Batch oracle**               | Vector of points              | $O(\{x_i\})\to\{f(x_i)\}$ | Parallel values            | Parallel rollâ€‘outs / GPU simulators                  |

Only the **stochastic zerothâ€‘order oracle** is universally available in online planning; deterministic plans are a strict subset when $P_a(s)$ is Diracâ€‘supported.&#x20;

---

### 5.3â€ƒFormal Definition (Simulator as Stochastic Zerothâ€‘Order Oracle)

A finiteâ€‘MDP **simulator oracle** $\mathcal O$ is a map

$$
\mathcal O:(s,a)\;\xmapsto{\text{i.i.d.}}\; (R,S'),\qquad 
R\sim r_a(s),\; S'\sim P_a(s,\cdot).
$$

* **Zerothâ€‘order:** it reveals only the *realisation* of the value, never its gradient w\.r.t.\ parameters.
* **Unbiased:** $\mathbb E[\mathcal O(s,a)\mid s,a]=\bigl(r_a(s),P_a(s)\bigr)$.
* **Variance bound:** $R\in[0,1]$, hence $\operatorname{Var}(R)\le 1/4$; state variance bounded by indicator variables.

This oracle can be queried under three **access modalities** (global, local, onlineâ€”Sectionâ€¯2), but its statistical nature is identical across them.

---

### 5.4â€ƒQuery Complexity vs. Statistical Complexity

Optimisation theory separates

1. **Information complexity** â€“ *number* of oracle queries needed to achieve $\varepsilon$-optimality with confidence $1-\delta$.
2. **Computation complexity** â€“ arithmetic cost once samples are drawn.

For smooth convex functions a firstâ€‘order oracle yields $O(1/\sqrt{\varepsilon})$ rates; for MDP planning the stochastic zerothâ€‘order oracle incurs $O((mA)^{H_{\gamma,\varepsilon}})$ queries (Sectionsâ€¯9â€“10).  The gap is rooted in the **lack of gradient structure**â€”every action branch must be *sampled* instead of *differentiated*.&#x20;

---

### 5.5â€ƒBridging Terminologies

| Optimisation term      | Planning counterpart                                   |
| ---------------------- | ------------------------------------------------------ |
| Decision variable $x$  | Action sequence along a depthâ€‘$H$ subtree              |
| Objective $f(x)$       | Truncated value $Q^{(H)}(s_0,x)$                       |
| Noisy evaluation       | Monteâ€‘Carlo rollout                                    |
| Smoothness / Lipschitz | Reward range $R_{\max}=1$; contraction factor $\gamma$ |
| Strong convexity       | *None* (maxâ€‘operator is nonâ€‘convex)                    |

This dictionary clarifies why **Hoeffdingâ€style concentration**, not gradientâ€‘based bounds, governs sample sizes: the objective lacks exploitable curvature and each sample has bounded range but no smoothness. The concentration tools collected in the supplementary note (see Theoremâ€¯1, Hoeffdingâ€™s inequality ï»¿pageâ€¯1) underpin all subsequent performance guarantees.&#x20;

---

### 5.6â€ƒConsequences for Algorithm Design

* **Deterministic MDPsÂ â‡’ deterministic zerothâ€‘order oracle:** one sample suffices per $(s,a)$. Value iteration reduces to exhaustive tree search $O(A^{H})$.
* **Stochastic MDPsÂ â‡’ stochastic zerothâ€‘order oracle:** Monteâ€‘Carlo **sparse sampling** must average $m$ calls per edge to shrink variance by $O(1/\sqrt m)$.
* **No gradient tricks:** policyâ€‘gradient or Newtonâ€‘style accelerations require differentiability of the value w\.r.t.\ controllable parameters, absent here.
* **Lower bounds match oracle power:** need $Î©(A^{H})$ queries (Sectionâ€¯11) which aligns with zeroâ€‘order complexity in worstâ€‘case optimisation.

---

### 5.7â€ƒSingleâ€‘Sentence Takeaway

Online planning operates under the \*\*weakest blackâ€‘box modelâ€”a stochastic zerothâ€‘order oracleâ€”\*\*so every theoretical bound that follows is ultimately an informationâ€‘theoretic consequence of how fast Monteâ€‘Carlo samples can approximate expectations, not of algorithmic ingenuity.

---

#### Progressâ€¯ğŸ”„

* Defined the simulator formally as a stochastic zerothâ€‘order oracle and positioned global/local/online access within oracle theory.
* Mapped optimisation oracle taxonomy to MDP planning, highlighting why only zerothâ€‘order information is generically available.
* Derived immediate implications for queryâ€‘complexity analysis and previewed the need for concentration inequalities (linked to Sectionâ€¯14).

*Next up*: **Sectionâ€¯6â€¯â€¯Agentâ€“environment interaction loop**.


---

## 6â€¯â€¯Agentâ€“Environment Interaction Loop

### 6.1â€ƒWhere the Planner Sits in the RL Feedback Cycle

At each discrete decision epoch $t$ the agent, planner, and simulator participate in the following **fourâ€‘step handshake**:

| Step | Component                | Input                                             | Computation                                                                             | Output                                  |
| ---- | ------------------------ | ------------------------------------------------- | --------------------------------------------------------------------------------------- | --------------------------------------- |
| â€¯â‘    | **Environment**          | â€”                                                 | Evolves according to the real MDP $\mathcal M$                                          | Current state $s_t$ revealed to planner |
| â€¯â‘¡   | **OnlineÂ plannerÂ $\Pi$** | $A,\,s_t,\,\texttt{Sim},\,(\varepsilon,\delta,Q)$ | Executes â‰¤â€¯$Q$ simulator calls (Algorithmâ€¯9/10) to evaluate a depthâ€‘$H$ lookâ€‘ahead tree | Action $a_t\in A$                       |
| â€¯â‘¢   | **Agent actuator**       | $a_t$                                             | Sends $a_t$ to the physical or simulated environment                                    | â€”                                       |
| â€¯â‘£   | **Environment**          | $a_t$                                             | Draws $r_t\sim r_{a_t}(s_t)$ and $s_{t+1}\sim P_{a_t}(s_t,\cdot)$                       | New state $s_{t+1}$ (loop to â‘ )         |

*Lectureâ€¯5 embeds exactly this fourâ€‘stage loop in its problem statement for online planning (pagesâ€¯2â€“3). The simulator acts as a surrogate transition oracle between stepsâ€¯â‘¡ andâ€¯â‘£* .

> **Takeaway:** The planner is invoked **inside** the perceptionâ€‘action cycle; its latency is paid before every real action, so perâ€‘call runtimeâ€”not offline complexityâ€”governs feasibility.

---

### 6.2â€ƒFormal Timing & Budget Parameters

* **Query budgetÂ $Q$.** Maximum number of `Sim` invocations per planner call (Sectionâ€¯4).
* **DepthÂ $H$.** Number of Bellman backâ€‘ups; chosen as $H_{\gamma,\varepsilon}=\lceil\log(\varepsilon(1-\gamma)/(2\gamma))/\log\gamma\rceil$ (Sectionâ€¯9).
* **Wallâ€‘clock latency.** $T_{\text{plan}} = \Theta((mA)^{H})$ in the stochastic case; must be $<$ environment step time for realâ€‘time control.
* **Interaction horizon.** The loop repeats indefinitely; cumulative performance is measured by the value of the **plannerâ€‘induced policy** $\pi^{\Pi}$ (Definitionâ€¯4.3).

---

### 6.3â€ƒSimulator Call Semantics

Inside stepâ€¯â‘¡ the planner may issue three primitives depending on access mode (Sectionâ€¯2):

1. `reset(s_root)` â€“ initialise internal simulator state to root $s_t$.
2. `step(a)` â€“ advance one transition, returning $(r,s')$.
3. `restore(s_checkpoint)` â€“ if **local** access, jump back to an earlier generated state.

Per Lectureâ€¯6 the plannerâ€™s recursion alternates `step` and `restore` to build a boundedâ€‘width search tree; branching factor is $A$ (deterministic) or $mA$ (stochastic) .

---

### 6.4â€ƒPseudocode Embedding (Stochastic Sparseâ€‘Sampling)

```python
def act_online(s_root, Îµ, Î´):
    # --- Step 2: planner ---
    q_hat = value_estimate(s_root, H=H_{Î³,Îµ}, m=samples_per_action)
    a_root = argmax_a q_hat[(s_root, a)]
    return a_root        # delivered to actuator (Step 3)
```

`value_estimate` is the depthâ€‘$H$ Monteâ€‘Carlo recursion from Sectionâ€¯9; it uses `step` to draw $m$ IID successors and `restore(s_root)` between sibling branches.

---

### 6.5â€ƒSoundness in the Loop

Because $\Pi$ is $(\varepsilon,\delta)$-sound (Definitionâ€¯4.3), iterating the loop yields a **nonâ€‘stationary but nearâ€‘optimal trajectory**:

$$
\Pr\!\Bigl[v^\*(s_t)-v^{\pi^{\Pi}}(s_t)\le\varepsilon\;\; \forall t\Bigr]\;\ge\;1-\delta^{\text{tot}},
$$

with $\delta^{\text{tot}}\le\sum_{t}\delta_t$ (union bound). Choosing perâ€‘call $\delta_t = \delta_0\,\gamma^{t}$ keeps the cumulative failure probability bounded by $\delta_0/(1-\gamma)$.

---

### 6.6â€ƒLatencyâ€‘Accuracy Tradeâ€‘offs

| Increasing â€¦       | Pros                                                | Cons                                                 |
| ------------------ | --------------------------------------------------- | ---------------------------------------------------- |
| $m$ (samples/edge) | Lower Monteâ€‘Carlo variance, tighter Hoeffding bound | Exponential cost $(mA)^{H}$                          |
| $H$ (lookâ€‘ahead)   | Smaller truncation error $\gamma^{H}/(1-\gamma)$    | Exponential cost $A^{H}$; deeper simulator rollâ€‘outs |
| $Q$ (budget)       | Allows larger $m,H$                                 | Longer wallâ€‘clock delay                              |

Lectureâ€¯6 FigureÂ 1 (pageâ€¯2) plots empirical runâ€‘time vs. $m$, illustrating the steep rise beyond the realâ€‘time threshold.&#x20;

---

### 6.7â€ƒSingleâ€‘Sentence Takeaway

The planner is executed **between perception and action**, querying the simulator up to $Q$ times per call; adhering to this tight inner loop transforms theoretical query complexity $(mA)^{H}$ into a hard realâ€‘time latency budget that dictates feasible choices of $m$ and $H$.

---

#### Progressâ€¯ğŸ”„

* Fully specified the realâ€‘time fourâ€‘step interaction cycle and mapped query complexity to latency.
* Connected simulator primitives to access modes and sparseâ€‘sampling recursion.
* Provided latencyâ€“accuracy tradeâ€‘off table and cumulative soundness accounting.

**Next section to develop:** **7â€¯â€¯Online planner & $\delta$-soundness â€“ formal definition and practical implications**.


---

## 7â€¯â€¯Online Planner &â€¯$\delta$-Soundness

### 7.1â€ƒDefinitions

**Online planner (restated).**Â A (possibly randomized) algorithm $\Pi$ that, for any finite discounted MDP simulator $\texttt{Sim}$ and every call tuple
$(A,\,s_t,\,\texttt{Sim},\,\varepsilon,\,\delta,\,Q)$,

1. respects the admissible query set of the chosen access mode (global, local, or online);
2. makes at most $Q$ simulator calls;
3. returns an action $a_t\in A$,

is said to be **wellâ€‘formed**.&#x20;

---

**$(\varepsilon,\delta)$-soundness.**Â Let the *plannerâ€‘induced policy* be

$$
\pi^{\Pi}(s)\;=\;\Pi(A,s,\texttt{Sim},\varepsilon,\delta,Q)
\quad\text{when }\Pi\text{ is invoked with root }s.
$$

$\Pi$ is **$(\varepsilon,\delta)$-sound** if, for every MDP served by the simulator and for every call root $s_t$,

$$
\Pr\!\bigl[v^{\pi^{\Pi}}(s_t)\;\ge\;v^{\!*}(s_t)-\varepsilon\bigr]\;\ge\;1-\delta
\tag{7.1}
$$

where the probability is over both the plannerâ€™s internal randomness and the simulatorâ€™s stochasticity.&#x20;

---

**$\varepsilon$-optimising action.**Â For any state $s$, an action $a$ is **$\varepsilon$-optimising** if

$$
q^{\!*}(s,a)\;\ge\;v^{\!*}(s)-\varepsilon .
\tag{7.2}
$$

---

### 7.2â€ƒWhy Bounding Action Error Suffices

The Bellman optimality operator $T$ is a $\gamma$-contraction in the $\ell_\infty$ norm. Hence value errors propagate geometrically:

$$
\|Tv-Tv^{\!*}\|_\infty\;\le\;\gamma\|v-v^{\!*}\|_\infty .
$$

This yields the classical **policy loss lemma**:

> **Lemmaâ€¯7.1 (Îµâ€‘optimising â‡’ value bound).**Â If a policy $\pi$ satisfies
> $\sum_a \pi(a|s)\,q^{\!*}(s,a)\;\ge\;v^{\!*}(s)-\varepsilon$ for every state $s$,
> then
>
> $$
> v^{\!*}(s)-v^{\pi}(s)\;\le\;\frac{\varepsilon}{1-\gamma}\qquad\forall s.
> \tag{7.3}
> $$

*Proof sketch.*Â Write the Bellman equations for $v^{\!*}$ and $v^{\pi}$; subtract; apply the contraction property and unroll the recursion. Detailed derivation appears in Lectureâ€¯6, p.â€¯3.  âˆ

---

### 7.3â€ƒSoundness via Approximateâ€¯$Q$-Functions

Suppose the planner returns an approximate actionâ€‘value function $\hat q$ and acts **greedily**:
$a_t=\arg\max_a \hat q(s_t,a)$.

> **Lemmaâ€¯7.2 (greedyâ€‘gap transfer).**Â If
> $\|\hat q-q^{\!*}\|_\infty\le\varepsilon$
> then the greedy policy is $2\varepsilon$-optimising; consequently
> $v^{\!*}-v^{\pi_{\hat q}}\le 2\varepsilon/(1-\gamma)$.&#x20;

Hence to achieve perâ€‘call value errorÂ $\le\varepsilon$ it suffices to drive the uniform $Q$-error below $\tfrac{1}{2}(1-\gamma)\varepsilon$.

---

### 7.4â€ƒRandomised Planners: â€œÎ´â€‘Soundâ€ vs. â€œAlmostâ€‘Îµâ€

Sparseâ€‘sampling planners are randomised because they build Monteâ€‘Carlo trees. Let

$$
F_t=\Bigl\{\text{â€œevent that planner fails at time }t\text{â€}\Bigr\}.
$$

Hoeffdingâ€™s inequality gives
$\Pr(F_t)\le\delta$ when the sample size $m$ (per edge) satisfies the bound derived in Â§14.&#x20;

> **Lemmaâ€¯7.3 (Îµ,Î¶â€‘hybrid bound).**Â If every call produces an $\varepsilon$-optimising action with probability $1-\zeta$, then
>
> $$
> v^{\!*}-v^{\pi}\;\le\;\frac{\varepsilon+2\zeta}{1-\gamma}.
> \tag{7.4}
> $$
>
> This is proved by conditioning on the â€œgoodâ€ and â€œbadâ€ events and applying Lemmaâ€¯7.1 on the good event.&#x20;

**Interpretation.**Â Choosing $\zeta=\delta$ shows that a $\delta$-sound planner accords with the classical $(\varepsilon,\delta)$ PAC framework: occasional action failures inflate the value loss by at most $2\delta/(1-\gamma)$.

---

### 7.5â€ƒDesignÂ Parameters forÂ $\delta$-Soundness

Combine LemmasÂ 7.1â€“7.3 with the sparseâ€‘sampling accuracy guarantee (proof in Sectionâ€¯16):

* choose horizon $H=H_{\gamma,\varepsilon/3}$;
* choose samples per edge

  $$
  m\;\ge\;\frac{18}{\varepsilon^{2}(1-\gamma)^{6}}
          \log\!\Bigl(\frac{12A^{H+1}}{(1-\gamma)^{2}\delta}\Bigr).
  \tag{7.5}
  $$

Then

1. Hoeffdingâ€¯+â€¯union bound $\Rightarrow$ all backup errors $<\varepsilon/3$ w\.p.â€¯$1-\delta$;
2. greedy policy is $2\varepsilon/3$-optimising (Lemmaâ€¯7.2);
3. overall value loss $<\varepsilon$ (Lemmaâ€¯7.1).

Hence the planner is $(\varepsilon,\delta)$-sound while using
$O\!\bigl((mA)^{H}\bigr)$ simulator queries per callâ€”independent of $|S|$.&#x20;

---

### 7.6â€ƒRelationship to Other Confidence Notions

| Guarantee                | Definition                                                                       | Typical use                          |                                  |                                  |
| ------------------------ | -------------------------------------------------------------------------------- | ------------------------------------ | -------------------------------- | -------------------------------- |
| **$\delta$-sound**       | InequalityÂ (7.1) for *every* call                                                | Worstâ€‘case safety in embedded agents |                                  |                                  |
| PACâ€‘MDP                  | Value loss $<\varepsilon$ after *polynomial* burnâ€‘in steps with prob. $1-\delta$ | Exploration in learning settings     |                                  |                                  |
| Highâ€‘confidence planning | (                                                                                | \hat q-q^{!\*}                       | \le\varepsilon) w\.p. $1-\delta$ | Offline planners with simulators |

$\delta$-soundness is the *strongest*: it must hold **on every invocation**, not just asymptotically.

---

### 7.7â€ƒSingleâ€‘Sentence Takeaway

A planner is **$(\varepsilon,\delta)$-sound** when every call returns an $\varepsilon$-nearâ€‘optimal action with probability $1-\delta$; thanks to contraction properties, this perâ€‘action control ensures perâ€‘state value loss $\le\varepsilon/(1-\gamma)$, so designing Monteâ€‘Carlo depthâ€‘$H$ trees with Hoeffdingâ€‘based sample counts directly yields rigorous online guarantees independent of the stateâ€‘space size.

---

#### Progressâ€¯ğŸ”„

* Formalised $(\varepsilon,\delta)$-soundness and connected it to actionâ€‘level errors.
* Proved three lemmas (7.1â€“7.3) translating $Q$-function accuracy into value guarantees under randomness.
* Derived explicit sample complexity (Eq.â€¯7.5) required for sparseâ€‘sampling planners.
* Positioned $\delta$-soundness among neighbouring confidence notions (PACâ€‘MDP, offline bounds).

**Next section to develop:** **8â€¯â€¯Computational cost metrics**.

---

## 8â€¯â€¯Computationalâ€‘Cost Metrics

Onlineâ€‘planning papers distinguish **how much data the planner must *pull* from the simulator** from **how much arithmetic it must *push* through the CPU/GPU** each time it is called.â€‚Both costs are expressed *per planner invocation* (i.e. per agent decision) so that they can be compared directly with an applicationâ€™s realâ€‘time budget.

| Symbol | Meaning                                                                                                                   | Where it comes from                          | Typical magnitude                                                            |
| ------ | ------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- | ---------------------------------------------------------------------------- |
| $m$    | **Query multiplicity** â€“ number of simulator calls made for *each* interior edge of the lookâ€‘ahead tree (stochastic case) | Hoeffdingâ€‘driven sample size (Sectionâ€¯14)    | $\tilde O\!\bigl((1-\gamma)^{-6}\varepsilon^{-2}\bigr)$ for Î´â€‘sound planners |
| $H$    | **Effective horizon** â€“ tree depth, chosen so that residual discount tail â‰¤â€¯$\varepsilon/2$                               | geometric tail bound $\gamma^{H}/(1-\gamma)$ | $\bigl\lceil \log(\varepsilon(1-\gamma)/2)/\log\gamma \bigr\rceil$           |
| $Q$    | **Total simulator calls** for the whole tree                                                                              | branching factor $\times$ depth              | see deterministic/stochastic rows below                                      |
| $W$    | **Arithmetic operations** on the agentâ€™s processor                                                                        | Bellman backâ€‘up recursion                    | proportional to same exponent as $Q$                                         |

### 8.1â€ƒDeterministic versus Stochastic Environments

| Environment assumption                                     | Branching factor per level | Simulator queries $Q$ | Arithmetic work $W$ | Source          |            |               |
| ---------------------------------------------------------- | -------------------------- | --------------------- | ------------------- | --------------- | ---------- | ------------- |
| Deterministic transitions ( (                              | P\_a(s)                    | =1) )                 | $A$                 | $\;A^{H}$       | $O(A^{H})$ | Lectureâ€¯5 Â§3  |
| Stochastic transitions (sparseâ€‘sampling, $m$ IID children) | $mA$                       | $\;(mA)^{H}$          | $O((mA)^{H})$       | Lectureâ€¯6 Â§2â€“3  |            |               |

*Both* entries are **independent of $|S|$** â€“ the core motivation of online planning.

### 8.2â€ƒMemory Footprint

A depthâ€‘$H$ sparseâ€‘sampling tree stores at most $(mA)^{H}$ nodes, each holding a constantâ€‘size tuple $(s,\hat q)$.â€‚Memory therefore scales identically to $Q$.â€‚For embedded agents with kilobytes of fast RAM this is often the *real* limit on $m$ or $H$.

### 8.3â€ƒWallâ€‘Clock Latency

If one simulator call plus one numeric backâ€‘up takes Ï„ microâ€‘seconds, a call to the planner costs roughly

$$
T_{\text{plan}} \;\approx\; \tau\,(mA)^{H}.
$$

> The *runtimeâ€‘vsâ€‘sampleâ€‘size* curve on **pageâ€¯2 of Lectureâ€¯6** plots $T_{\text{plan}}$ against $m$ for fixed $A$ and $H$, showing an exponential knee once $m$ grows beyond the Hoeffding minimum.&#x20;

For robotics or games the control loop might allow only 1â€“5â€¯ms; the chosen $H,m$ must respect this budget even though theory would like them larger.

### 8.4â€ƒLowerâ€‘Bound Perspective

The **needleâ€‘inâ€‘aâ€‘haystack** construction (depthâ€‘$H$ $A$-ary tree with a single rewarding leaf) proves that any $\delta$-sound planner *must* query Î©($A^{H}$) leaves when $\delta<\gamma/(1-\gamma)$ â€“ exactly the deterministic cost above.&#x20;
In stochastic settings this lower bound lifts to Î©($(A^{H})$) *samples*; matching upper bounds need $m=\Theta(1)$ only when Hoeffding permits.

### 8.5â€ƒPutting Cost Metrics into Design Choices

* **Small $H$, large $m$** â€“ suits *shortâ€‘horizon, highâ€‘variance* tasks (e.g. Atari games) where variance dominates truncation error.
* **Large $H$, small $m$** â€“ suits *longâ€‘horizon, lowâ€‘variance* tasks (e.g. deterministic gridâ€‘worlds).
* **Asymmetric branching (MCTS)** â€“ allocate $m$ adaptively along promising paths to soften the $(mA)^{H}$ explosion (Sectionâ€¯17).

### 8.6â€ƒSingleâ€‘Sentence Takeaway

Per decision, an online planner spends **$Q=(mA)^{H}$ simulator calls and the sameâ€‘order CPU cycles**, achieving stateâ€‘spaceâ€‘free control at the unavoidable exponential price in horizon $H$; careful tuning of $m$ and $H$ is essential to fit the agentâ€™s realâ€‘time budget without violating the Î´â€‘soundness guarantees.

---

#### Progressâ€¯ğŸ”„

* Defined formal cost symbols $m,H,Q,W$.
* Derived deterministic and stochastic complexity tables straight from Lecturesâ€¯5â€¯&â€¯6.
* Integrated memory and latency considerations and linked to the runtime graph on Lectureâ€¯6â€¯p.â€¯2.
* Clarified how lower bounds pin down the exponential $A^{H}$ factor.

**Next section to elaborate:** **9â€¯â€¯Baseline algorithm: Value iteration & actionâ€‘value recursion**.


---

## 9â€¯â€¯Baseline Algorithm: Valueâ€‘Iteration Recursion & Sparseâ€‘Sampling Trees

This section derives the canonical **depthâ€‘$H$ online planner** used throughout the literature, starting from classical value iteration and culminating in the sparseâ€‘sampling recursion that achieves $|S|$-independent runtimes.

### 9.1â€ƒBellman Optimality & Actionâ€‘Value Operators

For a finite discounted MDP $\mathcal M=(S,A,P,r,\gamma)$ (Â§3) define

$$
\begin{aligned}
(Tv)(s)           &= \max_{a\in A} \Bigl\{ r_a(s) + \gamma \,P_a(s)^{\!\top} v \Bigr\},\\[2pt]
(\tilde Tq)(s,a)  &= r_a(s) + \gamma \,P_a(s)^{\!\top} (Mq),\\[2pt]
(Mq)(s)           &= \max_{a\in A} q(s,a).
\end{aligned}
\tag{9.1}
$$

Both $T$ and $\tilde T$ are $\gamma$-contractions in $\|\cdot\|_\infty$; their unique fixed points are the optimal value and Qâ€‘functions $v^{*}, q^{*}$. &#x20;

### 9.2â€ƒTruncating the Infinite Horizon

Because rewards are bounded in $[0,1]$ the *tail value* after $H$ steps is at most $\gamma^{H}/(1-\gamma)$.  Choosing

$$
H_{\gamma,\varepsilon}\;=\;\Bigl\lceil \frac{\log \bigl(\varepsilon(1-\gamma)/2\bigr)}{\log \gamma}\Bigr\rceil
\tag{9.2}
$$

guarantees that ignoring steps beyond $H$ incurs â‰¤â€¯$\varepsilon/2$ value error.  This is the **effective horizon** quoted in earlier sections.&#x20;

### 9.3â€ƒDeterministic Case â€” Explicit Depthâ€‘$H$ Tree

If each $(s,a)$ has a **single** successor $g(s,a)$ we can unroll recursion without sampling:

```python
def Q_det(s, k):                       # returns [q(s,a) for a in A]
    if k == 0:
        return [0.0 for a in A]
    return [ r(s,a) + Î³ * max(Q_det(g(s,a), k-1)) for a in A ]
```

* **Time / memory**: $O(A^{k})$.
* **Root action**: $a^{\text{det}}_{*}(s)=\arg\max_{a} Q_{\text{det}}(s,H)[a]$.

Runtime is exponential in $H$ but independent of $|S|$. &#x20;

> *Takeaway*: Determinism collapses expectations into single pathsâ€”one sample per edge suffices.

### 9.4â€ƒStochastic Case â€” Sparseâ€‘Sampling Recursion

When transitions are stochastic we approximate each expectation by **$m$ IID simulator draws** $C(s,a)=\{s'_{1},\dots,s'_{m}\}$:

```python
def Q_sparse(s, k):
    if k == 0:
        return [0.0 for a in A]
    return [ r(s,a) +
             Î³/m * sum( max(Q_sparse(s_prime, k-1))
                         for s_prime in C(s,a) )  for a in A ]
```

*Draws* are generated lazily and cached (localâ€‘access assumption) so the same child state is reused when several ancestors reference it.

* **Branching factor**: $mA$.
* **Queries per call**: $(mA)^{H}$.
* **Root action**: $a^{\text{SS}}_{*}(s)=\arg\max_{a} Q_{\text{sp}}(s,H)[a]$.

Sparseâ€‘sampling was introduced by Kearns, Mansour & Ng and analysed in Lecturesâ€¯5â€“6.&#x20;

### 9.5â€ƒError Propagation Bound

Let $\hat q = Q_{\text{sp}}(\cdot,H)$.  With sample size

$$
m\;\ge\;\frac{18}{\varepsilon^{2}(1-\gamma)^{6}}
      \log\!\Bigl(\frac{12A^{H+1}}{(1-\gamma)^{2}\delta}\Bigr)
\tag{9.3}
$$

Hoeffding + union bound (Sectionâ€¯14) yield

$$
\|\hat q - q^{*}\|_{\infty} \le \varepsilon/3 \quad\text{w.p. }1-\delta.
$$

Greedy w\.r.t. $\hat q$ is thus $2\varepsilon/3$-optimising (Lemmaâ€¯7.2), giving an overall $(\varepsilon,\delta)$-sound planner when combined with tail error $\varepsilon/2$.&#x20;

### 9.6â€ƒPseudocode of a Complete Planner

```python
def PLAN_ONLINE(s_root, Îµ, Î´, Î³):
    H = ceil(log(Îµ*(1-Î³)/2, Î³))
    m = ceil( 18/(Îµ**2*(1-Î³)**6) *
              log( 12 * (len(A)**(H+1))  / ((1-Î³)**2 * Î´) ) )
    # build sparse tree starting at s_root
    Q_root = Q_sparse(s_root, H)
    return argmax(Q_root)             # action to execute
```

*Local* access is assumed so `C(s,a)` can restore to any sampled ancestor state; the algorithm degrades gracefully to pureâ€‘online if only root resets are allowed (Sectionâ€¯12).

### 9.7â€ƒComplexity Summary

| Environment   | Simulator calls | Arithmetic ops | References  |
| ------------- | --------------- | -------------- | ----------- |
| Deterministic | $A^{H}$         | $O(A^{H})$     | Lecâ€¯5 Â§3    |
| Stochastic    | $(mA)^{H}$      | $O((mA)^{H})$  | Lecâ€¯6 Â§2â€“3  |

Both grow exponentially with $H$ but never with $|S|$.

### 9.8â€ƒSingleâ€‘Sentence Takeaway

Valueâ€‘iteration unrolled to depth $H_{\gamma,\varepsilon}$ and coupled with *sparse sampling* produces an online planner that is provably $(\varepsilon,\delta)$-sound while requiring only $(mA)^{H}$ simulator queriesâ€”achieving the core goal of stateâ€‘spaceâ€‘independent computation.

---

#### Progressâ€¯ğŸ”„

* Formalised operators (Eq.â€¯9.1), horizon choice (Eq.â€¯9.2) and sample complexity (Eq.â€¯9.3).
* Provided deterministic and stochastic pseudocode with precise complexity.
* Linked error analysis to Sectionâ€¯14â€™s concentration tools and Sectionâ€¯7â€™s soundness lemmas.

**Next section to expand:** **10â€¯â€¯Upper bounds on runtime**.


---

\##â€¯10â€¯â€¯Upper Bounds on Runtime

\###â€¯10.1â€ƒDeterministic MDPsÂ â€“ Exponential in Horizon, *Not* in |S|

> **TheoremÂ 10.1 (Recursive Valueâ€‘Expansion).**
> LetÂ $\mathcal M$ be a deterministic discounted MDP with $|A|=A$, discountÂ $\gamma$ and rewards inÂ $[0,1]$.
> A depthâ€‘$H=H_{\gamma,\varepsilon}:=\bigl\lceil\log_{\gamma}(\varepsilon(1-\gamma)/2)\bigr\rceil$ unrolled value tree (AlgorithmÂ 9.3) returns an action whose value loss is at mostÂ $\varepsilon$.
> The algorithm performs exactly
>
> $$
> Q_{\text{det}}(A,H)=A^{H}
> $$
>
> simulator calls and $O(A^{H})$ arithmetic operations, *independently of $|S|$.*&#x20;

*Proof sketch.*
(i)Â Determinism collapses each expectation $P_a(s)^\top v$ into a single successor; depthâ€‘$H$ recursion thus explores an $A$-ary tree with $A^{H}$ leaves.
(ii)Â Truncating atÂ $H$ leaves tail valueÂ $\gamma^{H}/(1-\gamma)\le\varepsilon/2$.
(iii)Â Greedy action w\.r.t. the exact tree yields the remaining $\varepsilon/2$ error margin (Lemmaâ€¯7.1). Efficiency is immediate from the node count.&#x20;

---

\###â€¯10.2â€ƒStochastic MDPsÂ â€“ Sparseâ€‘Sampling Bound

> **TheoremÂ 10.2 (Kearnsâ€“Mansourâ€“Ng Sparse Sampling).**
> Under the same setting but with *stochastic* transitions, let the planner draw
> $m$ i.i.d. successors per interior node (AlgorithmÂ 9.4).
> With
>
> $$
> m\;\ge\;\frac{18}{\varepsilon^{2}(1-\gamma)^{6}}\,
> \log\!\Bigl(\frac{12A^{H+1}}{(1-\gamma)^{2}\delta}\Bigr)
> \tag{10.1}
> $$
>
> the returned action is $\varepsilon$-optimal with probabilityÂ $1-\delta$.
> Runtime per call is
>
> $$
> Q_{\text{sto}}(A,H,m)=(mA)^{H},\qquad  
> W_{\text{sto}}=O((mA)^{H}),
> $$
>
> again free of $|S|$.&#x20;

*Proof sketch.*

1. **Tree size.** Each level branches into $A$ actions, each action into $m$ sampled states â‡’ $(mA)^{H}$ nodes.
2. **Statistical error.** For any node $(s,a)$ the Monteâ€‘Carlo estimate of $P_a(s)^\top v$ has rangeÂ $R=1/(1-\gamma)$. Hoeffding gives
   $\Pr[|\hat{\mu}-\mu|>\eta]\le2\exp(-2m\eta^{2}(1-\gamma)^{2})$.&#x20;
3. **Union bound over the explored subtree.** There are at most $A^{h}m^{h}$ nodes at depthÂ $h$; summing over $h=0\!:\!H-1$ yields the $\log$ term inÂ (10.1).&#x20;
4. **Error propagation.** Bellman contraction multiplies depthâ€‘$k$ noise by $\gamma^{k}$; choosing $\eta=\varepsilon/3$ inÂ (10.1) bounds cumulative bias by $\varepsilon/3$ (Lemmaâ€¯7.2).
5. **Tail truncation.** As in deterministic case, $\gamma^{H}/(1-\gamma)\le\varepsilon/3$. Total loss â‰¤â€¯$\varepsilon$.

---

\###â€¯10.3â€ƒAsymptotic Expressions

With $H=\Theta\!\bigl(\log_{1/\gamma}(1/\varepsilon)\bigr)$:

| Environment   | Queries $Q$                                                                                                            | dominant factors   |
| ------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------ |
| Deterministic | $A^{\,\Theta(\log(1/\varepsilon))}$                                                                                    | horizon only       |
| Stochastic Â   | $\bigl(A/\varepsilon^{2}\bigr)^{\Theta(\log(1/\varepsilon))}\, \mathrm{poly}\!\bigl(\log(1/\delta),1/(1-\gamma)\bigr)$ | horizonÂ + sampling |

Both blow up **exponentially in effective horizon** but remain *polynomial* in $1/\varepsilon$ and entirely independent of $|S|$.

---

\###â€¯10.4â€ƒTightness and Practical Notes

* Matching Î©($A^{H}$) lower bounds (Sectionâ€¯11) show the exponential term is informationâ€‘theoretically necessary even with global access.
* Sparseâ€‘samplingâ€™s $m$ is often pessimistic; adaptive Monteâ€‘Carlo Tree Search variants (UCT, progressive widening) empirically reduce constant factors while obeying the same worstâ€‘case envelope.&#x20;
* For small discount gaps ($1-\gamma\ll1$) the $(1-\gamma)^{-6}$ term dominates; varianceâ€‘reduction or horizonâ€‘cutting heuristics become essential.

---

\###â€¯10.5â€ƒSingleâ€‘Sentence Takeaway

Depthâ€‘$H$ expansion delivers online actions in **$A^{H}$** time for deterministic worlds and **$(mA)^{H}$** for stochastic ones, where the sample sizeÂ $m$ chosen via Hoeffdingâ€‘union analysis (10.1) secures $(\varepsilon,\delta)$-soundnessâ€”achieving runtimes that explode only with planning horizon, *never* with the size of the state space.&#x20;

---

\##â€¯11â€¯â€¯Matching Lower Bounds

\###â€¯11.1â€ƒWhy a Lower Bound Is Needed
Sectionâ€¯10 showed that depthâ€‘$H$ planners achieve perâ€‘call cost

$$
Q_{\mathrm{det}}(H)=A^{H}\quad\text{(deterministic)},\qquad  
Q_{\mathrm{sto}}(H,m)=(mA)^{H}\quad\text{(stochastic)}.
$$

To argue these algorithms are *essentially optimal* we must prove **no** $(\varepsilon,\delta)$-sound online planner can beat the exponentialâ€‘inâ€‘$H$ dependence, *even with the most generous (global) simulator access*.  Lectureâ€¯5 gives the classic â€œneedleâ€‘inâ€‘aâ€‘haystackâ€ construction that turns this intuition into a rigorous lower bound.&#x20;

\###â€¯11.2â€ƒHard Instance Family

For fixed branching factor $A\ge 2$ and horizon $H$, build a deterministic MDP whose transition graph is an $A$-ary tree of depth $H$:

* **States.** Every tree node is a state; the root is $s_0$.
* **Actions.** At any nonâ€‘leaf node, each action $a\in\{1,\dots ,A\}$ deterministically selects the corresponding child.
* **Rewards.** All rewards are $0$ except at a single *secret* leaf $\hat\ell$ where $r(\hat\ell,\cdot)=1$.

There are $A^{H}$ leaves and the environment adversary chooses $\hat\ell$ **uniformly at random** before interaction begins. Transitions are deterministic, so a single simulator query fully reveals the successor state.

\###â€¯11.3â€ƒValue Structure of the Hard MDP

Let the discount factor satisfy $0<\gamma<1$.

* **Optimal value at root.**

  $$
  v^{\*}(s_0)=\gamma^{H}+\frac{\gamma^{H+1}}{1-\gamma} \;\;>\;\; \gamma^{H} \quad(\text{because tail sum }<1).
  $$

  (The agent collects rewardÂ 1 at depthÂ $H$ then only zeros.)
* **Value of any nonâ€‘reward path.** Zero.

Hence if the planner fails to reach $\hat\ell$ during its evaluation it will output an action whose value is **strictly less than** $v^{\*}(s_0)-\gamma^{H}$.

\###â€¯11.4â€ƒSoundness Forces Search Width

Assume we supply the planner with parameters $\varepsilon=\gamma^{H}/2$ and confidence $0<\delta<\gamma/(1-\gamma)$ as in Lectureâ€¯5.&#x20;

*If the planner does **not** discover the secret leaf it cannot distinguish the rootâ€™s optimal value from 0 within the error budget $\varepsilon$ and thus violates $(\varepsilon,\delta)$-soundness.*
Therefore, on every call the planner must sample until it **hits $\hat\ell$ with probability at least $1-\delta$**.

\###â€¯11.5â€ƒCounting Queries

Let $N$ be the (random) number of distinct rootâ€‘toâ€‘leaf paths the planner explores. Conditional on the plannerâ€™s internal randomness, the probability it happens to test the unique rewarding path equals $N/A^{H}$.  $\delta$-soundness requires

$$
\Pr(\text{miss } \hat\ell) \;=\; 1-\frac{N}{A^{H}} \;\le\;\delta
\quad\Longrightarrow\quad
N\;\ge\;(1-\delta)A^{H}.
$$

Thus the **expected** number of leaf evaluations (and hence simulator calls, because determinism makes one query per edge) is at least

$$
\mathbb E[N] \;\ge\; (1-\delta)A^{H}\;=\;\Omega(A^{H}).  \tag{11.1}
$$

Because $\delta<\gamma/(1-\gamma)<1$ is a constant, the hidden constant in Î©Â (11.1) does not depend on $H$.

\###â€¯11.6â€ƒLower Bound Statement

> **TheoremÂ 11.1 (Deterministicâ€‘case lower bound).**
> Fix $0<\gamma<1$ and accuracy $\varepsilon=\gamma^{H}/2$.
> Any $\delta$-sound online planner with $\delta<\gamma/(1-\gamma)$ requires
>
> $$
> Q_{\min}(H)\;=\;\Omega(A^{H})
> $$
>
> simulator queries **per call** in the worst case.

*Proof.*  The hardâ€‘instance family above is chosen with equal probability over leaves. For any planner issuing fewer than $(1-\delta)A^{H}$ rootâ€‘toâ€‘leaf simulations (11.1) its failure probability exceeds $\delta$, contradicting soundness. âˆ

\###â€¯11.7â€ƒExtension to Stochastic Environments

If we make the transitions stochastic but keep rewards deterministic (only $\hat\ell$ paysâ€¯1) the planner must both **find** the rewarding leaf *and* **estimate** its value by Monteâ€‘Carlo sampling.  Reâ€‘using the sparseâ€‘sampling analysis (Â§10) one shows:

* to keep the rollout estimate within $\pm \varepsilon/2$ with probabilityÂ $1-\delta$ the planner needs $m=\Omega(1/\varepsilon^{2})$ samples **per edge**,
* the searchâ€‘width lower bound remains $A^{H}$.

Hence for general MDPs

$$
Q_{\min}(H,m)\;=\;\Omega\!\bigl((mA)^{H}\bigr).
$$

\###â€¯11.8â€ƒTightness with Upper Bounds

Combined with the Sparseâ€‘Sampling algorithm of Sectionâ€¯10 we now have matching bounds:

| Setting       | Upper bound (Â§10) | Lower bound (this section) | Gap  |
| ------------- | ----------------- | -------------------------- | ---- |
| Deterministic | $A^{H}$           | $\Omega(A^{H})$            | none |
| Stochastic    | $(mA)^{H}$        | $\Omega((mA)^{H})$         | none |

Therefore the exponential dependence on branching factor $A$ and horizon $H$ is **informationâ€‘theoretically unavoidable**; improvements are only possible through *instanceâ€‘specific* structure or stronger access models.

\###â€¯11.9â€ƒSingleâ€‘Sentence Takeaway
A â€œneedleâ€‘inâ€‘aâ€‘haystackâ€ $A$-ary tree with one rewarding leaf forces any highâ€‘confidence online planner to explore **Î˜â€¯($A^{H}$)** rootâ€‘toâ€‘leaf paths, establishing that the exponential runtime achieved by sparse sampling is worstâ€‘case optimal.&#x20;


---

\##â€¯12â€¯â€¯Localâ€¯vsâ€¯Online Access: Tradeâ€‘offs and Open Gaps

\###â€¯12.1â€ƒRecap of the Two Simulator Modes

| Access mode     | Query privilege                                                                                                | Typical implementation                                                                           | Examples in notes                                                      |
| --------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **Local**       | May *reset* the simulator to **any** previously generated state within the current planning call.              | â€œCheckpoint / restoreâ€ API; planner keeps a search tree whose nodes are cached simulator states. | Recursive sparseâ€‘sampling code in Lectureâ€¯6, *p.â€¯1â€“3*                  |
| **Pure Online** | May reset **only to the root** $s_t$; cannot revisit deeper nodes without replaying the exact action sequence. | Realâ€‘time agents with no saveâ€‘state functionality (e.g., physical robots, Atari via OpenAI Gym). | Remarks on â€œonline accessâ€ in Lectureâ€¯5, *p.â€¯3* and Lectureâ€¯6, *p.â€¯15* |

Both modes still provide a stochastic zerothâ€‘order oracle (Â§5) but differ in *branching freedom*.

> **Takeâ€‘away:** Local access lets the planner branch freely inside the sampled subtree, whereas pure online forces it to explore new branches by replaying actions from the root each time.

---

\###â€¯12.2â€ƒUpperâ€‘Bound Transfer from Global to Local

The sparseâ€‘sampling planner (Â§9) uses `restore(state)` to revisit siblings when expanding a node. Under **local** access this call is cheap, so the perâ€‘call query complexity proven in Sectionâ€¯10 remains

$$
Q_{\text{local}}(H,m)=\bigl(mA\bigr)^{H}
$$

with the same sample size requirement

$$
m\;\ge\;\frac{18}{\varepsilon^{2}(1-\gamma)^{6}}\log\!\Bigl(\frac{12A^{H+1}}{(1-\gamma)^{2}\delta}\Bigr)Â , \tag{12.1}
$$

because the unionâ€‘bound analysis only counts nodes in the *generated* subtree â€” size â‰¤â€¯$(mA)^H$ â€” independent of $|S|$.  *See Lectureâ€¯6 proof sketch, p.â€¯6â€“9* .

---

\###â€¯12.3â€ƒLower Bounds Still Hold Locally

The deterministic â€œneedleâ€‘inâ€‘aâ€‘haystackâ€ instance (Â§11) uses only rootâ€‘level branching; thus, even with full restore capability the planner must query Î©($A^{H}$) leaves to be $\delta$-sound. Therefore

$$
\Omega\!\bigl(A^{H}\bigr) \;\le\; Q_{\text{local}}^{\star}(H)\;\le\; (mA)^{H},
$$

showing **no asymptotic gap** between global and local modes in the worst case. *Argument mirrors Lectureâ€¯5, p.â€¯4* .

---

\###â€¯12.4â€ƒPure Online Access: Additional Costs

Pure online planners cannot issue `restore(child)` for $child\neq s_t$. To evaluate a second branch at depth $d$ they must *replay* the first $d$ actions from the root, consuming extra simulator calls. Two consequences:

1. **Redundant rollâ€‘outs:** exploring $B$ siblings at depth $d$ costs $O(dB)$ queries instead of $O(B)$.
2. **Accumulated trajectory length:** total query count may reach $O\!\bigl((mA)^{H+1}\bigr)$ in the naÃ¯ve replay strategy.

> *Illustration:* Figure 3 in Lectureâ€¯6 (*p.â€¯2*) shows the replay overhead when only root resets are allowed .

#### 12.4.1â€ƒKnown Upper Bound with Rootâ€‘Only Resets

Using *iterative deepening* (expand depth 1, execute best action, then reâ€‘plan) yields a bound

$$
Q_{\text{online}}(H,m)\;\le\;(H+1)(mA)^{H},
$$

since each levelâ€‘$k$ subtree must be rebuilt after the real action is executed. This is at most a **factor $H$ loss** vs. local access.

*Proof sketch:* rebuild depthâ€‘$H$ tree once, execute $a_0$, then with new root depth becomes $H-1$, etc. Total simulator calls sum to $\sum_{k=0}^{H}(mA)^{H-k}\le (H+1)(mA)^{H}$.

#### 12.4.2â€ƒLowerâ€‘Bound Status

No matching lower bound is known. Whether any $\delta$-sound planner can avoid the linearâ€‘inâ€‘$H$ overhead without reset remains **open**; Lectureâ€¯6, *Notes section p.â€¯16* explicitly poses this as future work .

---

\###â€¯12.5â€ƒHierarchy of Practical Difficulty

$$
\text{Global} \;\xrightarrow{\;\text{no table}\;}\; \text{Local} \;\xrightarrow{\;\text{no checkpoint}\;}\; \text{PureÂ Online}.
$$

* Each restriction never *reduces* query complexity.
* Globalâ€¯=â€¯Local in worstâ€‘case asymptotics; gap between Local and Online currently bounded by $O(H)$ factor but could be tighter.

---

\###â€¯12.6â€ƒImplications for Algorithm Design

* **Robotics / real games:** implement cheap *state cloning* (e.g., emulator saveâ€‘states) to upgrade from pure online to local and regain exponential savings.
* **Memoryâ€‘bounded agents:** use *transposition tables* to memoize subâ€‘trees that reappear in future calls, partially emulating local access.
* **Adaptive rollâ€‘outs (MCTS):** progressive widening selectively invests simulator budget along the *actual* trajectory, mitigating replay costs without full restore (Sectionâ€¯17).

---

\###â€¯12.7â€ƒSingleâ€‘Sentence Takeaway

Local access preserves the $(mA)^{H}$ query bound of sparseâ€‘sampling, matching global lower bounds, whereas pure online access may incur an additional $O(H)$ replay overheadâ€”a gap whose tightness is an open theoretical problem highlighted in Lectureâ€¯6.


---

\##â€¯13â€¯â€¯SamplingÂ & AveragingÂ â€” Why Monteâ€‘Carlo Sidesteps |S|

\###â€¯13.1â€ƒUnbiased Oneâ€‘Step Estimates

Given a simulator oracle `Sim` (Â§3) the planner can **replace every expectation**

$$
P_a(s)^{\!\top}\!v \;=\;\mathbb E_{s'\!\sim P_a(s)}[\,v(s')\,]
$$

by an unbiased empirical mean

$$
\hat\mu_{m}(s,a)\;=\;\frac1m\sum_{i=1}^{m}v\bigl(s'_i\bigr),\qquad  
s'_i\;\overset{\text{i.i.d.}}\sim\;P_a(s).
$$

Because samples are drawn directly from the true kernel,

$$
\mathbb E[\hat\mu_{m}(s,a)] \;=\; P_a(s)^{\!\top}\!v,
$$

so estimator bias is **zero regardless of the stateâ€‘space size**.
The observation that â€œsampling allows one to approximate expected values, *where the error
of approximation is independent of the cardinality ofÂ S*â€ appears at the top of *Lectureâ€¯6* pageÂ 1.&#x20;

> **Key message:** Monteâ€‘Carlo turns a |S|â€‘dimensional integral into m scalar draws.

---

\###â€¯13.2â€ƒVariance and Concentration

With bounded value range $[0,Â V_{\max}]$ the variance of each sample is â‰¤â€¯$V_{\max}^2/4$.
Hoeffdingâ€™s inequality applied to the bounded i.i.d. sequence $\{v(s'_i)\}_{i=1}^{m}$ yields

$$
\Pr\!\bigl[\;|\hat\mu_{m}(s,a)-P_a(s)^{\!\top}\!v|>\varepsilon\bigr]
\;\le\;2\exp\!\Bigl(-\tfrac{2m\varepsilon^{2}}{V_{\max}^{2}}\Bigr). \tag{13.1}
$$

(*Lectureâ€¯6*, Lemma â€œHoeffdingâ€™s inequalityâ€, pageÂ 1; see also Concentration notesÂ Â§1.)&#x20;

Thus the **meanâ€‘squared error shrinks like $1/m$**, exactly as in classical Monteâ€‘Carlo, again
*independent of |S|*.

---

\###â€¯13.3â€ƒPropagating Estimates Through a Depthâ€‘H Tree

For stochastic sparse sampling (Â§9) the planner plugs $\hat\mu_{m}$ into each Bellman backup.
Error accumulates along a branch but contracts by a factorÂ $\gamma$ each level:

$$
\text{levelÂ }k:\quad
\Delta_k \le \gamma\,\Delta_{k-1}+\varepsilon_k,
\qquad
\varepsilon_k = V_{\max}\sqrt{\tfrac{\log(2/\zeta_k)}{2m}}.
$$

Choosing identical confidences $\zeta_k=\delta/A^{H}m^{H}$ and solving this recursion gives the
global bound used in Sectionâ€¯16.

> **Union bound trick:** only $(mA)^H$ nodes exist in the generated subtree,
> so controlling **that** many eventsâ€”*not* |S|â€”keeps logâ€‘terms manageable.
> Detailed derivation: *Lectureâ€¯6* pagesÂ 5â€‘9 around Eq.â€¯(4).&#x20;

---

\###â€¯13.4â€ƒCLT and Subâ€‘Gaussian Insight

For fixed horizonÂ H the depthâ€‘k return is a sum of at mostÂ $H$ bounded variables, hence
subâ€‘Gaussian with parameter $ \sigma^{2}\le V_{\max}^{2}/(1-\gamma)^{2}$.
Consequently, as $m\to\infty$

$$
\sqrt{m}\,\bigl(\hat\mu_{m}-P_a(s)^{\!\top}\!v\bigr)\;\xrightarrow{\,d\,}\;\mathcal N\bigl(0,\sigma^{2}\bigr),
$$

establishing the $1/\sqrt m$ rate and justifying Hoeffdingâ€™s tail in (13.1).

---

\###â€¯13.5â€ƒPractical Implications

| Quantity                  | Depends on                                                                  | **DoesÂ *not*** depend on |   |   |
| ------------------------- | --------------------------------------------------------------------------- | ------------------------ | - | - |
| Perâ€‘edge sample countâ€¯$m$ | accuracyâ€¯$\varepsilon$, confidenceâ€¯$\delta$, horizonâ€¯$H$, discountâ€¯$\gamma$ | (                        | S | ) |
| Variance of $\hat\mu_{m}$ | reward range, $m$                                                           | (                        | S | ) |
| Monteâ€‘Carlo runtime       | $m$, $A$, $H$                                                               | (                        | S | ) |

Thus planners can tackle *continuous or astronomically large* state spaces provided they can
simulate transitions.

---

\###â€¯13.6â€ƒSingleâ€‘Sentence Takeaway

Monteâ€‘Carlo rollâ€‘outs give **unbiased, $1/\sqrt m$-accurate** estimates whose error and sample
requirements hinge only on horizon and reward rangeâ€”*never* on the number of statesâ€”so
sparseâ€‘sampling achieves $|S|$-free planning by turning expectations into averages and taming
their noise with Hoeffding and union bounds.&#x20;

---

#### Progressâ€¯ğŸ”„

* Formalised estimator $\hat\mu_{m}$ and proved independence from $|S|$.
* Derived error bound (13.1) and contraction recursion for depthâ€‘H trees, citing *Lectureâ€¯6* and concentration notes.
* Summarised variance intuition via CLT and presented a practical dependency table.

**Next section to flesh out:** **14â€¯â€¯Concentration tools for error control** (detailed inequalities, union bound variants, CLT notes).


---

## 14â€¯â€¯Concentration Tools for Error Control

Modern onlineâ€‘planning analyses hinge on **turning Monteâ€‘Carlo noise into highâ€‘probability guarantees**.
This section states the main inequalities, sketches why they work, and shows how they slot into the depthâ€‘$H$ sparseâ€‘sampling proof.

### 14.1â€ƒHoeffdingâ€™s InequalityÂ â€“Â Single Estimate

> **TheoremÂ (Hoeffding, 1963).**Â Let $X_1,\dots ,X_m$ be independent, each bounded in $[a_i,b_i]$.
> Then for any $t>0$
>
> $$
> \Pr\!\Bigl[\Bigl|\tfrac1m\sum_{i=1}^m X_i-\mathbb E[X_i]\Bigr|\ge t\Bigr]
> \;\le\;2\exp\!\Bigl(-\tfrac{2m^2 t^{2}}{\sum_{i=1}^m(b_i-a_i)^2}\Bigr).
> $$
>
> (If all variables lie in $[0,1]$ the exponent is $-2mt^{2}$.)&#x20;

*Proof sketch.*Â Apply Chernoffâ€™s bound to $e^{\lambda\sum X_i}$, then use the fact that $e^{\lambda X_i}\le \tfrac{b_i-X_i}{b_i-a_i}e^{\lambda a_i}+ \tfrac{X_i-a_i}{b_i-a_i}e^{\lambda b_i}$ (a convexity trick). Optimising over $\lambda$ gives the exponent.

**Planner application.**
For fixed $(s,a)$ the oneâ€‘step return $R+\gamma V(s')$ is bounded in $[0,1/(1-\gamma)]$.
With $m$ IID rollâ€‘outs we have

$$
\Pr\!\bigl[|\widehat{P_a V}-P_aV|>\eta\bigr]\le 2\exp\!\bigl(-2m\eta^{2}(1-\gamma)^{2}\bigr). :contentReference[oaicite:1]{index=1}
$$

### 14.2â€ƒUnion BoundÂ â€“Â From One Edge to the Whole Tree

> **Booleâ€™s inequality**: $\Pr(\cup_i E_i)\le\sum_i\Pr(E_i)$.

Lectureâ€¯6 illustrates this with a Vennâ€‘diagram (grey ellipse containing disjoint â€œerror bubblesâ€) *on pageâ€¯7*; summing bubble probabilities bounds the outer ellipse.&#x20;

For a depthâ€‘$H$ sparseâ€‘sampling tree there are at most $(mA)^H$ estimator nodes; applying Hoeffding then unionâ€‘bounding gives

$$
\Pr\![\text{any node error}>\eta]\;\le\;2(mA)^H\exp\!\bigl(-2m\eta^{2}(1-\gamma)^{2}\bigr).
$$

Setting the RHS to $\delta$ yields the sample size condition (Eq.â€¯7.5).

### 14.3â€ƒSubâ€‘Gaussian Variables and the CLT Intuition

A zeroâ€‘mean r.v. $X$ is **$\sigma$-subâ€‘Gaussian** if
$\mathbb E[e^{\lambda X}]\le \exp(\lambda^{2}\sigma^{2}/2)$ for all $\lambda$.
Bounded variables with rangeâ€¯1 are $1/2$-subâ€‘Gaussian; sums of subâ€‘Gaussians remain subâ€‘Gaussian with variance scaling additively.

Hence the Monteâ€‘Carlo average satisfies

$$
\sqrt m\,(\widehat\mu-\mu)\;\xrightarrow{d}\;\mathcal N(0,\sigma^{2}),\qquad
\operatorname{Var}(\widehat\mu)=\sigma^{2}/m,
$$

the **Centralâ€‘Limitâ€‘Theorem** explanation behind the $1/\sqrt m$ rate.
Sectionâ€¯7 of the *Concentrationâ€‘ofâ€‘Measure notes* (pagesâ€¯97â€“102) presents the full derivation.&#x20;

### 14.4â€ƒPutting the Pieces Together for Depthâ€‘$H$ Trees

Let $\varepsilon_{\text{stat}}$ be the perâ€‘edge error budget, choose

$$
\varepsilon_{\text{stat}}=\frac{\varepsilon}{3},\qquad
m\;\ge\;\frac{18}{\varepsilon^{2}(1-\gamma)^{6}}
      \log\!\Bigl(\frac{12A^{H+1}}{(1-\gamma)^{2}\delta}\Bigr)\;(\text{Eq.\,7.5}).
$$

* Why the constants?

  * $2$ â€“ from the twoâ€‘sided Hoeffding probability.
  * $A^{H+1}$ â€“ union over all interior nodes (depth $0\!\ldots\!H$).
  * $(1-\gamma)^{-6}$ â€“ three contractions along each of at most $H$ levels.

FigureÂ 2 of Lectureâ€¯6 shows empirically how the empirical error (blue) matches the theoretical bound (orange) as $m$ grows.&#x20;

### 14.5â€ƒExample:Â Binaryâ€‘Bandit Planner

Consider horizonâ€¯1, $A=2$, rewards in $[0,1]$.
Hoeffding says $m\ge\frac{\log(4/\delta)}{2\varepsilon^{2}}$. Union bound is trivial (two arms).
Depth increases the logarithmic term to $\log(8/\delta)$ and multipliesâ€¯$A^H$.
Thus Sectionâ€¯11â€™s lower bound $\Omega(A^{H})$ is tight up to these logs.

### 14.6â€ƒTakeâ€‘Away

1. **Hoeffding** converts each Monteâ€‘Carlo average into a tail bound that shrinks like $e^{-m\eta^{2}}$.
2. **Union bound** blows the failure probability up by $(mA)^H$ but keeps only a *logarithmic* factor in $|S|$.
3. Subâ€‘Gaussian/CLT intuition explains the $1/\sqrt m$ rate that underlies the sampleâ€‘size formula.
4. Combining these gives the explicit $m$ of Eq.â€¯7.5, closing the statistical loop in the plannerâ€™s soundness proof.&#x20;

---


It looks like one or more of the PDFs you uploaded earlier are no longer available in the session storage.â€¯If you still need me to cite specific page numbers or pull additional details from those files, please reâ€‘upload them and Iâ€™ll fold the fresh citations in right away.

Below I continue with **Sectionâ€¯15â€¯â€¯Policyâ€‘Error Analysis** as planned, using the material already summarised from the lecture notes.

---

\##â€¯15â€¯â€¯Policyâ€‘Error Analysis

\###â€¯15.1â€ƒFrom Actionâ€‘Value Error to Policy Value Error

Recall (Lemmaâ€¯7.2) that if an approximate $Q$-function $\hat q$ satisfies

$$
\|\hat q - q^{\!*}\|_{\infty}\;\le\;\varepsilon,
\tag{15.1}
$$

then the policy $\pi_{\hat q}$ that is greedy with respect to $\hat q$ is **$2\varepsilon$-optimising**:

$$
q^{\!*}(s,\pi_{\hat q}(s))\;\ge\;v^{\!*}(s)-2\varepsilon
\quad\forall s.
\tag{15.2}
$$

Because the Bellman operator is a $\gamma$-contraction, the gap in (15.2) inflates by at most the geometric factor $(1-\gamma)^{-1}$ when propagated over an infinite horizon:

> **Lemmaâ€¯15.1 (Greedyâ€‘Gap Value Bound).**
> If (15.2) holds, then
>
> $$
> v^{\!*}(s)-v^{\pi_{\hat q}}(s)
> \;\le\;
> \frac{2\varepsilon}{1-\gamma}
> \qquad\forall s.
> \tag{15.3}
> $$

*Proof sketch.*Â Subtract the Bellman equations for $v^{\!*}$ and $v^{\pi_{\hat q}}$; bound the immediate reward difference by $2\varepsilon$ and contract the future value term by $\gamma$.

---

\###â€¯15.2â€ƒâ€œAlmostâ€‘Îµâ€ Optimising Policies

Sparseâ€‘sampling introduces randomness, so (15.2) may fail on a small set of states.
Define an **$(\varepsilon,\zeta)$-optimising policy**:

$$
\Pr_{s\sim\rho}\!\Bigl[
q^{\!*}\!\bigl(s,\pi(s)\bigr) 
< v^{\!*}(s)-\varepsilon
\Bigr]\;\le\;\zeta
\tag{15.4}
$$

for some distributionÂ $\rho$ over states (e.g., the onâ€‘policy state visitation distribution).

> **Lemmaâ€¯15.2 (Value Loss for Almostâ€‘Îµ Policies).**
> Under (15.4) the value gap is bounded by
>
> $$
> v^{\!*}(s_0)-v^{\pi}(s_0)
> \;\le\;
> \frac{\varepsilon + 2\zeta}{1-\gamma},
> \tag{15.5}
> $$
>
> assuming $\rho$ is the discounted occupancy measure of $\pi$ started at $s_0$.

*Idea.*Â Decompose the expectation into â€œgoodâ€ and â€œbadâ€ states; the good set contributes at most $\varepsilon/(1-\gamma)$ via Lemmaâ€¯15.1, the bad set contributes at most $2\zeta/(1-\gamma)$ because the probability of landing there is â‰¤â€¯$\zeta$.

---

\###â€¯15.3â€ƒPlugâ€‘In Bounds for Sparseâ€‘Sampling

Sectionâ€¯14 guaranteed

$$
\Pr\bigl[\|\hat q-q^{\!*}\|_{\infty}>\varepsilon/3\bigr]\;\le\;\delta.
$$

Conditioning on the complement event yields an $(2\varepsilon/3,\,\delta)$-optimising policy.
Applying Lemmaâ€¯15.2 with $\zeta=\delta$ and $\varepsilon\leftarrow 2\varepsilon/3$ gives

$$
v^{\!*}-v^{\pi_{\text{SS}}}\;\le\;
\frac{\varepsilon}{1-\gamma},
$$

so the sparseâ€‘sampling planner is indeed **$(\varepsilon,\delta)$-sound** (Theoremâ€¯10.2).

---

\###â€¯15.4â€ƒTightness of the $1/(1-\gamma)$ Factor

The linear factor is unavoidable: consider a twoâ€‘state MDP with selfâ€‘loops and oneâ€‘step reward gap $\varepsilon$; to accumulate that gap forever yields value loss $\varepsilon/(1-\gamma)$.  Hence contraction analysis is tight.

---

\###â€¯15.5â€ƒPractical Diagnostics

| Symptom                                        | Likely cause                                           | Fix                                                |
| ---------------------------------------------- | ------------------------------------------------------ | -------------------------------------------------- |
| High empirical value loss despite many samples | Horizon $H$ too small â€” truncation dominates           | Increase $H$ or use discountâ€‘aware reward shaping  |
| Large variance across runs                     | $\delta$ set too low â†’ insufficient $m$                | Raise $m$ via Eq.â€¯14.4 or tolerate higher $\delta$ |
| Bias despite meeting Hoeffding count           | Deterministic model misspecified â†’ rewards not bounded | Rescale rewards or use Bernstein inequality        |

---

\###â€¯15.6â€ƒSingleâ€‘Sentence Takeaway

Bounding the uniform error of the approximate $Q$-function translates, via greedyâ€‘gap and contraction lemmas, into a **tight $\varepsilon/(1-\gamma)$ valueâ€‘loss guarantee**, while allowing a small perâ€‘call failure probability $\delta$ only adds an additive $2\delta/(1-\gamma)$ penalty that sparseâ€‘sampling controls by picking $m$ from Hoeffdingâ€‘union analysis.

---

\###â€¯Next Steps

With statistical error now fully tied to policy quality, Sectionâ€¯16 will integrate horizon truncation, sampling variance, and greedy loss into the **complete parameter recipe** for a Î´â€‘sound planner.


---

\##â€¯16â€¯â€¯Putting Everything TogetherÂ â€”â€¯The Complete Parameter Recipe

We now synthesise the three error sources examined so farâ€”**tail truncation**, **Monteâ€‘Carlo noise**, and **greedy gap**â€”to give a single, explicit parameter schedule $(H,m)$ that makes the sparseâ€‘sampling online planner $(\varepsilon,\delta)$-sound while keeping its runtime $|S|$-independent.

\###â€¯16.1â€ƒError Budget Decomposition

Allocate the total tolerance $\varepsilon$ evenly:

| Symbol                        | Purpose                                         | Allowable magnitude |
| ----------------------------- | ----------------------------------------------- | ------------------- |
| $\varepsilon_{\text{tail}}$   | Missing value beyond depthÂ $H$                  | $\varepsilon/3$     |
| $\varepsilon_{\text{stat}}$   | Perâ€‘edge Monteâ€‘Carlo bias in Qâ€‘estimates        | $\varepsilon/3$     |
| $\varepsilon_{\text{greedy}}$ | Gap between greedy $q$ and optimal action value | $\varepsilon/3$     |

By contraction (Lemmaâ€¯7.1) a greedyâ€‘gap of $\varepsilon_{\text{greedy}}$ inflates to at most
$\varepsilon_{\text{greedy}}/(1-\gamma)$ in value, but choosing the same bound for every slice keeps algebra clean and still yields the headline $\varepsilon$ guarantee.

---

\###â€¯16.2â€ƒChoose Horizon $H$ forÂ Tailâ€¯Error

Tail error satisfies

$$
\varepsilon_{\text{tail}}
=
\frac{\gamma^{H}}{1-\gamma}
\;\le\;\frac{\varepsilon}{3}
\quad\Longrightarrow\quad
H
=
H_{\gamma,\varepsilon}
=
\Bigl\lceil
\frac{\log\!\bigl(\varepsilon(1-\gamma)/3\bigr)}{\log\gamma}
\Bigr\rceil.
\tag{16.1}
$$

This is identical (up to the constantâ€¯3) to Eq.â€¯9.2.â€‚Note $H=\Theta\!\bigl(\log_{1/\gamma}(1/\varepsilon)\bigr)$.

---

\###â€¯16.3â€ƒChoose SamplesÂ $m$ for Statistical Error

From Sectionâ€¯14, to make every Monteâ€‘Carlo estimate within $\varepsilon_{\text{stat}}$ uniformly over the generated subtree with failure probability â‰¤â€¯$\delta$ it suffices that

$$
 m
 \;\ge\;
 \frac{18}{\varepsilon^{2} (1-\gamma)^{6}}
 \log\!\Bigl(
   \frac{12\,A^{H+1}}{(1-\gamma)^{2}\,\delta}
 \Bigr).
 \tag{16.2}
$$

*Derivation recap*:

1. Plug range $R = 1/(1-\gamma)$ into Hoeffding (Sectionâ€¯14.1).
2. Unionâ€‘bound over at most $(mA)^H$ nodes (Sectionâ€¯14.2).
3. Solve for $m$ with $\eta=\varepsilon/3$.

Equationâ€¯(16.2) exactly matches the earlier sampleâ€‘size formula (Eq.â€¯7.5).

---

\###â€¯16.4â€ƒGreedy Gap Automatically Controlled

With (16.2) in place we obtain $\|\hat q-q^{*}\|_{\infty}\le \varepsilon/3$ w\.p.\ $1-\delta$.
Lemmaâ€¯7.2 then states **greedy w\.r.t. $\hat q$ is $2\varepsilon/3$-optimising**, fitting the third error slice ($\varepsilon_{\text{greedy}}=\varepsilon/3$).

No extra tuning is needed; the greedy gap is automatically covered.

---

\###â€¯16.5â€ƒSoundness Theorem

> **Theoremâ€¯16.1 (Complete Sparseâ€‘Sampling Planner).**
> With horizon $H$ as inâ€¯(16.1) and sample size $m$ as inâ€¯(16.2),
> the depthâ€‘$H$ sparseâ€‘sampling algorithm of Sectionâ€¯9 is $(\varepsilon,\delta)$-sound under local (hence global) simulator access.
>
> **Query complexity per call:**
>
> $$
> Q_{\text{SS}}(A,\varepsilon,\delta,\gamma)
> \;=\;
> (mA)^{H}
> \;=\;
> \bigl[
> A\cdot
> \tfrac{18}{\varepsilon^{2}(1-\gamma)^{6}}
> \log\!\bigl(\tfrac{12A^{H+1}}{(1-\gamma)^{2}\delta}\bigr)
> \bigr]^{\,H}.
> $$
>
> **Arithmetic complexity:**Â identical up to a small constant factor.
> **Stateâ€‘space dependence:**Â noneâ€”$Q_{\text{SS}}$ is independent of $|S|$.

*Proof.*Â Combine:

1. Tail bound â‰¤â€¯$\varepsilon/3$ by (16.1).
2. Statistical bound â‰¤â€¯$\varepsilon/3$ w\.p.\ $1-\delta$ by (16.2).
3. Greedy gap â‰¤â€¯$\varepsilon/3$ on the same event (Lemmaâ€¯7.2).
   Â Â Â Total value loss â‰¤â€¯$\varepsilon$.
4. Complexity is the node count of the sparse tree.

âˆ

---

\###â€¯16.6â€ƒRuntime at a Glance

Insert (16.1) into (16.2) to get an **asymptotic envelope** (suppress logs):

$$
Q_{\text{SS}}
=
\Bigl(
 \tfrac{A}{\varepsilon^{2}(1-\gamma)^{6}}
\Bigr)^{\,\Theta\!\bigl(\log_{1/\gamma}(1/\varepsilon)\bigr)}.
$$

Although daunting, this cost can be tractable for moderate $\gamma$ and small action sets; in practice adaptive sampling (UCT) trims large constants (Sectionâ€¯17).

---

\###â€¯16.7â€ƒChecklist for Practitioners

1. **Pick desired value error**Â $\varepsilon$Â and confidenceÂ $\delta$.
2. Compute **horizon**Â $H$ viaÂ (16.1).
3. Compute **sample count**Â $m$ viaÂ (16.2).
4. Ensure **time budget** can accommodate $Q=(mA)^{H}$ simulator calls.
5. Implement **local access** (state checkpoints) if at all possible to avoid replay overhead (Â§12).

---

\###â€¯16.8â€ƒSingleâ€‘Sentence Takeaway

Choosing horizon $H=\lceil\log_{\gamma}(\varepsilon(1-\gamma)/3)\rceil$ and sample size $m\propto\varepsilon^{-2}(1-\gamma)^{-6}\log(A^{H+1}/\delta)$ makes sparseâ€‘sampling **provably $(\varepsilon,\delta)$-sound** with perâ€‘decision cost $(mA)^{H}$â€”an exponential in planning depth but *utterly independent* of the stateâ€‘space size, thereby realising the longâ€‘sought â€œpipeâ€‘dreamâ€ runtime for online planning.&#x20;

---

#### Progressâ€¯ğŸ”„

* Unified the three error contributors into a single parameter recipe.
* Stated and proved Theoremâ€¯16.1, providing the final complexity expression.
* Supplied a practitioner checklist and asymptotic intuition.

**Next (final content) section:** **17â€¯â€¯Open questions & extensions**.


---

\##â€¯17â€¯â€¯Open Questions & Extensions

Despite the tight worstâ€‘case characterisation delivered in Sectionsâ€¯10â€“16, **online planning for MDPs** still harbours several important theoretical and practical gaps.

| Theme                                   | StatusÂ (2025)          | What is known                                                                                                     | Key open problems                                                                                               | Promising ideas                                                                                                  |                    |                                                                                              |
| --------------------------------------- | ---------------------- | ----------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------ | -------------------------------------------------------------------------------------------- |
| **Pureâ€‘online access without reset**    | *Partially resolved*   | Best upper bound: $O\!\bigl((H+1)(mA)^{H}\bigr)$ via iterative deepening; lower bound remains $\Omega((mA)^{H})$. | Close or separate the $O(H)$ replay overhead. Can resets be simulated with polynomial memory?                   | Hashâ€‘based transposition tables; incremental tree stitching; informationâ€‘theoretic replay arguments.             |                    |                                                                                              |
| **Adaptive sampling (MCTS / UCT)**      | *Empirically dominant* | UCT achieves logarithmic â€œregretâ€ in i.i.d. bandit trees; polynomial sample savings on *benign* instances.        | Worstâ€‘case upper bounds remain $(mA)^{H}$. Can adaptive rollâ€‘outs beat the matching lower bound for *all* MDPs? | Informationâ€‘directed sampling; progressive widening with admissible heuristics; instanceâ€‘dependent lower bounds. |                    |                                                                                              |
| **Continuous action spaces**            | *Active area*          | Sparseâ€‘sampling generalises via discretisation; complexity explodes with covering number $N_{\eta}(A)$.           | Remove exponential dependence on covering size while retaining (                                                | S                                                                                                                | )-free guarantees. | Gradientâ€‘free optimisation in bandit simplex; Lipschitzâ€‘continuous action value assumptions. |
| **Model bias / imperfect simulators**   | *Explored in MBâ€‘RL*    | Empirical model learning + Dyna expands planner scope; error bounds require simulationâ€‘estimation tradeâ€‘off.      | Tight online guarantees when simulator error scales with visited states (nonâ€‘IID).                              | Robust MDPs, Wasserstein ambiguity sets; bootstrap world models with errorâ€‘aware planning.                       |                    |                                                                                              |
| **Discountâ€‘free finiteâ€‘horizon tasks**  | *Well understood*      | Replace $\gamma$â€‘tail with fixed horizon $H$.                                                                     | Adaptive algorithms that handle unknown horizon without worstâ€‘case overestimation.                              | Discount scheduling, anytime dynamic programming.                                                                |                    |                                                                                              |
| **Multiâ€‘agent & partial observability** | *Early explorations*   | Sparseâ€‘sampling extends to DECâ€‘POMDPs but with doubleâ€‘exponential blowâ€‘up.                                        | Stateâ€‘spaceâ€‘free complexity bounds in POMDPs; equilibrium computation under online constraints.                 | Belief compression; pointâ€‘based value iteration in rollout form.                                                 |                    |                                                                                              |
| **Hardware acceleration**               | *Emerging*             | GPU rayâ€‘casting simulators let $m$ reach $10^{4}$ per 16â€¯ms in robotics.                                          | Formal complexity models that incorporate parallel query batching.                                              | Blockâ€‘synchronous sampling; GPUâ€‘aware tree layouts.                                                              |                    |                                                                                              |

\###â€¯17.1â€ƒCase Study: Adaptive Sampling Outperforms Sparseâ€‘Sampling on Game Trees

Empirical work on *Go,* *Chess,* and *Atari* shows Monteâ€‘Carlo Tree Search (MCTS) can reach competitive play with budgets orders of magnitude smaller than $(mA)^{H}$.
The gap is explained by **instanceâ€‘dependent error**: real game trees contain large regions of nearâ€‘determinism and sparse rewards, letting UCT focus rollâ€‘outs on critical lines.  However, no generalisationâ€‘independent lower bound yet matches these savings, leaving the worstâ€‘case theory intact.

\###â€¯17.2â€ƒOnlineÂ Planners in Modelâ€‘Based RL Pipelines

In modern reinforcementâ€‘learning agents (e.g., MuZero, Dreamerâ€‘V4) the exact simulator is replaced by a *learned latentâ€‘dynamics model*.
Online planners act **inside** the learning loop, compounding model estimation error with sparseâ€‘sampling error.
A principled theory would unify:

$$
\text{model bias} \;+\; \text{Monteâ€‘Carlo variance} \;+\; \text{greedy gap}\Longrightarrow\text{policy regret}.
$$

Current bounds remain loose (often $O(1/\sqrt n)$) because model errors are nonâ€‘IID.

\###â€¯17.3â€ƒResearch Directions

1. **Replayâ€‘efficient trees.**Â Design planners that reuse subâ€‘trees across consecutive calls in pureâ€‘online settings to approach localâ€‘access cost without full checkpoints.
2. **Bernsteinâ€‘style concentration.**Â Replace Hoeffding with varianceâ€‘adaptive bounds to shrink the $(1-\gamma)^{-6}$ factor in (16.2).
3. **Instanceâ€‘optimal lower bounds.**Â Characterise classes of MDPs (e.g., deterministic rewards, low â€œbranch entropyâ€) where online planning can provably beat $A^{H}$.
4. **Parallel hardware models.**Â Update queryâ€‘complexity theory to account for batch simulators where many calls run in lockâ€‘step.
5. **Hierarchical action spaces.**Â Use option or skill abstractions to reduce effective branching factor $A$ while maintaining $\delta$-soundness.

---

\###â€¯17.4â€ƒSingleâ€‘Sentence Takeaway

While sparseâ€‘sampling settles the worstâ€‘case complexity of online planning, advancing practice demands **closing replay overhead gaps, exploiting adaptive sampling, and integrating modelâ€‘learning errors**â€”fertile territory where theoretical guarantees still lag behind empirical breakthroughs.&#x20;

---

**End of Sectionâ€¯17 and of the requested exposition.**

---
