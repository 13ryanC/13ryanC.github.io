A Technical Note on Finite Discounted Markov Decision ProcessesTable of NotationSymbolDefinitionSFinite set of states, $AFinite set of actions, $$P(s's,a)$r(s,a)Expected immediate reward for taking action a in state s.γDiscount factor, $\gamma \inState Space S: A finite set of states that the agent can occupy. The size of the state space is denoted by ∣S∣.Action Space A: A finite set of actions available to the agent. The size of the action space is denoted by ∣A∣.Transition Probability Kernel P: A function $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to $, where P(s′∣s,a) denotes the probability of transitioning to state s′∈S after taking action a∈A in state s∈S. For any state-action pair (s,a), the transition probabilities must sum to one: ∑s′∈S​P(s′∣s,a)=1.Reward Function r: A function r:S×A→R that specifies the expected immediate reward, r(s,a), received after taking action a in state s. We assume rewards are bounded, i.e., ∣r(s,a)∣≤Rmax​ for some constant Rmax​.Discount Factor γ: A scalar $\gamma \inP(St+1​=s′∣St​=s,At​=a,St−1​=st−1​,At−1​=at−1​,…)=P(St+1​=s′∣St​=s,At​=a)This assumption implies that the current state St​ is a sufficient statistic of the history. This is the foundational assumption that makes the entire framework computationally and analytically tractable. It allows the complex problem of planning over all possible histories to be reduced to a problem of planning from the current state. Without this property, the state space for a policy would need to encompass the set of all possible histories, which grows exponentially with time, rendering the problem intractable. The validity of the Markov property is therefore a critical consideration in applying MDPs to real-world problems; a significant part of practical reinforcement learning involves designing a state representation that is rich enough to satisfy this property.3Interaction and TrajectoriesThe agent-environment interaction unfolds over discrete time steps t=0,1,2,…. At each time step t, the agent observes the current state St​∈S, selects an action At​∈A, receives an immediate reward Rt+1​=r(St​,At​), and transitions to a new state St+1​∼P(⋅∣St​,At​). This interaction generates a sequence of states, actions, and rewards known as a trajectory or history: τ=(S0​,A0​,R1​,S1​,A1​,R2​,…).4Existence of a Probability MeasureA crucial, though often overlooked, technical point is the existence of a well-defined probability space over these infinite trajectories. Given an MDP, a starting state distribution μ, and a policy π (a rule for choosing actions), does a unique probability measure Pμπ​ over the space of trajectories exist that is consistent with the dynamics? The Ionescu-Tulcea theorem provides an affirmative answer, guaranteeing the existence and uniqueness of such a measure on the canonical measurable space of trajectories.2 This formalizes the notion of an agent following a policy within an MDP and ensures that expectations over trajectories are well-defined.2. Policies and Value FunctionsPoliciesA policy, denoted by π, is a rule that specifies how an agent selects actions. The space of policies can be categorized based on their dependence on history and their use of randomness.1History-Dependent Policy: A general policy πt​ at time t is a mapping from the history of past states and actions Ht​=(S0​,A0​,…,St−1​,At−1​,St​) to a probability distribution over actions, πt​:Ht​→Δ(A), where Δ(A) is the probability simplex over A.Stationary Policy: A policy is stationary if the action-selection rule does not change over time. It is a mapping π:S→Δ(A). These are also known as memoryless policies because they depend only on the current state.2 As will be shown, for infinite-horizon discounted MDPs, we can restrict our search for an optimal policy to this class without loss of optimality.Deterministic Policy: A deterministic policy is a special case where the mapping is to a single action rather than a distribution. For a stationary deterministic policy, π:S→A.Stochastic Policy: A policy is stochastic if it specifies a non-degenerate probability distribution over actions for at least one state.Value FunctionsThe objective of an agent is to maximize the expected cumulative discounted reward. This long-term notion of "goodness" is captured by value functions.5State-Value Function (Vπ): The state-value function for a policy π is the expected discounted return when starting in state s and subsequently following policy π.$$V^\pi(s) = \mathbb{E}_\pi \left$$Here, the expectation is taken over the distribution of trajectories induced by the policy π and the MDP dynamics, starting from state s.7Action-Value Function (Qπ): The action-value function (or Q-function) for a policy π is the expected discounted return after taking action a in state s and thereafter following policy π.$$Q^\pi(s,a) = \mathbb{E}_\pi \left$$The value function can be viewed as an embedding of a high-dimensional object (a policy, which specifies behavior across the entire state-action space) into a lower-dimensional Euclidean space, R∣S∣ for Vπ or R∣S∣×∣A∣ for Qπ. This is a many-to-one mapping, as different policies can yield the same value function. The study of this mapping and its image is the central theme of the geometric perspective on MDPs.8Relationship between V-functions and Q-functionsThe state-value and action-value functions are tightly related through the following fundamental identities 6:\begin{equation} \label{eq:V_Q_relation_1}V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s,a)\end{equation}\begin{equation} \label{eq:V_Q_relation_2}Q^\pi(s,a) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V^\pi(s')\end{equation}Equation \ref{eq:V_Q_relation_1} states that the value of a state is the expected value of its action-values, averaged over the policy's action probabilities. Equation \ref{eq:V_Q_relation_2} decomposes the action-value into the immediate reward and the discounted expected value of the next state. This relationship is the engine of policy improvement: if for some action a′ we find that Qπ(s,a′)>Vπ(s), it signals that the policy π is suboptimal at state s. A better policy can be constructed by increasing the probability of selecting action a′, a concept that forms the basis of the Policy Iteration algorithm.3. The Bellman Expectation EquationDerivationThe Bellman expectation equation provides a recursive characterization of the value function for a given policy π. It expresses a self-consistency condition: the value of a state under a policy must equal the expected immediate reward plus the discounted expected value of the next state. It can be derived by substituting Equation \ref{eq:V_Q_relation_2} into Equation \ref{eq:V_Q_relation_1}:\begin{equation} \label{eq:bellman_expectation}V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V^\pi(s') \right)\end{equation}This equation must hold for all states s∈S simultaneously, forming a system of ∣S∣ linear equations in the ∣S∣ unknown values Vπ(s).6Matrix-Vector FormFor a stationary policy π, the system of Bellman expectation equations can be written more compactly in matrix-vector form. Let Vπ be a column vector in R∣S∣ representing the value function. We define the policy-induced reward vector rπ∈R∣S∣ and transition matrix Pπ∈R∣S∣×∣S∣ as:(rπ)s​=∑a∈A​π(a∣s)r(s,a)(Pπ)ss′​=∑a∈A​π(a∣s)P(s′∣s,a)With this notation, the Bellman expectation equation becomes 4:\begin{equation} \label{eq:bellman_matrix}V^\pi = r^\pi + \gamma P^\pi V^\pi\end{equation}Direct SolutionEquation \ref{eq:bellman_matrix} is a system of linear equations. We can solve for Vπ by rearranging the terms:(I−γPπ)Vπ=rπwhere I is the identity matrix. The matrix (I−γPπ) is guaranteed to be invertible for $\gamma \in:\begin{equation} \label{eq:direct_solution}V^\pi = (I - \gamma P^\pi)^{-1} r^\pi\end{equation}This direct solution reveals a deep connection between policy evaluation and linear algebra. While conceptually simple, computing the matrix inverse is computationally expensive, with a complexity of O(∣S∣3). This cost motivates the use of iterative methods for large state spaces. More importantly, this analytical form is the key to understanding the geometry of the value function space. The mapping from a policy π to its value function Vπ is non-linear due to the matrix inversion of a term, (I−γPπ), that depends linearly on π. It is this complex but well-defined mapping that gives rise to the polytope structure of the value function space, as explored in Part III.8Part II: Optimality and Core Algorithms4. Bellman Operators on the Value Function SpaceTo analyze the convergence of algorithms that solve the Bellman equations, it is invaluable to adopt the language of operator theory. We define operators that act on the space of value functions, which we take to be the vector space R∣S∣ of all real-valued functions over the states.Bellman Policy OperatorFor a fixed stationary policy π, the Bellman policy operator (or Bellman expectation operator) Tπ:R∣S∣→R∣S∣ maps a value function V to a new value function (TπV) according to the one-step Bellman backup rule 9:\begin{equation} \label{eq:t_pi_operator}(T^\pi V)(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left\end{equation}In matrix-vector form, this is written as TπV=rπ+γPπV. The Bellman expectation equation, Vπ=rπ+γPπVπ, can now be expressed as a fixed-point equation: Vπ=Tπ(Vπ). This means the true value function Vπ is the unique fixed point of its corresponding Bellman operator Tπ.11Bellman Optimality OperatorThe Bellman optimality operator T:R∣S∣→R∣S∣ corresponds to the one-step backup for the optimal value function. It improves a value function V by acting greedily over all possible actions 2:\begin{equation} \label{eq:t_star_operator}(T V)(s) = \max_{a \in \mathcal{A}} \left\end{equation}Properties of Bellman OperatorsThese operators possess several crucial properties that form the basis of their analysis:Monotonicity: Both operators are monotone. If U,V∈R∣S∣ and U(s)≤V(s) for all s∈S (denoted U≤V), then (TπU)(s)≤(TπV)(s) and (TU)(s)≤(TV)(s) for all s. This follows from the non-negativity of probabilities.Constant Offset: Adding a constant c to a value function results in the addition of a discounted constant to the new value function. For any c∈R and the vector of ones 1:Tπ(V+c1)=TπV+γc1T(V+c1)=TV+γc1Linearity vs. Non-linearity: The Bellman policy operator Tπ is an affine transformation on R∣S∣. In contrast, the Bellman optimality operator T is non-linear due to the max operation.11This operator-theoretic perspective unifies the problems of policy evaluation and control into a common framework of finding fixed points.11 The non-linear operator T can be understood as the upper envelope of the affine operators for all deterministic policies: TV=maxπ∈Πdet​​TπV. This geometric view is fundamental. Each application of T is equivalent to performing a backup for every possible deterministic policy and then selecting the best outcome for each state. This is precisely the mechanism that drives an estimate of the value function towards the "outer boundary" of what is achievable—the boundary of the value function polytope—and ultimately to the optimal vertex V∗.5. The Contraction Mapping Principle in Metric SpacesThe convergence of the core dynamic programming algorithms is guaranteed by a powerful result from functional analysis: the Banach Fixed-Point Theorem, also known as the Contraction Mapping Principle.Metric Spaces and ContractionsMetric Space: A metric space is a pair (X,d), where X is a set and d:X×X→R is a metric (or distance function) that satisfies non-negativity, identity of indiscernibles, symmetry, and the triangle inequality. A metric space is complete if every Cauchy sequence of points in X has a limit that is also in X.13Contraction Mapping: A mapping f:X→X is a contraction if there exists a constant q∈[0,1), called the contraction factor, such that for all x,y∈X:d(f(x),f(y))≤q⋅d(x,y)Geometrically, a contraction mapping brings any two points in the space closer together.13Banach Fixed-Point TheoremThe theorem provides a powerful guarantee for such mappings.Theorem (Banach, 1922): Let (X,d) be a non-empty complete metric space and let f:X→X be a contraction mapping on X. Then f has a unique fixed point x∗∈X (i.e., f(x∗)=x∗). Furthermore, for any arbitrary starting point x0​∈X, the sequence defined by the iteration xk+1​=f(xk​) converges to the unique fixed point x∗.13Convergence RateThe convergence of the iterative process is geometric, with a rate determined by the contraction factor q. The distance to the fixed point is bounded as follows 15:d(xk​,x∗)≤1−qqk​d(x1​,x0​)This theorem is the fundamental reason why the core dynamic programming algorithms for discounted MDPs are guaranteed to work. The entire theoretical foundation of Value Iteration and Policy Evaluation rests on demonstrating that their respective Bellman operators are contractions on a complete metric space.14 The theorem not only proves convergence but also provides the algorithm itself (iterative application of the operator) and a bound on its speed. The theorem's assumptions—a contraction mapping (hence γ<1) and a complete metric space—delineate the classical theory. This explains why the analysis of undiscounted problems (γ=1) is more complex and why using function approximation (where the space of representable functions may not be complete or closed under the Bellman operator) can lead to instability and divergence.6. Convergence Guarantees for Policy EvaluationWe can now apply the Banach Fixed-Point Theorem to prove the convergence of iterative policy evaluation.The Space of Value Functions as a Banach SpaceThe space of all possible value functions is the set of real-valued functions on the state space, which can be represented as the vector space R∣S∣. We equip this space with the infinity norm (or max norm), defined as ∥V∥∞​=maxs∈S​∣V(s)∣. The distance between two value functions U and V is then given by the metric d(U,V)=∥U−V∥∞​. The space (R∣S∣,∥⋅∥∞​) is a complete normed vector space, also known as a Banach space, and therefore a complete metric space.14Proof of Contraction for TπThe key step is to show that the Bellman policy operator Tπ is a contraction mapping on this space.Theorem: For any stationary policy π and any two value functions U,V∈R∣S∣, the Bellman policy operator Tπ is a γ-contraction with respect to the infinity norm:∥TπU−TπV∥∞​≤γ∥U−V∥∞​Proof:Let U,V∈R∣S∣ be two arbitrary value functions. For any state s∈S, we consider the difference ∣(TπU)(s)−(TπV)(s)∣:\begin{align*}|(T^\pi U)(s) - (T^\pi V)(s)| &= \left| \left(r^\pi(s) + \gamma \sum_{s'} P^\pi(s,s') U(s')\right) - \left(r^\pi(s) + \gamma \sum_{s'} P^\pi(s,s') V(s')\right) \right| \&= \left| \gamma \sum_{s'} P^\pi(s,s') (U(s') - V(s')) \right| \&\le \gamma \sum_{s'} P^\pi(s,s') |U(s') - V(s')| \quad \text{(by Jensen's inequality)} \&\le \gamma \sum_{s'} P^\pi(s,s') \max_{z \in \mathcal{S}} |U(z) - V(z)| \quad \text{(by definition of max norm)} \&= \gamma |U - V|\infty \sum{s'} P^\pi(s,s') \&= \gamma |U - V|\infty \quad \text{(since } P^\pi \text{ is a stochastic matrix, its rows sum to 1)}\end{align*}Since this inequality holds for all s∈S, we can take the maximum over all states on the left-hand side, which yields the final result 2:$$ |T^\pi U - T^\pi V|\infty = \max_{s \in \mathcal{S}} |(T^\pi U)(s) - (T^\pi V)(s)| \le \gamma |U - V|_\infty $$Since $\gamma \inThe Bellman Optimality EquationThe Bellman Optimality Equation (BOE) defines a recursive relationship that must be satisfied by the optimal value function. It is derived from the Principle of Optimality, which states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.11This principle leads to the following equations for V∗ and Q∗:\begin{align}V^(s) &= \max_{a \in \mathcal{A}} \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V^(s') \right) \label{eq:boe_v} \Q^(s,a) &= r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) \max_{a' \in \mathcal{A}} Q^(s',a') \label{eq:boe_q}\end{align}The BOE for V∗ (Equation \ref{eq:boe_v}) states that the value of a state under an optimal policy must be equal to the return from taking the best possible action in that state. In operator notation, this is simply V∗=T(V∗).2The max operator in the BOE is the fundamental source of its non-linearity and is what gives rise to the piecewise-affine structure of the optimal value function. As noted previously, the Bellman optimality operator T is the upper envelope of the affine operators Tπ for all deterministic policies. Consequently, the optimal value function V∗ is not a smooth function but is instead piecewise-affine. The "pieces" correspond to regions in the value space where a particular deterministic policy is optimal, and the "kinks" or boundaries between these pieces are where the identity of the optimal action changes. This structure is precisely what defines the facets of the value function polytope, a concept central to the geometric interpretation of MDPs.8 Furthermore, the BOE can be relaxed into a set of linear inequalities, V(s)≥r(s,a)+γ∑s′​P(s′∣s,a)V(s′) for all (s,a), which forms the basis of the primal Linear Programming formulation for solving MDPs.198. Existence and Uniqueness of Optimal ValuesThe Bellman Optimality Equation defines what it means to be optimal, but it does not, on its own, guarantee that a solution exists or that it is unique. Once again, we turn to the Contraction Mapping Principle to establish this fundamental result.Proof of Contraction for the Bellman Optimality OperatorWe must show that the Bellman optimality operator T is a γ-contraction on the Banach space (R∣S∣,∥⋅∥∞​).Theorem: For any two value functions U,V∈R∣S∣, the Bellman optimality operator T is a γ-contraction with respect to the infinity norm:∥TU−TV∥∞​≤γ∥U−V∥∞​Proof:Let U,V∈R∣S∣. For an arbitrary state s, consider the difference (TU)(s)−(TV)(s).Let a∗ be an action that achieves the maximum for (TU)(s):(TU)(s)=r(s,a∗)+γs′∑​P(s′∣s,a∗)U(s′)By definition of the max operator, for any other action, including the one that is optimal for V, the value for U must be at least as large. Thus:\begin{align*}(T U)(s) - (T V)(s) &= \left( r(s,a^) + \gamma \sum_{s'} P(s'|s,a^) U(s') \right) - \max_{a} \left( r(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s') \right) \&\le \left( r(s,a^) + \gamma \sum_{s'} P(s'|s,a^) U(s') \right) - \left( r(s,a^) + \gamma \sum_{s'} P(s'|s,a^) V(s') \right) \&= \gamma \sum_{s'} P(s'|s,a^) (U(s') - V(s')) \&\le \gamma \sum_{s'} P(s'|s,a^) |U - V|\infty \&= \gamma |U - V|\infty\end{align*}By a symmetric argument, we can show that (TV)(s)−(TU)(s)≤γ∥U−V∥∞​. Combining these two inequalities, we have:∣(TU)(s)−(TV)(s)∣≤γ∥U−V∥∞​Since this holds for any state s, taking the maximum over all states gives the desired result 2:∥TU−TV∥∞​≤γ∥U−V∥∞​Conclusion on Existence and UniquenessSince the Bellman optimality operator T is a γ-contraction on the complete metric space (R∣S∣,∥⋅∥∞​), the Banach Fixed-Point Theorem guarantees that T has a unique fixed point.14 This unique fixed point is, by definition, the optimal value function V∗.This result is of paramount importance. It provides the theoretical foundation for all value-based reinforcement learning algorithms by guaranteeing that there is a single, well-defined target, V∗, for these algorithms to pursue. Without this proof, the entire endeavor of "finding the optimal value function" would be ill-defined, as we would not know if a solution exists or if there might be multiple, conflicting "optimal" value functions. The proof is also constructive: the iterative application of the operator, Vk+1​=T(Vk​), is not merely a component of the proof but is the very definition of the Value Iteration algorithm. The theorem thus elegantly proves that a solution exists, is unique, and provides the algorithm to find it.9. The Fundamental Theorem of Markov Decision ProcessesThe Fundamental Theorem of MDPs connects the concepts of optimal value functions and optimal policies, showing that one can be derived from the other. It solidifies the entire value-based approach to solving MDPs.Theorem StatementThe theorem consists of two main parts 2:Fixed-Point Property: The optimal value function V∗ is the unique fixed point of the Bellman optimality operator T.V∗=T(V∗)Optimality of Greedy Policies: Any policy π∗ that is greedy with respect to V∗ is an optimal policy. A policy is greedy with respect to V∗ if, for every state s, it selects an action that achieves the maximum in the Bellman optimality equation:π∗(s)∈arga∈Amax​(r(s,a)+γs′∈S∑​P(s′∣s,a)V∗(s′))For such a policy, it holds that Vπ∗=V∗.Proof SketchThe first part of the theorem was established in the previous section via the Banach Fixed-Point Theorem. The second part follows from the properties of the Bellman operators.Let π∗ be a policy that is greedy with respect to V∗. By the definition of a greedy policy, for every state s:$$ (T^{\pi^} V^)(s) = r(s,\pi^(s)) + \gamma \sum_{s'} P(s'|s,\pi^(s)) V^(s') = \max_{a \in \mathcal{A}} \left( r(s,a) + \gamma \sum_{s'} P(s'|s,a) V^(s') \right) = (T V^)(s) $$Therefore, the operators $T^{\pi^}$ and T have the same effect when applied to V∗, so Tπ∗V∗=TV∗.From the first part of the theorem, we know V∗=TV∗. Substituting this in, we get:V∗=Tπ∗V∗This shows that V∗ is a fixed point of the Bellman policy operator Tπ∗. However, we know from Section 6 that Tπ∗ is a contraction and has a unique fixed point, which is the value function of the policy π∗, namely Vπ∗. Therefore, it must be that V∗=Vπ∗. This proves that the greedy policy π∗ is indeed an optimal policy.2Existence of an Optimal Deterministic Stationary PolicyThe theorem guarantees the existence of an optimal policy that is both deterministic and stationary. For each state s, the argmax in the greedy policy definition is taken over a finite set of actions A. Therefore, a maximizing action is guaranteed to exist. We can construct a policy by selecting one such maximizing action for each state. This policy is deterministic (it maps each state to a single action) and stationary (the rule does not change with time). The Fundamental Theorem then ensures this policy is optimal.3 While optimal stochastic policies may also exist (e.g., if multiple actions achieve the maximum value), there is always at least one optimal policy that is deterministic and stationary.This theorem elegantly decouples the problem of finding the optimal values from finding the optimal actions. An algorithm can first focus entirely on solving the functional equation V=T(V) to find V∗. Once V∗ is known (or well-approximated), the theorem provides a simple, local procedure to recover the optimal policy: perform a one-step lookahead using V∗ and choose the action that maximizes the expected return. This separation of concerns is fundamental to the structure of value-based reinforcement learning.10. Value Iteration (VI): Algorithm and ConvergenceValue Iteration is the most direct algorithmic instantiation of the ideas presented so far. It is a dynamic programming method that iteratively applies the Bellman optimality operator to compute the optimal value function.Algorithm RecipeThe algorithm proceeds as follows 21:Initialization: Choose an arbitrary initial value function V0​∈R∣S∣. A common choice is V0​(s)=0 for all s∈S. Set an iteration counter k=0.Iteration: Repeat the following until convergence:a. For each state s∈S, perform the Bellman update:$$ V_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V_k(s') \right) $$This is equivalent to the vector-wise update Vk+1​←T(Vk​).b. Increment k←k+1.Termination: The loop terminates when the value function has converged, typically checked by monitoring the Bellman residual: ∥Vk+1​−Vk​∥∞​<ϵ for some small tolerance ϵ>0.Policy Extraction: Once the algorithm terminates with a final value function Vfinal​ (which is an approximation of V∗), an optimal policy π∗ is extracted by acting greedily with respect to Vfinal​:$$ \pi^*(s) = \arg\max_{a \in \mathcal{A}} \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V_{final}(s') \right) $$Convergence and RateThe convergence of the sequence of value functions {Vk​} to the optimal value function V∗ is a direct consequence of the Banach Fixed-Point Theorem. Since the update rule is Vk+1​=T(Vk​) and T has been proven to be a γ-contraction, the sequence {Vk​} must converge to the unique fixed point of T, which is V∗.14The convergence is geometric, with a rate determined by the discount factor γ. The error at iteration k is bounded by 22:\begin{equation} \label{eq:vi_conv_rate}|V^* - V_k|\infty \le \gamma^k |V^* - V_0|\infty\end{equation}This bound shows that the error shrinks by a factor of at least γ at every iteration.Value Iteration can be interpreted as a generalization of the Bellman-Ford algorithm for finding shortest paths in a graph. In a deterministic MDP with costs instead of rewards and γ=1, Vk​(s) would represent the cost of the shortest path of length at most k. The Bellman update propagates this cost information through the graph. The stochasticity and discounting in a general MDP make it a more complex version of this fundamental graph search process.A powerful feature of Value Iteration is its robustness. The updates do not need to be performed synchronously for all states in each pass. Asynchronous Value Iteration, where states are updated in any arbitrary order (provided that every state is updated infinitely often in an infinite run), is also guaranteed to converge to V∗.21 This property is of immense practical importance, as it forms the theoretical basis for many online learning algorithms like Q-learning, which can be viewed as a stochastic, asynchronous sample-based version of Value Iteration.11. Analysis of Value Iteration's ComplexityUnderstanding the computational complexity of Value Iteration is crucial for assessing its practicality and comparing it to other solution methods.Per-Iteration ComplexityA single iteration of Value Iteration requires updating the value for each of the ∣S∣ states. For each state, this involves iterating through all ∣A∣ actions. For each state-action pair, we must compute the expectation over all ∣S∣ possible next states. Therefore, the computational complexity of one full sweep of Value Iteration is O(∣S∣2∣A∣) for a tabular representation with dense transition matrices.21 If the transition dynamics are sparse (i.e., each action from each state leads to only a small number of possible next states), this can be reduced.Iteration Complexity (Upper and Lower Bounds)The iteration complexity refers to the number of iterations required to achieve a desired level of accuracy.Upper Bound: To obtain a value function Vk​ that is ϵ-close to V∗, i.e., ∥Vk​−V∗∥∞​≤ϵ, we can use the convergence rate from Equation \ref{eq:vi_conv_rate}. Assuming rewards are bounded by Rmax​, the initial error ∥V∗−V0​∥∞​ is at most 2Rmax​/(1−γ) if V0​=0. To ensure γk1−γ2Rmax​​≤ϵ, we need a number of iterations k that is approximately:$$ k \ge \frac{\log(\frac{2R_{max}}{\epsilon(1-\gamma)})}{\log(1/\gamma)} \approx O\left(\frac{1}{1-\gamma} \log\frac{1}{\epsilon}\right) $$The total runtime to find an ϵ-optimal value function is therefore O~(1−γ∣S∣2∣A∣​logϵ1​), where the O~ notation hides logarithmic terms in the problem parameters.24 The dependence on 1/(1−γ) is often referred to as the "curse of the horizon," as it shows that the number of iterations required by VI explodes as the agent becomes more far-sighted (γ→1). This is a fundamental limitation of the "shallow" one-step backup process of VI and motivates algorithms like Policy Iteration that can take larger steps in the policy space.Lower Bound: It has been shown that for any algorithm to be guaranteed to find a near-optimal policy, it must, in the worst case, perform a number of operations proportional to the size of the problem description. A lower bound on the complexity of planning in a general tabular MDP is Ω(∣S∣2∣A∣).24 This suggests that, for some problems, simply reading the input is the computational bottleneck. More advanced complexity theory results for the finite-horizon case, where the horizon H is part of the input (encoded in binary), show that the problem is EXPTIME-complete, indicating a much higher intrinsic difficulty.27Comparison of Dynamic Programming AlgorithmsValue Iteration is one of three classical dynamic programming methods for solving MDPs. The others are Policy Iteration and Linear Programming.MethodDescriptionPer-Iteration ComplexityIteration ComplexityOverall ComplexityValue IterationIteratively applies the Bellman optimality operator to find V∗.$O(\mathcal{S}^2Policy IterationAlternates between policy evaluation (solving for Vπ) and policy improvement (acting greedily w.r.t. Vπ).$O(\mathcal{S}^2Linear ProgrammingSolves a linear program whose solution corresponds to V∗ (primal) or the optimal occupancy measure (dual).Polynomial (e.g., using interior-point methods).Not an iterative method.Polynomial, but often less efficient in practice than PI/VI.Notes on Comparison:Policy Iteration (PI): Each iteration of PI is more computationally expensive than VI because policy evaluation involves solving a system of linear equations (or iterating until convergence). However, PI often converges in far fewer iterations because each policy improvement step can make a large jump in the value space. In practice, PI often converges faster than VI, especially when γ is close to 1.29Linear Programming (LP): The LP formulation provides a non-iterative way to solve for the optimal policy. While polynomial-time algorithms for LP exist, they are often less efficient in practice for typical MDPs than iterative methods like PI and VI.32 The main value of the LP formulation is often theoretical, providing a powerful analytical tool and a dual perspective on the problem.1912. Performance Guarantees for Greedy PoliciesIn practice, Value Iteration is terminated after a finite number of iterations, yielding an approximate value function V~≈V∗. A policy π~ is then extracted by acting greedily with respect to V~. A crucial question is: how good is this policy π~? The policy error bound provides the answer.The Policy Error Bound TheoremThis key theorem bounds the suboptimality of a greedy policy as a function of the error in its value function estimate.Theorem (Singh & Yee, 1994): Let V~∈R∣S∣ be an arbitrary value function, and let π~ be a policy that is greedy with respect to V~ (i.e., Tπ~V~=TV~). Then the value of this policy, Vπ~, is bounded by:\begin{equation} \label{eq:policy_error_bound}|V^* - V^{\tilde{\pi}}|\infty \le \frac{2\gamma}{1-\gamma} |V^* - \tilde{V}|\infty\end{equation}DerivationThe proof is a classic example of using the properties of Bellman operators. Let ϵ=∥V∗−V~∥∞​. We want to bound ∥V∗−Vπ~∥∞​. We start with the value difference for a single state s:\begin{align*}V^(s) - V^{\tilde{\pi}}(s) &= (T V^)(s) - (T^{\tilde{\pi}} V^{\tilde{\pi}})(s) \quad \text{(by definitions of } V^, V^{\tilde{\pi}} \text{ as fixed points)} \&= (T^{\pi^} V^)(s) - (T^{\tilde{\pi}} V^{\tilde{\pi}})(s) \quad \text{(where } \pi^ \text{ is greedy w.r.t. } V^* \text{)} \&\le (T^{\pi^} V^)(s) - (T^{\tilde{\pi}} V^)(s) + \gamma |V^ - V^{\tilde{\pi}}|\infty \quad \text{(from contraction of } T^{\tilde{\pi}} \text{)}\end{align*}This path is complex. A more direct proof follows.24 We start with V∗−Vπ~ and add and subtract terms:\begin{align*}V^* - V^{\tilde{\pi}} &= T V^* - T^{\tilde{\pi}} V^{\tilde{\pi}} \&= T V^* - T \tilde{V} + T \tilde{V} - T^{\tilde{\pi}} V^{\tilde{\pi}} \&= (T V^* - T \tilde{V}) + (T^{\tilde{\pi}} \tilde{V} - T^{\tilde{\pi}} V^{\tilde{\pi}}) \quad \text{(since } \tilde{\pi} \text{ is greedy w.r.t. } \tilde{V} \text{)}\end{align*}Now we take the norm of both sides and apply the triangle inequality:\begin{align*}|V^* - V^{\tilde{\pi}}|\infty &\le |T V^* - T \tilde{V}|\infty + |T^{\tilde{\pi}} \tilde{V} - T^{\tilde{\pi}} V^{\tilde{\pi}}|\infty \&\le \gamma |V^* - \tilde{V}|\infty + \gamma |\tilde{V} - V^{\tilde{\pi}}|\infty \quad \text{(by contraction of } T \text{ and } T^{\tilde{\pi}} \text{)}\end{align*}Using the triangle inequality again on the second term, ∥V~−Vπ~∥∞​≤∥V~−V∗∥∞​+∥V∗−Vπ~∥∞​, we get:\begin{align*}|V^* - V^{\tilde{\pi}}|\infty &\le \gamma |V^* - \tilde{V}|\infty + \gamma (|V^* - \tilde{V}|\infty + |V^* - V^{\tilde{\pi}}|\infty) \|V^* - V^{\tilde{\pi}}|\infty &\le 2\gamma |V^* - \tilde{V}|\infty + \gamma |V^* - V^{\tilde{\pi}}|\infty\end{align*}Rearranging the terms gives the final bound 24:$$ (1-\gamma) |V^* - V^{\tilde{\pi}}|\infty \le 2\gamma |V^* - \tilde{V}|\infty \implies |V^* - V^{\tilde{\pi}}|\infty \le \frac{2\gamma}{1-\gamma} |V^* - \tilde{V}|_\infty $$ImplicationsThis bound quantifies how errors in value function estimation propagate to performance loss in the resulting policy. The factor 1−γ2γ​ acts as an "error amplification" constant. For γ close to 1, this factor can be very large (e.g., 198 for γ=0.99). This implies that even a small error in the value function estimate can lead to a policy with a much larger loss in performance, especially for problems with long effective horizons. This is a critical insight for Approximate Dynamic Programming and Deep Reinforcement Learning, where value function approximation errors are unavoidable. It suggests that simply minimizing the Bellman error (the error in the value function estimate) might not be the most robust objective for learning a good policy, and it highlights the inherent difficulty of learning in problems where γ is close to 1.Part III: The Geometric PerspectiveThe classical analysis of MDPs relies on algebraic and analytic tools. A more recent and powerful perspective emerges from studying the geometry of the spaces of policies and value functions. This view, particularly as developed by Dadashi et al. (2019), provides novel intuition for the structure of the problem and the behavior of learning algorithms.813. The Dual Perspective: Discounted Occupancy MeasuresBefore delving into the geometry of the value space, it is instructive to consider a dual representation of a policy's behavior: the discounted occupancy measure.Formal DefinitionThe discounted state-action occupancy measure, ρπ, for a policy π and a starting state distribution μ, is a distribution over state-action pairs that represents the discounted frequency of visiting each pair. It is formally defined as 19:\begin{equation} \label{eq:occupancy_measure}\rho_\mu^\pi(s,a) = (1-\gamma) \mathbb{E}\pi \left\end{equation}The factor (1−γ) is a normalization constant ensuring that $\sum{s,a} \rho_\mu^\pi(s,a) = 1$, making it a probability distribution. Some definitions omit this factor, resulting in a measure whose total mass is 1/(1−γ).2Value as a Linear Function of OccupancyA key property of occupancy measures is that the expected value of a policy can be expressed as a simple inner product between the (unnormalized) occupancy measure and the reward vector. Let νμπ​=ρμπ​/(1−γ) be the unnormalized measure. Then:$$ \mathbb{E}{s \sim \mu}[V^\pi(s)] = \sum{s,a} \nu_\mu^\pi(s,a) r(s,a) = \langle \nu_\mu^\pi, r \rangle $$This relationship transforms the problem of maximizing value into finding an occupancy measure that places high mass on high-reward state-action pairs.33The Occupancy Measure Polytope and Linear ProgrammingThe set of all valid occupancy measures for stationary policies forms a convex polytope. This polytope is defined by a set of linear equality constraints known as the Bellman flow constraints, which ensure that the discounted flow of probability into a state equals the flow out of it 19:$$ \sum_{a \in \mathcal{A}} \rho(s,a) = (1-\gamma)\mu(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s|s',a') \rho(s',a') \quad \forall s \in \mathcal{S} $$This dual perspective leads directly to a Linear Programming (LP) formulation for solving MDPs. The problem of finding the optimal policy is equivalent to solving the following LP 19:\begin{itemize}\item Primal LP (in Value Space):$$ \min_V \sum_s \mu(s) V(s) \quad \text{s.t.} \quad V(s) \ge r(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s') \quad \forall s,a $$\item Dual LP (in Occupancy Space):$$ \max_\rho \sum_{s,a} \rho(s,a) r(s,a) \quad \text{s.t.} \quad \text{Bellman flow constraints and } \rho(s,a) \ge 0 $$\end{itemize}This primal-dual view of RL is a cornerstone of MDP theory. The primal view, associated with Value Iteration, operates in the value space, seeking a function that satisfies the Bellman optimality equation. The dual view, associated with Linear Programming, operates in the space of occupancy measures, seeking a valid flow distribution that maximizes expected reward. This dual perspective is a powerful analytical tool and has inspired algorithms in areas like imitation learning, where the goal is to match an expert's occupancy measure rather than their specific actions.3514. The Geometry of Value: The Value Function PolytopeThe most direct geometric analysis occurs in the primal space of value functions itself. This perspective characterizes the shape and properties of the set of all achievable value functions.The Value Function PolytopeLet Π be the set of all stationary policies. The space of value functions is the image of the policy space under the value function mapping, VΠ​={Vπ∣π∈Π}.8A central result from Dadashi et al. (2019) is the characterization of this space's geometry:Theorem: The space of value functions VΠ​ is a polytope in R∣S∣.A polytope is the convex hull of a finite set of points. In this case, the vertices of the convex hull of VΠ​ are a subset of the value functions corresponding to the deterministic policies, {Vπ∣π∈Πdet​}. The full set VΠ​ is the image of the policy simplex under the non-linear mapping from Equation \ref{eq:direct_solution}, and it may be non-convex or self-intersecting, but it is contained within this convex hull.8 The optimal value function V∗ is always a vertex of the convex hull of VΠ​.The Line TheoremA key property that reveals the structure of this polytope is the Line Theorem.Theorem (Dadashi et al., 2019): Consider two deterministic policies, π1​ and π2​, that agree on their action choices for all states except for one state, s0​. Let πα​ be a stochastic policy that, at state s0​, chooses action π1​(s0​) with probability α and action π2​(s0​) with probability 1−α, and follows the common action choices elsewhere. Then the set of value functions {Vπα​∣α∈} forms a straight line segment in R∣S∣ connecting Vπ1​ and Vπ2​.8The proof relies on showing that the matrix (I−γPπα​)−1 can be expressed as an affine function of α using the Sherman-Morrison formula, which makes Vπα​ an affine function of α. This theorem demonstrates a fundamental local linearity in the mapping from policies to values.The Boundary TheoremThis theorem relates the boundary of the policy space to the boundary of the value space.Theorem (Dadashi et al., 2019): The boundary of the value function polytope VΠ​ is contained within the image of the boundary of the policy simplex.The boundary of the policy simplex consists of policies that are deterministic for at least one state. This theorem implies that the "extreme" value functions, which lie on the boundary of the achievable set, are generated by policies that make at least some deterministic choices.8This geometric framework provides a unified landscape for understanding MDPs. It complements the algebraic view of Bellman equations and the dual view of occupancy measures by providing a primal geometric structure. It shows that the value functions of deterministic policies act as the vertices of a convex hull that contains all other value functions. The optimal value function V∗ must be one of these vertices, providing a geometric re-statement of the Fundamental Theorem that an optimal deterministic policy always exists. This perspective can also inspire new formalisms for representation learning, where the goal might be to find a low-dimensional state embedding that preserves the essential geometric structure of the value function polytope.715. Visualizing Dynamics: A Geometric View of Value IterationThe value function polytope provides a powerful canvas for visualizing and understanding the behavior of reinforcement learning algorithms. Each algorithm traces a different kind of path through the value space as it seeks the optimal vertex, V∗.Value Iteration as a Trajectory in Value SpaceThe sequence of value functions generated by Value Iteration, V0​,V1​,V2​,…, can be interpreted as a path of points in the ∣S∣-dimensional value space. The algorithm starts at an initial point V0​ (e.g., the origin) and iteratively applies the Bellman optimality operator T. Since T is a γ-contraction with V∗ as its unique fixed point, this path is guaranteed to converge to the optimal vertex V∗.36An important observation is that the path of Value Iteration does not necessarily stay within the value function polytope VΠ​. The intermediate iterates Vk​ may not correspond to the value function of any stationary policy. However, the limit point of the sequence, V∗, is guaranteed to be a vertex of the polytope's convex hull.Geometrically, the Bellman operator T maps any point V∈R∣S∣ towards the optimal vertex V∗. As noted, T(V) is the point that results from taking the upper envelope of the affine transformations corresponding to all deterministic policies, applied to V. This operation pulls the current estimate Vk​ towards the "optimistic frontier" defined by the vertices of the polytope.Visual Comparison of AlgorithmsProjecting the high-dimensional value space onto 2D or 3D allows for insightful visualizations that contrast the dynamics of different algorithms 36:Value Iteration (VI): Traces a smooth, direct path through the value space towards the optimal vertex V∗. Each step is a contraction, so the steps become progressively smaller as the iterate approaches the solution. The path is generally not confined to the polytope itself.Policy Iteration (PI): Generates a path that jumps between the vertices of the value function polytope. Each iteration consists of two steps: policy evaluation (which can be seen as finding the vertex Vπ corresponding to the current policy π) and policy improvement (which identifies a new greedy policy π′ and jumps to the corresponding vertex Vπ′). The Policy Improvement Theorem guarantees that each jump is monotonic, i.e., Vπ′≥Vπ.Policy Gradient (PG): Methods like Policy Gradient trace a path on the surface of the value function polytope. They perform local search in the policy space, which translates to making infinitesimal steps along the gradient on the surface of the value manifold.This geometric view provides a powerful intuition for the differing characteristics of these algorithms. Policy Iteration often converges in fewer iterations because it makes large, directed jumps between vertices. Value Iteration takes smaller, more numerous steps, as each application of the contraction mapping only moves it a fraction of the distance to the fixed point. This geometric understanding does more than just explain existing algorithms; it can inspire new ones. For example, the Geometric Policy Iteration (GPI) algorithm was proposed based on these insights. Instead of a standard greedy update, GPI updates a single state's policy by choosing an action that moves the value function directly towards a calculated point on the boundary of the value function polytope, aiming for faster improvement without the full computational cost of a PI step.18 This demonstrates how a deeper theoretical and geometric understanding can lead directly to novel and potentially more efficient algorithms.ConclusionThis technical note has provided a comprehensive overview of the theory of finite, discounted Markov Decision Processes. Beginning with the foundational definitions of the MDP framework, policies, and value functions, we established the core analytical tools of the field: the Bellman equations and their corresponding operators.The application of the Banach Fixed-Point Theorem was shown to be the linchpin of the classical theory, providing rigorous guarantees for the existence and uniqueness of the optimal value function and the convergence of fundamental algorithms like Value Iteration. The analysis of VI's computational complexity and the policy error bound for greedy policies highlighted the practical implications and limitations of these methods, particularly the challenges posed by long-horizon problems where the discount factor γ approaches 1.The latter part of this note shifted to a more modern, geometric perspective, centering on the Value Function Polytope. This framework, which characterizes the space of all achievable value functions as a polytope, offers a powerful new lens through which to understand the structure of MDPs and the dynamics of learning algorithms. Key results like the Line Theorem reveal the local structure of the value space, while the overall polytope view provides a unified landscape where algorithms like Value Iteration and Policy Iteration trace distinct paths toward the optimal solution. This geometric intuition not only deepens our understanding of why these algorithms behave as they do but also opens new avenues for algorithm design, bridging the gap between abstract theory and practical performance. The interplay between the algebraic, analytic, and geometric viewpoints provides a rich and multifaceted understanding of one of the most fundamental models in reinforcement learning.ReferencesAltman, E. (1999). Constrained Markov decision processes. CRC Press.Banach, S. (1922). Sur les opérations dans les ensembles abstraits et leur application aux équations intégrales. Fundamenta Mathematicae, 3(1), 133-181.Bellman, R. (1957). Dynamic Programming. Princeton University Press.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena Scientific.Dadashi, R., Taïga, A. A., Le Roux, N., Schuurmans, D., & Bellemare, M. G. (2019). The Value Function Polytope in Reinforcement Learning. In International Conference on Machine Learning (pp. 1536-1545). PMLR.Howard, R. A. (1960). Dynamic programming and Markov processes. MIT Press.Puterman, M. L. (2014). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons.Singh, S. P., & Yee, R. C. (1994). An upper bound on the loss from approximate value iteration. In Proceedings of the 7th Annual Conference on Computational Learning Theory (pp. 2-9).Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press.Szepesvári, C. (2010). Algorithms for reinforcement learning. Morgan & Claypool Publishers.Wu, Y., & De Loera, J. A. (2022). A Geometric Study of the Value Function Polytope and its Applications to Reinforcement Learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 2099-2108).
